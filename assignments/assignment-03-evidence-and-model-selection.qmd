---
title: "Assignment 03"
subtitle: "EVIDENCE AND MODEL SELECTION"
description: "The goal of this assignment is to build your understanding of using information criteria for model selection. This assignment is worth 15 points. <br /><br /> *Copyright EPSY 8252, 2024*"
format:
  html:
   css: "assets/styles.css"
date: today
---

<br />

In this assignment, you will use the data from the file *wine.csv* to examine several different predictors of wine rating (a measure of the wine's quality). The literature has suggested that price of wine is quite predictive of a wine's quality.

- [[CSV]](https://raw.githubusercontent.com/zief0002/fluffy-ants/main/data/wine.csv)
- [[Data Codebook]](http://zief0002.github.io/fluffy-ants/codebooks/wine.html)


```{r}
#| echo: false
#| out-width: "50%"
#| fig-align: "center"
#| fig-alt: "Decorative image"
knitr::include_graphics("figs/assign-03.png")
```


## Instructions

**Quarto Submissions**

Create a project directory called "assignment-02". Within this directory, create a QMD document (this can be a QMD, or word-processed documents) to respond to each of the questions below. All code chunks should include a label `#| label:` and include comments. Submit a zipped version of your entire `assignment-02` project directory.


**Non-Quarto Submissions**

Submit both the word-processed documents that respond to each of the questions below, as well as a commented, organized script file.


For all submissions your syntax will be evaluated based on the rubric at the end of the assignment. This will count for *2pts.** of the assignment score and will be scored as follows:

- 2pts: Most/All of the evaluation  is "Always" AND you have no categories marked as "Rarely" or "Never"
- 1.5pts: Most/All of the evaluation is "Most of the time" AND you have no categories marked as "Rarely" or "Never"
- 1pt: Most/All of the evaluation is "Sometimes" OR you have one category marked as "Rarely" marks AND no "Never" marks
- 0.5pts: Most/All of the evaluation is "Sometimes" or you have more than one categories marked as "Rarely" and no categories marked as "Never"
- 0pts: You have one or more categories marked as "Never"

<br />


## Preparation

Read the article: Snipes, M., &amp; Taylor, D. C. (2014). Model selection and Akaike Information Criteria: An example from wine ratings and prices. *Wine Economics and Policy, 3*(1), 3--9. https://doi.org/10.1016/j.wep.2014.03.001


You will be carrying out a form of a robustness study to evaluate the working hypotheses proposed in Snipes and Taylor (2014). In this study, we will fit the same candidate models that Snipes and Taylor fitted in their analysis, however we will be using a different data set (e.g., our data includes more regions than Snipes and Taylor’s data). We will also make one change in the analysis, and that is we will treat vintage as a continuous variable; we won’t categorize it like Snipes and Taylor did. By using a different set of data and making slightly different analytic decisions we can more vigorously evaluate the underlying working hypotheses.


Fit the same nine candidate models that Snipes and Taylor fitted in their analysis, using the *wine.csv* data. In these models use wine rating (`rating`) as the outcome. Remember to treat `vintage` as a continuous variable in the models that include it. These models will be named Model 0, Model 1, ..., Model 8 to be consistent with the naming in the Snipes and Taylor paper. Use these fitted models to answer the questions in the assignment.

<br />


## Model Selection: Likelihood Framework of Evidence

1. Compute and interpret the likelihood ratio for comparing the empirical support between Model 2 and Model 3. (In computing the likelihoods, use the `logLik()` function, not `dnorm()`.)

2. Can we carry out a likelihood ratio test to evaluate whether the amount of empirical support when comparing Model 2 and Model 3 is more than we expect because of sampling error? If so, compute and report the results from the $\chi^2$-test. If not, explain why not.

3. Compute and interpret the likelihood ratio for comparing the empirical support between Model 2 and Model 5.

4. Can we carry out a likelihood ratio test to evaluate whether the amount of empirical support when comparing Model 2 and Model 5 is more than we expect because of sampling error? If so, compute and report the results from the $\chi^2$-test. If not, explain why not.

<br />


## Model Selection: Information Criteria

5. Create a table of model evidence that includes the following information for each of the nine candidate models. **(2pts.)**

  - Model
  - Log-likelihood
  - *K*
  - AICc
  - $\Delta$ AICc
  - Model Probability (AICc Weight)

Use this table of model evidence to answer Questions 6--12.

6. Use the AICc values to select the working hypothesis with the most empirical evidence.

7. Interpret the model probability (i.e., AICc weight) for the working hypothesis with the most empirical evidence.

8. Compute and interpret the evidence ratio that compares the two working hypotheses with the most empirical evidence.

9. Based on previous literature, Snipes and Taylor hypothesized that price was an important predictor of wine quality. Based on your analyses, is price an important predictor of wine quality? Justify your response by referring to the model evidence. (Hint: Pay attention to which models include price and which do not.)

10. Does the empirical evidence support adopting more than one working hypothesis? Justify your response by referring to the model evidence.

11. Does the empirical evidence from the Snipes and Taylor analyses support adopting more than one candidate model? Justify your response by by referring to the model evidence.

12. Based on your responses to the last two questions, which set of analyses (yours or Snipes and Taylor) has more model selection uncertainty? Explain.

<br />


# Rubric for Evaluating Student Syntax

**Documentation:** Is the syntax well annotated to aid communication to other educational scientists and researchers? Each of these are scored "Always", "Most of the time", "Sometimes", "Rarely", "Never" 

- Labels (in QMD) should be descriptive of the syntax in the chunk or more holistic comments should be included (in .R files) to explain chunks of syntax
- Comments are liberally included to explain smaller pieces of syntax (especially where non-common code is used).
- Object names accurately describe the intent of the object (e.g., `pew_data` rather than `d`)
- Plots should include a comment that provides the caption (or usage of `fig-cap:` in QMD)


**Structure and Presentation:** Is the syntax organized and structured to be "readable" and easy to follow for other educational scientists and researchers? Each of these are scored "Always", "Most of the time", "Sometimes", "Rarely", "Never"

- Consistent use of case (e.g., lower-case, camel-case) for all objects
- Consistent use of operators throughout the code (e.g., always using `<-` or `=` for assignment)
- Spacing is included for readability (e.g., `lm.1 = lm(...)` rather than `lm.1=lm(...)`)
- Indentation and line breaks are included in long code (e.g., when using `|>` or when a function has many arguments), especially in `{dplyr}` and `{ggplot}` syntax

<br />



