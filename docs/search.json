[
  {
    "objectID": "umn-policies.html",
    "href": "umn-policies.html",
    "title": "University of Minnesota Policies and Procedures",
    "section": "",
    "text": "Academic freedom is a cornerstone of the University. Within the scope and content of the course as defined by the instructor, it includes the freedom to discuss relevant matters in the classroom. Along with this freedom comes responsibility. Students are encouraged to develop the capacity for critical judgment and to engage in a sustained and independent search for truth. Students are free to take reasoned exception to the views offered in any course of study and to reserve judgment about matters of opinion, but they are responsible for learning the content of any course of study for which they are enrolled.1 Reports of concerns about academic freedom are taken seriously, and there are individuals and offices available for help. Contact the instructor (Andrew Zieffler; zief0002@umn.edu), the Department Chair (Anne Foegen; afoegen@umn.edu), your adviser, the associate dean of the college (Tabitha Grier-Reed; grier001@umn.edu), or the Vice Provost for Faculty and Academic Affairs in the Office of the Provost (Beth Lewis; blewis@umn.edu)."
  },
  {
    "objectID": "umn-policies.html#academic-freedom-and-responsibility",
    "href": "umn-policies.html#academic-freedom-and-responsibility",
    "title": "University of Minnesota Policies and Procedures",
    "section": "",
    "text": "Academic freedom is a cornerstone of the University. Within the scope and content of the course as defined by the instructor, it includes the freedom to discuss relevant matters in the classroom. Along with this freedom comes responsibility. Students are encouraged to develop the capacity for critical judgment and to engage in a sustained and independent search for truth. Students are free to take reasoned exception to the views offered in any course of study and to reserve judgment about matters of opinion, but they are responsible for learning the content of any course of study for which they are enrolled.1 Reports of concerns about academic freedom are taken seriously, and there are individuals and offices available for help. Contact the instructor (Andrew Zieffler; zief0002@umn.edu), the Department Chair (Anne Foegen; afoegen@umn.edu), your adviser, the associate dean of the college (Tabitha Grier-Reed; grier001@umn.edu), or the Vice Provost for Faculty and Academic Affairs in the Office of the Provost (Beth Lewis; blewis@umn.edu)."
  },
  {
    "objectID": "umn-policies.html#appropriate-student-use-of-class-notes-and-course-materials",
    "href": "umn-policies.html#appropriate-student-use-of-class-notes-and-course-materials",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Appropriate Student Use of Class Notes and Course Materials",
    "text": "Appropriate Student Use of Class Notes and Course Materials\nTaking notes is a means of recording information but more importantly of personally absorbing and integrating the educational experience. However, broadly disseminating class notes beyond the classroom community or accepting compensation for taking and distributing classroom notes undermines instructor interests in their intellectual work product while not substantially furthering instructor and student interests in effective learning. Such actions violate shared norms and standards of the academic community. For additional information, please see: https://policy.umn.edu/education/studentresp."
  },
  {
    "objectID": "umn-policies.html#disability-accommodations",
    "href": "umn-policies.html#disability-accommodations",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Disability Accommodations",
    "text": "Disability Accommodations\nThe University of Minnesota views disability as an important aspect of diversity, and is committed to providing equitable access to learning opportunities for all students. The Disability Resource Center (DRC) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations.\n\nIf you have, or think you have, a disability in any area such as, mental health, attention, learning, chronic health, sensory, or physical, please contact the DRC office on your campus (612.626.1333) to arrange a confidential discussion regarding equitable access and reasonable accommodations.\nStudents with short-term disabilities, such as a broken arm, can often work with instructors to minimize classroom barriers. In situations where additional assistance is needed, students should contact the DRC as noted above.\nIf you are registered with the DRC and have a disability accommodation letter dated for this semester or this year, please contact your instructor early in the semester to review how the accommodations will be applied in the course.\nIf you are registered with the DRC and have questions or concerns about your accommodations please contact your (access consultant/disability specialist).\n\nAdditional information is available on the DRC website or e-mail drc@umn.edu with questions."
  },
  {
    "objectID": "umn-policies.html#equity-diversity-equal-opportunity-and-affirmative-action",
    "href": "umn-policies.html#equity-diversity-equal-opportunity-and-affirmative-action",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Equity, Diversity, Equal Opportunity, and Affirmative Action",
    "text": "Equity, Diversity, Equal Opportunity, and Affirmative Action\nThe University will provide equal access to and opportunity in its programs and facilities, without regard to race, color, creed, religion, national origin, gender, age, marital status, disability, public assistance status, veteran status, sexual orientation, gender identity, or gender expression. For more information, please consult Board of Regents Policy."
  },
  {
    "objectID": "umn-policies.html#makeup-work-for-legitimate-absences",
    "href": "umn-policies.html#makeup-work-for-legitimate-absences",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Makeup Work for Legitimate Absences",
    "text": "Makeup Work for Legitimate Absences\nStudents will not be penalized for absence during the semester due to unavoidable or legitimate circumstances. Such circumstances include verified illness, participation in intercollegiate athletic events, subpoenas, jury duty, military service, bereavement, and religious observances. Such circumstances do not include voting in local, state, or national elections. For complete information, please see: https://policy.umn.edu/education/makeupwork."
  },
  {
    "objectID": "umn-policies.html#mental-health-and-stress-management",
    "href": "umn-policies.html#mental-health-and-stress-management",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Mental Health and Stress Management",
    "text": "Mental Health and Stress Management\nAs a student you may experience a range of issues that can cause barriers to learning, such as strained relationships, increased anxiety, alcohol/drug problems, feeling down, difficulty concentrating and/ or lack of motivation. These mental health concerns or stressful events may lead to diminished academic performance and may reduce your ability to participate in daily activities. University of Minnesota services are available to assist you. You can learn more about the broad range of confidential mental health services available on campus via the Student Mental Health Website."
  },
  {
    "objectID": "umn-policies.html#scholastic-dishonesty",
    "href": "umn-policies.html#scholastic-dishonesty",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Scholastic Dishonesty",
    "text": "Scholastic Dishonesty\nYou are expected to do your own academic work and cite sources as necessary. Failing to do so is scholastic dishonesty. Scholastic dishonesty means plagiarizing; cheating on assignments or examinations; engaging in unauthorized collaboration on academic work; taking, acquiring, or using test materials without faculty permission; submitting false or incomplete records of academic achievement; acting alone or in cooperation with another to falsify records or to obtain dishonestly grades, honors, awards, or professional endorsement; altering, forging, or misusing a University academic record; or fabricating or falsifying data, research procedures, or data analysis. (Student Conduct Code)\nIf it is determined that a student has cheated, the student may be given an “F” or an “N” for the course, and may face additional sanctions from the University. For additional information, please see the policy statement. The Office for Community Standards has compiled a useful list of Frequently Asked Questions pertaining to scholastic dishonesty. If you have additional questions, please clarify with your instructor for the course. Your instructor can respond to your specific questions regarding what would constitute scholastic dishonesty in the context of a particular class—e.g., whether collaboration on assignments is permitted, requirements and methods for citing sources, if electronic aids are permitted or prohibited during an exam."
  },
  {
    "objectID": "umn-policies.html#senate-academic-workload-policy",
    "href": "umn-policies.html#senate-academic-workload-policy",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Senate Academic Workload Policy",
    "text": "Senate Academic Workload Policy\nOne conventional credit is hereby defined as equivalent to three hours of learning effort per week, averaged over an appropriate time interval, necessary for an average student taking that course to achieve an average grade in that course. It is expected that the academic work required of graduate and professional students will exceed three hours per credit per week or 45 hours per semester."
  },
  {
    "objectID": "umn-policies.html#sexual-harassment-sexual-assault-stalking-and-relationship-violence",
    "href": "umn-policies.html#sexual-harassment-sexual-assault-stalking-and-relationship-violence",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Sexual Harassment, Sexual Assault, Stalking and Relationship Violence",
    "text": "Sexual Harassment, Sexual Assault, Stalking and Relationship Violence\nThe University prohibits sexual misconduct, and encourages anyone experiencing sexual misconduct to access resources for personal support and reporting. If you want to speak confidentially with someone about an experience of sexual misconduct, please contact your campus resources including the Aurora Center, Boynton Mental Health or Student Counseling Services https://eoaa.umn.edu/report-misconduct. If you want to report sexual misconduct, or have questions about the University’s policies and procedures related to sexual misconduct, please contact your campus Title IX office or relevant policy contacts.\nInstructors are required to share information they learn about possible sexual misconduct with the campus Title IX office that addresses these concerns. This allows a Title IX staff member to reach out to those who have experienced sexual misconduct to provide information about personal support resources and options for investigation. You may talk to instructors about concerns related to sexual misconduct, and they will provide support and keep the information you share private to the extent possible given their University role. https://regents.umn.edu/sites/regents.umn.edu/files/2019-09/policy_sexual_harassment_sexual_assault_stalking_and_relationship_violence.pdf"
  },
  {
    "objectID": "umn-policies.html#sexual-harassment",
    "href": "umn-policies.html#sexual-harassment",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Sexual Harassment",
    "text": "Sexual Harassment\n“Sexual harassment” means unwelcome sexual advances, requests for sexual favors, and/or other verbal or physical conduct of a sexual nature. Such conduct has the purpose or effect of unreasonably interfering with an individual’s work or academic performance or creating an intimidating, hostile, or offensive working or academic environment in any University activity or program. Such behavior is not acceptable in the University setting. For additional information, please consult Board of Regents Policy"
  },
  {
    "objectID": "umn-policies.html#student-conduct-code",
    "href": "umn-policies.html#student-conduct-code",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Student Conduct Code",
    "text": "Student Conduct Code\nThe University seeks an environment that promotes academic achievement and integrity, that is protective of free inquiry, and that serves the educational mission of the University. Similarly, the University seeks a community that is free from violence, threats, and intimidation; that is respectful of the rights, opportunities, and welfare of students, faculty, staff, and guests of the University; and that does not threaten the physical or mental health or safety of members of the University community. As a student at the University you are expected adhere to Board of Regents Policy: Student Conduct Code.\nNote that the conduct code specifically addresses disruptive classroom conduct, which means “engaging in behavior that substantially or repeatedly interrupts either the instructor’s ability to teach or student learning. The classroom extends to any setting where a student is engaged in work toward academic credit or satisfaction of program-based requirements or related activities.”"
  },
  {
    "objectID": "umn-policies.html#use-of-personal-electronic-devices-in-the-classroom",
    "href": "umn-policies.html#use-of-personal-electronic-devices-in-the-classroom",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Use of Personal Electronic Devices in the Classroom",
    "text": "Use of Personal Electronic Devices in the Classroom\nUsing personal electronic devices in the classroom setting can hinder instruction and learning, not only for the student using the device but also for other students in the class. To this end, the University establishes the right of each faculty member to determine if and how personal electronic devices are allowed to be used in the classroom. For complete information, please reference: https://policy.umn.edu/education/studentresp."
  },
  {
    "objectID": "umn-policies.html#grading-and-transcripts",
    "href": "umn-policies.html#grading-and-transcripts",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Grading and Transcripts",
    "text": "Grading and Transcripts\nUniversity Grading Scales The University has two distinct grading scales: A–F and S–N.\nA–F grading scale. The A–F grading scale allows the following grades and corresponding GPA points:\n\n\n\n\n\n\n\n\nGrade\nGPA Points\nDefinitions for undergraduate credit\n\n\n\n\nA\n4.000\nRepresents achievement that significantly exceeds expectations in the course.\n\n\nA-\n3.667\n\n\n\nB+\n3.333\n\n\n\nB\n3.000\nRepresents achievement that is above the minimum expectations in the course.\n\n\nB-\n2.667\n\n\n\nC+\n2.333\n\n\n\nC\n2.000\nRepresents achievement that meets the minimum expectations in the course.\n\n\nC-\n1.667\n\n\n\nD+\n1.333\n\n\n\nD\n1.000\nRepresents achievement that partially meets the minimum expectations in the course. Credit is earned but it may not fulfill major or program requirements.\n\n\nF\n0.000\nRepresents failure in the course and no credit is earned.\n\n\n\nS–N grading scale. The S–N grading scale allows for the following grades and corresponding GPA points:\n\n\n\nGrade\nGPA Points\nDefinitions for undergraduate credit\n\n\n\n\nS\n0.000\nSatisfactory (equivalent to a C- or better).\n\n\nN\n0.667\nNot Satisfactory\n\n\n\nFor additional information, please refer to: https://policy.umn.edu/education/gradingtranscripts."
  },
  {
    "objectID": "umn-policies.html#footnotes",
    "href": "umn-policies.html#footnotes",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLanguage adapted from the American Association of University Professors “Joint Statement on Rights and Freedoms of Students”.↩︎"
  },
  {
    "objectID": "readings/14-lmer-other-random-effects-and-covariates.html",
    "href": "readings/14-lmer-other-random-effects-and-covariates.html",
    "title": "📖 LMER: Other Random-Effects and Covariates",
    "section": "",
    "text": "Required\nRead the following::\n\nUchikoshi, Y. (2005). Narrative development in bilingual kindergarteners: Can Arthur help? Developmental Psychology, 41(3), 464–478. doi: 10.1037/0012-1649.41.3.464\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about mixed-effects regression models. Here are some resources that may be helpful in that endeavor:\n\nLong, J. D. (2012). Longitudinal data analysis for the behavioral sciences using R. Thousand Oaks, CA: Sage.\nEstimation of the Log-Likelihood in lmer"
  },
  {
    "objectID": "readings/10-logistic-regression.html",
    "href": "readings/10-logistic-regression.html",
    "title": "📖 Logistic Regression Model",
    "section": "",
    "text": "Required\nWatch the following::\n\n3Blue1Brown. Exponential growth and epidemics. YouTube video.\n\nWhen you watch this video, focus on the ideas and not the equations.\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about logistic regression models. Here are some resources that may be helpful in that endeavor:\n\nFox, J. (2009). The Binomial and Bernoulli distributions (Section 3.2.1). In A mathematical primer for social statistics. Sage. [Required Textbook]\nDunteman, G. H., & Ho, M.-H. R. (2005). An introduction to generalized linear models. Sage.\nFox, J. (2015). Applied regression analysis and generalized linear models (3rd ed.). Sage"
  },
  {
    "objectID": "readings/07-logarithmic-transformations-outcome.html",
    "href": "readings/07-logarithmic-transformations-outcome.html",
    "title": "📖 Log-Transforming the Outcome",
    "section": "",
    "text": "Required\nRequired\nRefresh your knowledge about logarithms:\n\nKhan Academy Intro to Logarithms Tutorial\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about log-transformations. Here are some resources that may be helpful in that endeavor:\n\nMath with Bad Drawings: ABC Book of e"
  },
  {
    "objectID": "readings/05-polynomial-effects.html",
    "href": "readings/05-polynomial-effects.html",
    "title": "📖 Polynomial Effects",
    "section": "",
    "text": "Required\nRefresh your knowledge about parabolas and quadratic functions by going though the Khan Academy Parabolas Intro.\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about polynomial functions. Here are some resources that may be helpful in that endeavor:\n\nVarsity Tutors: Quadratic Function\nMath Centre: Polynomial Function\nOverfitting: A guided tour"
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html",
    "href": "readings/03-introduction-to-quarto.html",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Head over to https://quarto.org/ and click on the big, blue “Get Started” button. Then follow the instructions to install Quarto.\nOpen RStudio. (Note if RStudio was open when you installed Quarto, quit and re-open it.)\n\nAt this point in RStudio, if you go to File &gt; New File... you should see options to choose Quarto Document and Quarto Presentation."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#install-quarto",
    "href": "readings/03-introduction-to-quarto.html#install-quarto",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Head over to https://quarto.org/ and click on the big, blue “Get Started” button. Then follow the instructions to install Quarto.\nOpen RStudio. (Note if RStudio was open when you installed Quarto, quit and re-open it.)\n\nAt this point in RStudio, if you go to File &gt; New File... you should see options to choose Quarto Document and Quarto Presentation."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#help-and-tutorials",
    "href": "readings/03-introduction-to-quarto.html#help-and-tutorials",
    "title": "Introduction to Quarto",
    "section": "Help and Tutorials",
    "text": "Help and Tutorials\nThere are several guides and tutorials available on the Quarto website. These include:\n\nCreating basic content (e.g., figures, tables, diagrams, footnotes, citations)\nIncluding computation/syntax\nGenerating output in different formats (e.g., HTML, PDF, MS Word)\nCreating slides and presentations (These notes are a Quarto presentation!)\nBuilding websites\nBuilding books\nCreating interactive dashboards"
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#create-your-first-quarto-document",
    "href": "readings/03-introduction-to-quarto.html#create-your-first-quarto-document",
    "title": "Introduction to Quarto",
    "section": "Create Your First Quarto Document",
    "text": "Create Your First Quarto Document\nClick the Tutorial: Hello Quarto link on the left-side of the “Getting Started” page. Make sure that the RStudio tool is selected at the top of the tutorial.\n\nWork through this tutorial. This will introduce you to some of the basic concepts of creating and rendering a Quarto document."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#learning-how-to-integrate-r-code",
    "href": "readings/03-introduction-to-quarto.html#learning-how-to-integrate-r-code",
    "title": "Introduction to Quarto",
    "section": "Learning How to Integrate R Code",
    "text": "Learning How to Integrate R Code\nAlso work through the Tutorial: Computations. Again, before starting, ensure that the RStudio tool is selected at the top of the tutorial."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#learn-more",
    "href": "readings/03-introduction-to-quarto.html#learn-more",
    "title": "Introduction to Quarto",
    "section": "Learn More",
    "text": "Learn More\nCheck out the Welcome to Quarto Workshop. This is a 2 1/2 hour workshop introducing people to Quarto.\n\nWelcome to Quarto Workshop! (YouTube Video)\n\nIf you are familiar with RMarkdown and want to learn how to switch over to Quarto, here is a nice 30 minute talk by Mine Çetinkaya-Rundel that helps you do that.\n\n2022 Toronto Workshop on Reproducibility - Mine Çetinkaya-Rundel (YouTube Video)"
  },
  {
    "objectID": "readings/02-project-organization.html",
    "href": "readings/02-project-organization.html",
    "title": "📖 Project Management",
    "section": "",
    "text": "Before next class, look at your computer files and your organization of those files. Here are some things to reflect on:\n\nAre your files organized into folders/directories? Or are they all in your Downloads folder?\n\nHow did you organize all the data files, notes, etc. from EPsy 8251?\n\nIf I asked you to find a specific file, could you locate it without using “Search”?\nCan you tell what is in a particular file by just looking at its name?\nDo your file names contain spaces? What about characters that aren’t letters, numbers, dashes, or underscores?\nAre your file names consistent (all lower case letters, or all title case)? Or are they all different?"
  },
  {
    "objectID": "notes/15-optional-lmer-alt-representations-and-assumptions.html",
    "href": "notes/15-optional-lmer-alt-representations-and-assumptions.html",
    "title": "Linear Mixed-Effects Models: Alternative Representations and Assumptions",
    "section": "",
    "text": "In this set of notes, you will learn alternative ways of representing the linear mixed-effects model. You will also learn about the underlying assumptions for the linear mixed-effects model, as well as how to evaluate them empirically. To do this, we will use data from the file minneapolis.csv to analyze change in students’ reading scores over time.\n# Load libraries\nlibrary(broom.mixed) #for tidy, glance, and augment functions for lme4 models\nlibrary(educate)\nlibrary(lme4) #for fitting mixed-effects models\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Read in data\nmpls = read_csv(\"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/minneapolis.csv\")\n\n# View data\nmpls\nTo illustrate some the concepts in this set of notes, we will consider the following three models:\n\\[\n\\begin{split}\n\\mathbf{Model~1:~~}\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\beta_1(\\mathrm{Grade}_{ij}-5) + \\\\\n&~~\\beta_2(\\mathrm{Special~Education}_j)+ \\epsilon_{ij}\\\\[2em]\n\\mathbf{Model~2:~~}\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\\\ &~~\\beta_2(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\\\\[2em]\n\\mathbf{Model~3:~~}\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\\\ &~~\\beta_2(\\mathrm{Special~Education}_j) + \\\\\n&~~\\beta_3(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]\nwhere,"
  },
  {
    "objectID": "notes/15-optional-lmer-alt-representations-and-assumptions.html#multilevel-equation-for-model-2",
    "href": "notes/15-optional-lmer-alt-representations-and-assumptions.html#multilevel-equation-for-model-2",
    "title": "Linear Mixed-Effects Models: Alternative Representations and Assumptions",
    "section": "Multilevel Equation for Model 2",
    "text": "Multilevel Equation for Model 2\nAs a second example, consider the composite (mixed-effects) model for Model 2:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) +\\\\\n&~~\\beta_2(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]\nThe Level-1 equation is again:\n\\[\n\\mathrm{Reading~Score}_{ij} = \\beta_{0j} + \\beta_{1j}(\\mathrm{Grade~Level}_{ij}) + \\epsilon_{ij}\n\\]\nThe Level-2 equations are:\n\\[\n\\begin{split}\n\\beta_{0j} &= \\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j} \\\\\n\\beta_{1j} &= \\beta_1 + b_{1j}\\\\\n\\end{split}\n\\]\nThe Level-2 intercept equation hasn’t changed from that for Model 1. However, the equation for the effect of grade-level now includes the random-effect of slope. This means that student’s growth rate is allowed to vary from the average."
  },
  {
    "objectID": "notes/15-optional-lmer-alt-representations-and-assumptions.html#going-from-multilevel-equations-to-the-mixed-effects-model",
    "href": "notes/15-optional-lmer-alt-representations-and-assumptions.html#going-from-multilevel-equations-to-the-mixed-effects-model",
    "title": "Linear Mixed-Effects Models: Alternative Representations and Assumptions",
    "section": "Going from Multilevel Equations to the Mixed-Effects Model",
    "text": "Going from Multilevel Equations to the Mixed-Effects Model\nIf we have the multilevel equations, we can substitute the Level-2 equation(s) into the Level-1 equation to get the composite equation or mixed-effects equation. For example, for Model 2, substituting \\(\\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j}\\) into \\(\\beta_{0j}\\) and \\(\\beta_1 + b_{1j}\\) into \\(\\beta_{1j}\\) gives us:\n\\[\n\\mathrm{Reading~Score}_{ij} = \\bigg[\\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j}\\bigg] + \\bigg[\\beta_1 + b_{1j}\\bigg](\\mathrm{Grade~Level}_{ij}) + \\epsilon_{ij}\n\\]\nRe-arranging this:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\bigg[\\beta_0 + b_{0j}\\bigg] + \\bigg[\\beta_1 + b_{1j}\\bigg](\\mathrm{Grade~Level}_{ij}) + \\\\\n&~~\\beta_2(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/15-optional-lmer-alt-representations-and-assumptions.html#guidelines-for-writing-the-multilevel-equations",
    "href": "notes/15-optional-lmer-alt-representations-and-assumptions.html#guidelines-for-writing-the-multilevel-equations",
    "title": "Linear Mixed-Effects Models: Alternative Representations and Assumptions",
    "section": "Guidelines for Writing the Multilevel Equations",
    "text": "Guidelines for Writing the Multilevel Equations\nHere are some guidelines in helping you think about writing multilevel equations.\n\nWrite the Level-1 equation first. This will be an equation that expresses the outcome’s relationship to a series of within-student parameters, and a within-student residual.\nThe number of within-student parameters in the Level-1 equation (aside from the residual) dictate the number of Level-2 equations you will have.\nThe within-student parameters from the Level-1 equation will be the outcomes in the Level-2 equations.\nRandom-effects are the residuals in the Level-2 equations, and therefore are in the Level-2 equations; one per equation.\nVariables from the data go to their appropriate level. For example within-student variables (i.e., having an ij subscript) will be put in the Level-1 equation, and between-student predictors (i.e., having only a j subscript) will be put in one or more of the Level-2 equations."
  },
  {
    "objectID": "notes/15-optional-lmer-alt-representations-and-assumptions.html#evaluate-assumptions-about-the-level-1-residuals",
    "href": "notes/15-optional-lmer-alt-representations-and-assumptions.html#evaluate-assumptions-about-the-level-1-residuals",
    "title": "Linear Mixed-Effects Models: Alternative Representations and Assumptions",
    "section": "Evaluate Assumptions about the Level-1 Residuals",
    "text": "Evaluate Assumptions about the Level-1 Residuals\nWe will evaluate the Level-1 residuals in the exact same way we evalauted the residuals from a fixed-effects (LM) analysis. The augment() function from the {broom.mixed} package produces the Level-1 residuals and fitted values.\n\n# Augment the model to get the Level-1 residuals and fitted values\nout_2 = augment(lmer.2)\n\n# View\nout_2\n\n\n  \n\n\n\nThe Level-1 residuals are found in the .resid column, and the .fitted column contains the \\(\\hat{Y}\\) values. As with LM residual analysis, we want to examine the normality of the residuals in a density plot (or some other plot that allows you to evaluate this), and the other assumptions by plotting the residuals against the fitted values in a scatterplot.\n\n# Density plot of the level-1 residuals\np1 = ggplot(data = out_2, aes(x = .resid)) +\n  stat_density_confidence(model = \"normal\") +\n  stat_density(geom = \"line\") +\n  theme_bw() +\n  xlab(\"Level-1 residuals\")\n\n\n# Scatterplot of the Level-1 residuals versus the fitted values\np2 = ggplot(data = out_2, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_smooth() +\n  geom_hline(yintercept = 0) +\n  theme_bw() +\n  xlab(\"Fitted values\") +\n  ylab(\"Level-1 residuals\")\n\n# Plot side-by-side\np1 | p2\n\n\n\n\n\n\n\nFigure 1: Plots to evaluate the level-1 residuals.\n\n\n\n\n\nBased on the plots, the distributional assumptions for the Level-1 residuals seem reasonably satisfied. The density plot suggests that the normality assumption is tenable. The scatterplot shows symmetry around the \\(Y=0\\) line (average residual is 0), and that the data are consistent with the assumption of homoskedasticity."
  },
  {
    "objectID": "notes/15-optional-lmer-alt-representations-and-assumptions.html#assumptions-about-the-random-effects",
    "href": "notes/15-optional-lmer-alt-representations-and-assumptions.html#assumptions-about-the-random-effects",
    "title": "Linear Mixed-Effects Models: Alternative Representations and Assumptions",
    "section": "Assumptions about the Random-Effects",
    "text": "Assumptions about the Random-Effects\nWe also need to examine the assumptions for any random-effects included in the model. For this course, we will examine the normality assumption. In our example that means we need to examine the normality assumption about the intercept and grade-level random-effects. To do this we need to extract the random-effects from the model into a data frame so we can use ggplot2 functions to evaluate normality.\n\n# Obtain a data frame of the random-effects\nlevel_2 = tidy(lmer.2, effects = \"ran_vals\")\n\n# View random-effects\nlevel_2\n\n\n  \n\n\n\nRecall that this tibble includes both the intercept and the grade-level random-effects. We will facet_wrap() on the term column to plot each of these distributions separately.\n\n# Density plot of the RE for intercept\nggplot(data = level_2, aes(x = estimate)) +\n  stat_density_confidence(model = \"normal\") +\n  stat_density(geom = \"line\") +\n  theme_bw() +\n  xlab(\"Level-2 residuals\") +\n  facet_wrap(~term)\n\n\n\n\n\n\n\nFigure 2: Density plot of the estimated random-effects for intercept (left) and grade-level (right). The confidence envelope is based on the expected variation in a normal distribution.\n\n\n\n\n\nNeither distribution of random-effects seem consistent with the assumption of normality."
  },
  {
    "objectID": "notes/15-optional-lmer-alt-representations-and-assumptions.html#interpreting-the-output-from-the-log-transformed-model",
    "href": "notes/15-optional-lmer-alt-representations-and-assumptions.html#interpreting-the-output-from-the-log-transformed-model",
    "title": "Linear Mixed-Effects Models: Alternative Representations and Assumptions",
    "section": "Interpreting the Output from the Log-Transformed Model",
    "text": "Interpreting the Output from the Log-Transformed Model\nWe interpret fixed-effect coefficients from the LMER similar to those from the LM. We now just apply the same interpretational changes that we did when previously log-transformed the outcomes.\n\ntidy(lmer.3, effects = \"fixed\")\n\n\n  \n\n\n\n\nThe intercept is the average log-transformed reading score for non-special education students in the 5th grade. Back-transforming this, we find the average reading score for these students is \\(e^{5.34} = 208.52\\).\nBack-transforming the grade-level effect, we find that each one grade difference is associated with a 2% (\\(e^{0.021} = 1.02\\)) increase in reading scores, on average, after controlling for differences in special education status.\nBack-transforming the special education status effect, we find that special education students have reading scores that are \\(e^{-0.0905} = 0.91\\) times those for non-special education students, on average, after controlling for differences in grade-level.\n\nWe can also plot the average growth trajectory and any individual students’ growth trajectories for the back-transformed models to aid in the interpretation.\n\nggplot(data = mpls, aes(x = (grade-5), y = reading_score)) +\n  geom_function(fun = function(x){exp(5.34 - 0.0905 + 0.0210*x)}, color = \"#0072B2\") + #SpEd (Avg)\n  geom_function(fun = function(x){exp(5.34 + 0.0210*x)}, color = \"#E69F00\") + #Non-SpEd (Avg)\n  geom_function(fun = function(x){exp(5.34 - 0.157 + 0.0210*x)}, color = \"#E69F00\", linetype = \"dashed\") + #Student 1; non-SpEd\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"5th\", \"6th\", \"7th\", \"8th\")\n  ) +\n  ylab(\"Reading Score\")\n\n\n\n\n\n\n\nFigure 4: Change in reading scores over time for the average special education (solid, blue line) and non-special education student (solid, orange line). Student 1’s growth trajectory (dashed line) is also displayed.\n\n\n\n\n\nThe almost linear rate-of-change is further indication that the log-transformation is probably not necessary."
  },
  {
    "objectID": "notes/15-optional-lmer-alt-representations-and-assumptions.html#footnotes",
    "href": "notes/15-optional-lmer-alt-representations-and-assumptions.html#footnotes",
    "title": "Linear Mixed-Effects Models: Alternative Representations and Assumptions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, there is also a covariance between \\(b_{0j}\\) and \\(b_{1j}\\), but this term does not play into the assumptions.↩︎"
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html",
    "href": "notes/13-lmer-average-change-over-time.html",
    "title": "LMER: Average Change Over Time",
    "section": "",
    "text": "In this set of notes, you will learn how to use the linear mixed-effects model to examine the mean change over time in a set of longitudinal/repeated measurements. To do this, we will use data from the file vocabulary.csv.\nThese data include repeated measurements of scaled vocabulary scores for \\(n=64\\) students.\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom.mixed) #for tidy, glance, and augment functions for lme4 models\nlibrary(corrr)\nlibrary(educate)\nlibrary(lme4) #for fitting mixed-effects models\nlibrary(patchwork)\nlibrary(texreg)\nlibrary(tidyverse)\n\n# Read in data\nvocabulary = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/vocabulary.csv\")\n\n# View data\nvocabulary\nWe will use these data to explore the change in vocabulary over time (longitudinal variation in the vocabulary scores). In most longitudinal analyses, the primary goal is to examine the mean change of some outcome over time. To do this we will focus on the following research question: (1) What is the growth pattern in the average vocabulary score over time?"
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#switching-between-the-two-data-structures",
    "href": "notes/13-lmer-average-change-over-time.html#switching-between-the-two-data-structures",
    "title": "LMER: Average Change Over Time",
    "section": "Switching between the Two Data Structures",
    "text": "Switching between the Two Data Structures\nThe library {tidyr} (loaded as part of the {tidyverse} metapackage) has two functions that convert data between these two structures:\n\npivot_longer() (wide \\(\\rightarrow\\) tidy/long), and\npivot_wider() (tidy/long \\(\\rightarrow\\) wide).\n\nBelow, I show the code for going from the wide structured data (vocabulary) to the tidy/long structure.\n\n# Convert from wide to long structured data\nvocabulary_long = vocabulary |&gt;\n  pivot_longer(cols = vocab_08:vocab_11, names_to = \"grade\", values_to = \"vocab_score\") |&gt;\n  arrange(id, grade)\n\n# View data\nvocabulary_long\n\n\n  \n\n\n\nHere is an animated tutorial for understanding this syntax.\n\nFor more information about using these functions, Google “tidyr pivot” and read through any number of great tutorials or vignettes; for example here. You can also read Hadley Wickham’s (2014) original paper on tidy data."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#fitting-and-interpreting-the-model",
    "href": "notes/13-lmer-average-change-over-time.html#fitting-and-interpreting-the-model",
    "title": "LMER: Average Change Over Time",
    "section": "Fitting and Interpreting the Model",
    "text": "Fitting and Interpreting the Model\nWe fit the model and display the output below. We include the argument REML=FALSE to force the lmer() function to produce maximum likelihood estimates (rather than restricted maximum likelihood estimates). In practice, we will generally want to fit these models using ML estimation.\n\n# Fit model\nlmer.0 = lmer(vocab_score ~ 1 + (1 | id), data = vocabulary_long, REML = FALSE)\n\nRecall that there are two types of fitted equations we can produce from this model: (1) the global fitted equation, and (2) student-specific fitted equation. The global fitted equation is based on only the fixed effects. We obtain the fixed-effects estimates using the tidy() function from the {broom.mixed} package with the argument effects=\"fixed\".\n\n# Fit model\nlmer.0 = lmer(vocab_score ~ 1 + (1 | id), data = vocabulary_long, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.0, effects = \"fixed\")\n\n\n  \n\n\n\nUsing the fixed-effects estimates, the global fitted equation based on the fixed-effects model is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = 2.53\n\\]\nWe interpret coefficients from the fixed-effects model the same way we interpret coefficients produced from the lm() output. For example,\n\nThe predicted average vocabulary language score for all students at all time points is 2.53.\n\nWe can also write the student specific fitted equations. Each student specific fitted equation is based on the fixed-effects AND the student-specific random-effect. To obtain the student-specific random-effects we use the tidy() function from the {broom.mixed} package with the argument effects=\"ran_vals\".\n\n# Obtain random effects\ntidy(lmer.0, effects = \"ran_vals\")\n\n\n  \n\n\n\nFor example, Student 1’s fitted equation is:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Vocabulary~Score}_{i1}} &= 2.53 + 0.359 \\\\[1ex]\\hat{\\mathrm{Vocabulary~Score}_{i1}} &= 2.53 + 0.359 \\\\[1ex]\n&= 2.89\n\\end{split}\n\\]\n\nThe predicted average vocabulary language score for Student 1 at all time points is 2.889.\n\nThe argument effects=\"ran_coefs\" in the tidy() function gives the student-specific intercepts directly.\n\n# Obtain student-specific coefficients\ntidy(lmer.0, effects = \"ran_coefs\")\n\n\n  \n\n\n\nFrom here, we see that Student 2’s fitted equation is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{i2}} = 2.34\n\\]\nThe predicted average vocabulary language score for Student 2 (at all time points) is 2.34. From the two students’ fitted equations we see that Student 1’s “growth” curve is higher than the average “growth” curve and Student 2’s “growth” curve is lower than the average “growth” curve. Here is a plot of those three “growth” curves.\n\n\nCode\nggplot(data = vocabulary_long, aes(x = grade, y = vocab_score)) +\n  geom_point(alpha = 0) +\n  geom_hline(yintercept = 2.53) + #Global 'growth' curve\n  geom_hline(yintercept = 2.89, linetype = \"dashed\", color = \"#0072B2\") + #Student 1 'growth' curve\n  geom_hline(yintercept = 2.34, linetype = \"dashed\", color = \"#E69F00\") + #Student 1 'growth' curve\n  theme_light() +\n  scale_x_discrete(\n    name = \"Grade-level\",\n    labels = c(\"8th-grade\", \"9th-grade\", \"10th-grade\", \"11th-grade\")\n    ) +\n  ylab(\"Vocabulary score\") +\n  ylim(0, 4)\n\n\n\n\n\n\n\n\nFigure 2: Predicted change in vocabulary score over time based on the unconditional random intercepts model for all students (solid, black line), Student 1 (blue, dashed line), and Student 2 (orange, dashed line)."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#partitioning-unexplained-variation",
    "href": "notes/13-lmer-average-change-over-time.html#partitioning-unexplained-variation",
    "title": "LMER: Average Change Over Time",
    "section": "Partitioning Unexplained Variation",
    "text": "Partitioning Unexplained Variation\nThis model is not very informative for applied researchers in terms of characterizing change over time, but it is essential to decompose the variation into within- and between-subjects variation. In our model, there are two sources of variation. The first source of variation measures the between-student variation (how different are students’ individual “growth” curves). The second source of variation measures the within-student variation (how different are an individual’s outcome values from their individual “growth” curve).\nTo see this graphically, we show Student 1’s data along with the global “growth” curve and Student 1’s specific “growth” curves. The between-student variation is akin to the average of the squared deviations between each student-specific “growth” curve and the global “growth” curve. The left-hand plot below shows these residuals for Student 1, referred to as Level-2 residuals. The within-student variation is akin to the average of the squared deviations between each student’s outcome (vocabulary scores) and that student’s specific “growth” curve. The right-hand plot below shows these residuals for Student 1, referred to as Level-1 residuals.\n\n\n\n\n\n\n\n\nFigure 3: Student 1’s vocabulary scores at each time point along with the predicted change in vocabulary score over time based on the unconditional random intercepts model for all students (solid, black line) and Student 1 (blue, dashed line). The between-student variation (LEFT) and within-student variation (RIGHT) based on the residuals for Student 1 are also displayed.\n\n\n\n\n\nWe quantify the amount of between- and within-student variation using an estimate of variance. The between-student variance estimate is denoted \\(\\sigma^2_{b_0}\\) (or simply \\(\\sigma^20\\)), and the within-student variance estimate is denoted \\(\\sigma^2_{\\epsilon}\\). We can access these paramater estimates using the argument effects=\"ran_pars\" in the tidy() function.\n\n# Obtain variance estimates\ntidy(lmer.0, effects = \"ran_pars\")\n\n\n  \n\n\n\nNote that the output is in the standard deviation metric. We need to square these values to get the actual variance estimates.\nBetween-Student Variance:\n\\[\n\\hat\\sigma^2_0 = 1.72^2 = 2.96\n\\]\nWithin-Student Variance:\n\\[\n\\hat\\sigma^2_{\\epsilon} = 1.35^2 = 1.82\n\\]\nThese values represent the unexplained variation at each level. The total unexplained variation is the sum of these two terms.\n\n# Total unexplained variation\n2.96 + 1.82\n\n[1] 4.78\n\n\nWe can compute the proportion of unexplained variation at each level by dividing the appropriate variance estimate by this total.\n\n# Proportion of unexplained variation that is between-subjects\n2.96 / (2.96 + 1.82)\n\n[1] 0.6192469\n\n# Proportion of unexplained variation that is within-subjects\n1.82 / (2.96 + 1.82)\n\n[1] 0.3807531\n\n\nInterpreting these values:\n\nRoughly 62% of the unexplained variation in vocabulary scores is between-student variation.\nRoughly 38% of the unexplained variation in vocabulary scores is within-student variation.\n\nBased on this partitioning from the unconditional random intercepts model we have evidence that it may be fortuitous to include both between-student and within-student predictors; there is unaccounted for variation at both levels. To explain the unaccounted for between-student variation, include between-student-level predictors (e.g., female) in the model. To explain the unaccounted for within-student variation, include within-student predictors (e.g., grade) in the model.\nSince the larger proportion of unexplained variation is between-student variation, we might ultimately focus on between-student predictors more than within-student predictors. However, because this is a longitudinal analysis, our primary focus is on the grade predictor, which is a within-subjects predictor. It is here that we turn next.\n\nThis partitioning of variation should be done in every analysis, and ALWAYS is done using the unconditional random intercepts model. The unconditional random intercepts model will serve as our baseline model. As we add predictors, we can compare the unexplained variation at each level in the predictor models to the baseline unaccounted for variation in the unconditional means model. This is one way of measuring how effective predictors are at further explaining variation in the model."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#repeated-measures-anova-rm-anova",
    "href": "notes/13-lmer-average-change-over-time.html#repeated-measures-anova-rm-anova",
    "title": "LMER: Average Change Over Time",
    "section": "Repeated-Measures ANOVA (RM-ANOVA)",
    "text": "Repeated-Measures ANOVA (RM-ANOVA)\nOne historic method of analyzing longitudinal data is Repeated Measures Analysis of Variance (RM-ANOVA).1 The linear mixed-effects model that includes time as one or more categorical predictors and a random-effect of intercept produces the same results as the RM-ANOVA.\n\nYou should not use RM-ANOVA to analyze longitudinal data. It requires a condition called sphericity that makes some stringent assumptions about the variances and correlations between repeated measures. One requirement of sphericity is that the correlation between any two time points are exactly the same. Consider the correlations of our data:\n\n# Compute correlations between repeated measures\nvocabulary |&gt;\n  select(vocab_08:vocab_11) |&gt;\n  correlate()\n\n\n  \n\n\n\nThe correlations are not equal, and are likely not to be equal in the population! In repeated measures data, observations that are more closely spaced in time tend to be more correlated. This violates the sphericity assumption needed for RM-ANOVA. It is rare that sphericity is ever tenablein practice with longitudinal data. Because of this it is preferable to use LMER to carry out these analyses."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#quantitative-time-predictor-a-more-flexible-model-for-repeated-measures-data",
    "href": "notes/13-lmer-average-change-over-time.html#quantitative-time-predictor-a-more-flexible-model-for-repeated-measures-data",
    "title": "LMER: Average Change Over Time",
    "section": "Quantitative Time Predictor: A More Flexible Model for Repeated Measures Data",
    "text": "Quantitative Time Predictor: A More Flexible Model for Repeated Measures Data\nOne advantage to using the linear mixed-effects model to analyze repeated measures data over traditional methods (e.g., RM-ANOVA or MANOVA) is that the regression model allows for both categorical and quantitative variables. For example, rather than code our grade-levels categorically (as vocab_08, vocab_09, vocab_10 and vocab_11), which was a necessity in days of yore, we could have simply coded them as 8, 9, 10, and 11. Then we could have fitted the LMER model using this quantitative predictor. The statistical model when time is quantitative would be:\n\\[\n\\mathrm{Vocabulary~Score}_{ij} = \\big[\\beta_0 + b_{0j}\\big] + \\beta_1(\\mathrm{Grade}_{ij}) + \\epsilon_{ij}\n\\]\nwhere,\n\n\\(\\mathrm{Vocabulary~Score}_{ij}\\) is the vocabulary score at time point \\(i\\) for student \\(j\\);\n\\(\\beta_0\\) is the fixed-effect of intercept;\n\\(b_{0j}\\) is the random-effect of intercept for student \\(j\\);\n\\(\\mathrm{Grade}_{ij}\\) is a quantitative variable indicating grade-level,\n\\(\\beta_1\\) is the effect of a one-unit change in grade, and\n\\(\\epsilon_{ij}\\) is the error at time point \\(i\\) for student \\(j\\).\n\nThis is still referred to as the unconditional growth model since the only predictor is a fixed-effect of time.\n\n\nLookup Table: Mapping Categories to Quantities\nOne method to convert grade to a quantitative variable is to create a lookup table. A lookup table maps the levels of the categorical time predictor to the values we want to use in our new quantitative predictor. Below I create a lookup table to map the categorical time predictor to the relevant grade-level (grade_quant).\n\n# Create lookup table\nlookup_table = data.frame(\n  grade = c(\"vocab_08\", \"vocab_09\", \"vocab_10\", \"vocab_11\"),\n  grade_quant = c(8, 9, 10, 11)\n)\n\n# View lookup table\nlookup_table\n\n\n  \n\n\n\nThen, we join (or merge) the tidy/long data with the lookup table. This adds the quantitative variables (with the correct mapping) to our tidy data. There are, of course, other ways to accomplish the same thing. For example a mutate() using the case_when() function could also be used to create this mapping.\n\n# Join the data with the lookup table\nvocabulary_long = vocabulary_long |&gt;\n  left_join(lookup_table, by = \"grade\")\n\n# View data\nvocabulary_long\n\n\n  \n\n\n\nIn this join, we are adding records from the lookup_table data (right data) to the vocabulary_long data (left data).2 When we perform this left join, the final joined data will include all records from vcabulary_long (left dataset) but only those records from the lookup_table dataset whose key (grade value) is contained in the student data. For example, if the lookup_table data included information about a grade level that was not included in the vocabulary_long data, it would not appear in the joined data.\nThe {tidyverse} package includes six different join functions. You can read about four common join functions and see an animation illustrating them here."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#centering-the-time-predictor-better-interpretations-of-the-intercept",
    "href": "notes/13-lmer-average-change-over-time.html#centering-the-time-predictor-better-interpretations-of-the-intercept",
    "title": "LMER: Average Change Over Time",
    "section": "Centering the Time Predictor: Better Interpretations of the Intercept",
    "text": "Centering the Time Predictor: Better Interpretations of the Intercept\nOne method of improving the interpretations of the intercept is to center the time predictor. Recall that centering a variable means to add/subtract a constant value from each case. In a longitudinal analysis, we typically center the time predictor by subtracting the value of the time predictor at baseline (i.e., the first time point). In our example:\n\\[\n\\mathrm{Centered~Grade}_{ij} = \\mathrm{Grade}_{ij} - 8\n\\]\nThis essentially maps the values of {8, 9, 10, 11} in the quant_grade variable to {0, 1, 2, 3}. Then we include this centered predictor as fixed-effect in the model. Mathematically, the model is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = \\bigg[\\beta_0 + b_0\\bigg]  + \\beta_1(\\mathrm{Grade\\mbox{-}level}_{ij} - 8) + \\epsilon_{ij}\n\\]\nFitting this model:\n\n# Fit unconditional growth model with centered grade\nlmer.3 = lmer(vocab_score ~ 1 + I(grade_quant-8) + (1|id), data = vocabulary_long, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.3, effects = \"fixed\")\n\n\n  \n\n\n\nThe fitted equation is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = 1.41 + 0.75(\\mathrm{Grade\\mbox{-}level}_{ij}-8)\n\\]\nInterpreting the coefficients,\n\nThe predicted average vocabulary score for 8th-grade students is 1.41. Centering removes the problem of extrapolation in the interpretation because we have now made 0 a legitimate value in the predictor.\nEach one-unit difference in grade-level is associated with a 0.75-point difference in vocabulary score, on average. This is identical to the previous model since we have not changed what a one-unit difference in the predictor represents.\n\nWe can see why the intercepts are different and the slopes are the same by comparing the plots of the individual growth profiles and the fitted fixed-effects models for the uncentered and centered predictors.\n\n\nCode\np1 = ggplot(data = vocabulary_long, aes(x = grade_quant, y = vocab_score)) +\n  geom_line(aes(group = id), alpha = 0.3) +\n  geom_abline(intercept = -4.56, slope = 0.75, color = \"blue\") +\n  geom_point(x = 0, y = -4.56, size = 1.5, color = \"blue\") +\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade-level\",\n    limits = c(0, 11),\n    breaks = c(0, 2, 4, 6, 8, 10)\n    ) +\n  scale_y_continuous(\n    name = \"Vocabulary score\",\n    limits = c(-5, 10)\n    )\n\np2 = ggplot(data = vocabulary_long, aes(x = I(grade_quant-8), y = vocab_score)) +\n  geom_line(aes(group = id), alpha = 0.3) +\n  geom_abline(intercept = 1.41, slope = 0.75, color = \"blue\") +\n  geom_point(x = 0, y = 1.41, size = 1.5, color = \"blue\") +\n  theme_light() +\n  scale_x_continuous(\n    name = \"Centered grade-level (0 = 8th grade)\",\n    limits = c(0, 11),\n    breaks = c(0, 2, 4, 6, 8, 10)\n    ) +\n  scale_y_continuous(\n    name = \"Vocabulary score\",\n    limits = c(-5, 10)\n    )\n\n# Display plots\np1 | p2\n\n\n\n\n\n\n\n\nFigure 4: Plot showing the change in vocabulary score over time for 64 students. The average growth profile is also displayed. This is shown for the non-centered (left) and 8th-grade centered (right) grade-level. A large blue point is shown at the intercept value in both plots.\n\n\n\n\n\nLooking at the variance components:\n\n# SD\ntidy(lmer.3, effects = \"ran_pars\")\n\n\n  \n\n\n# Compute variance components\n1.784 ^ 2 #Between-students\n\n[1] 3.182656\n\n0.947 ^ 2 #Within-students\n\n[1] 0.896809\n\n\nComparing them to the unconditional random intercepts model:\nWithin-Student Variance (\\(\\hat\\sigma^2_{\\epsilon}\\))\nThe unexplained variance that is within-students has decreased from 1.82 to 0.90. Including grade-level in the model has explained 50.8% of the within-student variation. This is because grade is a within-student predictor (it has values that vary within each student).\nBetween-Student Variance (\\(\\hat\\sigma^2_{0}\\))\nThe unexplained variance that is between-students has increased from 2.96 to 3.18. This means that by including grade in the model we have increased the amount of unexplained variation that is between students by 7.8%; a mathematical artifact of the estimation process.\nThese values are identical to the variance components obtained from the previous model. That is because the model we fitted is mostly identical to the uncentered quantitative model. The only difference is in the intercept value, which now is interpretable since a grade of 0 on the centered scale corresponds to the initial time point in the data; it is no longer extrapolation."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#student-specific-profiles",
    "href": "notes/13-lmer-average-change-over-time.html#student-specific-profiles",
    "title": "LMER: Average Change Over Time",
    "section": "Student-Specific Profiles",
    "text": "Student-Specific Profiles\nObtaining the coefficients for the student specific growth curves we get:\n\n# Get student estimated parameters\ntidy(lmer.3, effects = \"ran_coefs\") |&gt;\n  arrange(as.numeric(level))\n\n\n  \n\n\n\nThe fitted equations for the student-specific growth curves for Student 1 and Student 2 are:\n\\[\n\\begin{split}\n\\mathbf{Student~1:~} \\hat{\\mathrm{Vocabulary~Score}_{ij}} &= 1.80 + 0.75(\\mathrm{Grade\\mbox{-}level}_{ij}-8)\\\\[1ex]\n\\mathbf{Student~2:~} \\hat{\\mathrm{Vocabulary~Score}_{ij}} &= 1.20 + 0.75(\\mathrm{Grade\\mbox{-}level}_{ij}-8)\n\\end{split}\n\\]\nThe student-specific growth rates are identical for all students (i.e., the slope value is the same in all the equations), and also is the same as the average global growth rate. Including a random-effect of intercept, although accounting for the within-student correlation among vocabulary scores, does not allow students to have different rates-of-growth. The difference is in the intercept values, which indicates where students start (i.e., students’ initial vocabulary score in the 8th grade).\nWe can see these same things by plotting these student specific growth curves along with the global average growth curve.\n\n# Create plot\nggplot(data = vocabulary_long, aes(x = I(grade_quant-8), y = vocab_score)) +\n  geom_point(alpha = 0) +\n  geom_abline(intercept = 1.41, slope = 0.75, color = \"black\") +     # Average growth curve\n  geom_abline(intercept = 1.80, slope = 0.75, color = \"#0072B2\") +   # Student 1\n  geom_abline(intercept = 1.20, slope = 0.75, color = \"#E69F00\") +   # Student 2\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade-level\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"0\\n(8th)\", \"1\\n(9th)\", \"2\\n(10th)\", \"3\\n(11th)\")\n    ) +\n  ylab(\"Vocabulary score\")\n\n\n\n\n\n\n\nFigure 5: Predicted change in vocabulary score over time based on the linear (centered) unconditional growth model for all students (solid, black line), Student 1 (blue, dashed line), and Student 2 (orange, dashed line).\n\n\n\n\n\nThe parallel lines suggest the same rate-of-growth across students."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#comparing-the-unconditional-growth-models",
    "href": "notes/13-lmer-average-change-over-time.html#comparing-the-unconditional-growth-models",
    "title": "LMER: Average Change Over Time",
    "section": "Comparing the Unconditional Growth Models",
    "text": "Comparing the Unconditional Growth Models\nThe unconditional growth model was fitted using three different methods.\n\nThe first model treated time (grade) as categorical.\nThe second two models treated time as a continuous variable.\n\nTreating time continuously rather than as a categorical predictor has many advantages:\n\nIn the real-world time is continuous.\nThe model is simpler. In the model where time was treated categorically, we had to estimate four regression coefficients and two variance components. In the continuous models, we only had to estimate two regression coefficients and two variance components. If there were additional time points, we would still only need to estimate four parameters for the continuous model, but the number of estimates would increase for the categorical model.\nIt allows us to include participants being measured at different times.\nIt allows us to model nonlinear relationships more easily.\n\n\nIn general, you should always treat time continuously when you have a longitudinal study! This guidance also implies that you should use linear mixed-effects models rather than RM-ANOVA for the analysis of longitudinal data.\nMoreover, because of the interpretive value of the intercept when we center the grade-level predictor at the first time point, it is often a good idea to center your time predictor."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#evaluating-the-three-functional-forms",
    "href": "notes/13-lmer-average-change-over-time.html#evaluating-the-three-functional-forms",
    "title": "LMER: Average Change Over Time",
    "section": "Evaluating the Three Functional Forms",
    "text": "Evaluating the Three Functional Forms\nAs with any model, we want to examine the evidence to determine the correct functional form. In practice, you would also want to evaluate the residuals to see which of the potential candidate models meets the assumptions. (We will look at residuals in an upcoming set of notes.) Suffice it to say, the residual plots look similar across all three models indicating that all the models seem to meet the assumptions equally well. Since we are using the same outcome and data in all three models, we can also evaluate the models using information criteria and their related metrics.\n\n# Model-evidence\naictab(\n  cand.set = list(lmer.0, lmer.3, lmer.4, lmer.5),\n  modnames = c(\"No change\", \"Linear growth\", \"Quadratic growth\", \"Log-linear growth\")\n)\n\n\n  \n\n\n\nGiven the data and candidate models, the evidence primarily supports the log-linear model. There is also some evidence for the quadratic model and almost no evidence for the linear model. This is consistent with the nonlinearity we observed in the mean profile in the spaghetti plot. Given this, the higher evidence for the log-linear model, and the simplicity of the log-linear model relative to the quadratic model, we will adopt the log-linear functional form for our unconditional growth model."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#examining-the-output-for-the-adopted-log-linear-fitted-model",
    "href": "notes/13-lmer-average-change-over-time.html#examining-the-output-for-the-adopted-log-linear-fitted-model",
    "title": "LMER: Average Change Over Time",
    "section": "Examining the Output for the Adopted Log-Linear Fitted Model",
    "text": "Examining the Output for the Adopted Log-Linear Fitted Model\n\n# Coefficient-level output\ntidy(lmer.5, effects = \"fixed\")\n\n\n  \n\n\n\nThe fitted equation is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = 1.21 + 1.67\\bigg[\\ln(\\mathrm{Grade\\mbox{-}level}_{ij}-7)\\bigg]\n\\]\nInterpreting the coefficients,\n\nThe predicted average vocabulary score for 8th-grade students is 1.21. Remember that the centered value for 8th-grade is 0, which results in \\(1.21 + 1.67\\bigg[\\ln(1)\\bigg] = 1.21 + 1.67(0) = 1.21\\).\nSince we used the natural log, we can interpret the change in X as a percent change and the change in Y as \\(\\hat{\\beta_1}/100\\); Each one-percent difference in grade-level is associated with a 0.0166-point difference in vocabulary score, on average.\n\nLooking at the variance components:\n\n# SD\ntidy(lmer.5, effects = \"ran_pars\")\n\n\n  \n\n\n# Compute variance components\n1.790 ^ 2 #Between-student\n\n[1] 3.2041\n\n0.907 ^ 2 #Within-student\n\n[1] 0.822649\n\n\nComparing them to the unconditional random intercepts model:\nWithin-Student Variance (\\(\\hat\\sigma^2_{\\epsilon}\\))\nThe unexplained variance that is within-students has decreased from 1.82 to 0.82, a 54.9% explanation of the within-student variation.\nBetween-Student Variance (\\(\\hat\\sigma^2_{0}\\))\nThe unexplained variance that is between-students has increased from 2.96 to 3.20, an increase of 8.1% in the unexplained between-students variation.\nThese values are quite similar to the variance components obtained from the other unconditional growth models since time (grade level) is the only effect included in the model."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#plot-of-the-unconditional-growth-model",
    "href": "notes/13-lmer-average-change-over-time.html#plot-of-the-unconditional-growth-model",
    "title": "LMER: Average Change Over Time",
    "section": "Plot of the Unconditional Growth Model",
    "text": "Plot of the Unconditional Growth Model\nTo better understand the relationship between grade-level and vocabulary score represented in the adopted unconditional growth model, we can plot the predicted mean profile based on the model’s fixed-effects.\n\n# Create plot\nggplot(data = vocabulary_long, aes(x = I(grade_quant-8), y = vocab_score)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {1.21 + 1.67 * log(x + 1)},\n    color = \"blue\"\n    ) +\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade-level\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"0\\n(8th)\", \"1\\n(9th)\", \"2\\n(10th)\", \"3\\n(11th)\")\n    ) +\n  ylab(\"Vocabulary score\")\n\n\n\n\n\n\n\nFigure 6: Predicted change in vocabulary score as a function of grade-level."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#student-specific-profiles-1",
    "href": "notes/13-lmer-average-change-over-time.html#student-specific-profiles-1",
    "title": "LMER: Average Change Over Time",
    "section": "Student-Specific Profiles",
    "text": "Student-Specific Profiles\nObtaining the coefficients for the student specific growth curves we get:\n\n# Get student estimated parameters\ntidy(lmer.5, effects = \"ran_coefs\") |&gt;\n  arrange(as.numeric(level))\n\n\n  \n\n\n\nThe fitted equations for the student-specific growth curves for Student 1 and Student 2 are:\n\\[\n\\begin{split}\n\\mathbf{Student~1:~} \\hat{\\mathrm{Vocabulary~Score}_{ij}} &= 1.60 + 1.67\\bigg[\\ln(\\mathrm{Grade\\mbox{-}level}_{ij}-7)\\bigg]\\\\[1ex]\n\\mathbf{Student~2:~} \\hat{\\mathrm{Vocabulary~Score}_{ij}} &= 1.00 + 1.67\\bigg[\\ln(\\mathrm{Grade\\mbox{-}level}_{ij}-7)\\bigg]\n\\end{split}\n\\]\nAgain, because we only included a random-effect of intercept the student-specific growth rates are identical for all students (i.e., the slope value is the same in all the equations), and also is the same as the average global growth rate. The random-effect of intercept does allow students to have different intercept values (i.e., students’ initial vocabulary score in the 8th grade can differ).\nWe can see these same things by plotting these student specific growth curves along with the global average growth curve.\n\n# Create plot\nggplot(data = vocabulary_long, aes(x = I(grade_quant-8), y = vocab_score)) +\n  geom_point(alpha = 0) +\n  # Average growth curve\n  geom_function(\n    fun = function(x) {1.21 + 1.67 * log(x + 1)},\n    color = \"black\"\n    ) +\n  # Student 1\n  geom_function(\n    fun = function(x) {1.60 + 1.67 * log(x + 1)},\n    color = \"#0072B2\"\n    ) +\n  # Student 2\n  geom_function(\n    fun = function(x) {1.00 + 1.67 * log(x + 1)},\n    color = \"#E69F00\"\n    ) +\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade-level\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"0\\n(8th)\", \"1\\n(9th)\", \"2\\n(10th)\", \"3\\n(11th)\")\n    ) +\n  ylab(\"Vocabulary score\")\n\n\n\n\n\n\n\nFigure 7: Predicted change in vocabulary score over time based on the log-linear unconditional growth model for all students (solid, black line), Student 1 (blue, dashed line), and Student 2 (orange, dashed line).\n\n\n\n\n\nThe parallel curves suggest the same rate-of-growth across students."
  },
  {
    "objectID": "notes/13-lmer-average-change-over-time.html#footnotes",
    "href": "notes/13-lmer-average-change-over-time.html#footnotes",
    "title": "LMER: Average Change Over Time",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnfortunately RM-ANOVA is still used, despite many known methodological limitations of the methodology, and the availability of better analytic options (e.g., LMER).↩︎\nThey are referred to as ‘left’ and ‘right’ because of where they appear in the syntax. If we wrote the syntax without the pipe it would be left_join(vocabulary_long, lookup_table, by = \"grade\"). In the syntax, vocabulary_long is to the left of lookup_table.↩︎"
  },
  {
    "objectID": "notes/11-more-logistic-regression.html",
    "href": "notes/11-more-logistic-regression.html",
    "title": "More Logistic Regression",
    "section": "",
    "text": "In this set of notes, you will expand your knowledge of the logistic regression model to include categorical predictors, non-linear predictor effects, covariates, and interaction terms. We will again use data from the file graduation.csv to explore predictors of college graduation.\n\nCSV File\nData Codebook\n\nWithin the analysis, we will focus on the research question of whether the probability of obtaining a degree differs for first and non-first generation students. We will use information criteria as the framework of evidence to adopt different models, along with an evaluation of the models’ residuals.\n\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom)\nlibrary(corrr)\nlibrary(patchwork)\nlibrary(performance)\nlibrary(texreg)\nlibrary(tidyverse)\n\n# Read in data and create dummy variable for categorical variables\ngrad = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/graduation.csv\") |&gt;\n  mutate(\n    got_degree = if_else(degree == \"Yes\", 1, 0),\n    is_firstgen = if_else(first_gen == \"Yes\", 1, 0),\n    is_nontrad = if_else(non_traditional == \"Yes\", 1, 0),\n  )\n\n# View data\ngrad\n\n\n  \n\n\n# Fit models from previous notes\nglm.0 = glm(got_degree ~ 1, data = grad, family = binomial(link = \"logit\"))\nglm.1 = glm(got_degree ~ 1 + act, data = grad, family = binomial(link = \"logit\"))"
  },
  {
    "objectID": "notes/11-more-logistic-regression.html#contingency-tables",
    "href": "notes/11-more-logistic-regression.html#contingency-tables",
    "title": "More Logistic Regression",
    "section": "Contingency Tables",
    "text": "Contingency Tables\nThe primary method of displaying summary information when both the predictor and outcome are categorical is via a contingency table. In our two-way table, we will display counts from the four different combinations of first generation and degree obtainment status. Since each of those variables includes two levels (degree/no degree and first gen/non-first gen), this is referred to as a 2x2 table; each number refers to the number of levels in one of the variables being displayed.\n\n\nCode\ntab_01 = data.frame(\n  first_gen = c(\"No\", \"Yes\", \"Total\"),\n  no = c(267, 360, 627),\n  yes = c(440, 1277, 1717),\n  total = c(707, 1637, 2344)\n)\n\n# Create table\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    first_gen = md(\"*First Generation Status*\"),\n    no = md(\"No\"),\n    yes = md(\"Yes\"),\n    total = md(\"Total\")\n  ) |&gt;\n  cols_align(\n    columns = c(first_gen),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(no, yes, total),\n    align = \"center\"\n  ) |&gt;\n  tab_spanner(\n    label = md(\"*Obtained Degree*\"),\n    columns = c(no, yes)\n  ) |&gt;\n  tab_style(\n        style = cell_text(align = \"left\", indent = px(20)),\n        locations = cells_body(\n          columns = first_gen,\n          rows = c(1, 2)\n        )\n    )\n\n\n\n\nTable 1: 2x2 table of counts for degree obtainment by first generation status.\n\n\n\n\n\n\n\n\n\n\nFirst Generation Status\nObtained Degree\nTotal\n\n\nNo\nYes\n\n\n\n\nNo\n267\n440\n707\n\n\nYes\n360\n1277\n1637\n\n\nTotal\n627\n1717\n2344\n\n\n\n\n\n\n\n\n\n\n\nThere are three types of counts (sample sizes) that are typically included in contingency tables:\n\nCell Counts: The “cells” in the two-way table include counts for the four combinations of first generation and degree obtainment status. The counts in those four cells are referred to as cell counts or cell sample sizes. These are generally notated symbolically as \\(n_{i,j}\\) where i indicates the row and j indicates the column. For example, \\(n_{1,2}=440\\) indicates the number of non-first generation students who obtained a degree.\nMarginal Counts: The counts in the “Total” row and column are called the marginal counts, or marginal sample sizes. They indicate the total counts collapsing across either rows or columns. These are generally notated symbolically as \\(n_{\\bullet,j}\\) or \\(n_{i, \\bullet}\\) where the dot indicates that we are collapsing across either rows or columns. For example, \\(n_{2,\\bullet}=1637\\) indicates the number of first generation students.\nGrand Count: Finally, there is the cell that includes the grand total sample size. This is usually just denoted as N (or sometimes \\(n_{\\bullet,\\bullet}\\)). In our example, \\(n=2344\\), the total number of students in the sample.\n\nThe three different totals (row total, column total, grand total) means that we can compute three different proportions for each of the four cells. Each proportion has a different interpretation depending on which total is used in the denominator. For example, consider the cell that designates first generation students who obtained a degree (\\(n_{2,2}=1227\\)).\n\nThe grand total of 2344 indicates all student, so computing the proportion using that denominator, \\(1227/2344 = 0.523\\), indicates that 0.523 of all students are first generation who obtained a degree.\nThe row total of 1637 indicates the number of students who are first generation students, so computing the proportion using that denominator, \\(1227/1637 = 0.750\\), indicates that 0.750 of all first generation students obtained a degree.\nThe column total of 1717 indicates the number of students who obtained a degree, so computing the proportion using that denominator, \\(1227/1717 = 0.715\\), indicates that 0.715 of students who obtained a degree are first generation students."
  },
  {
    "objectID": "notes/11-more-logistic-regression.html#computing-proportions-order-in-group_by-matters",
    "href": "notes/11-more-logistic-regression.html#computing-proportions-order-in-group_by-matters",
    "title": "More Logistic Regression",
    "section": "Computing Proportions: Order in group_by() Matters",
    "text": "Computing Proportions: Order in group_by() Matters\nWe can compute proportions using the row and column totals in the denominator by using the same set of dplyr operations we used previously, namely grouping by the predictor and outcome, summarizing to compute the sample size, and then mutating to find the proportion. The key is the order in which we include the variables in group_by().\nIf we want to use the row totals (the number of first generation and non-first generation students) in the denominator, we need to include first_gen in the group_by() prior to degree. The computation of sum() in mutate() will continue to be based on the first generation grouping (group_by() is in effect until we explicitly use ungroup()).\n\n# Use first generations status totals in proportion\ngrad |&gt;\n  group_by(first_gen, degree) |&gt;\n  summarize(N = n()) |&gt;\n  mutate(\n    Prop = N / sum (N)\n    )\n\n\n  \n\n\n\nTo compute proprtions based on the number of students who obtained a degree and the number who did not obtain a degree, we need to include degree in the group_by() function prior to first_gen.\n\n# Use degree status totals in proportion\ngrad |&gt;\n  group_by(degree, first_gen) |&gt;\n  summarize(N = n()) |&gt;\n  mutate(\n    Prop = N / sum (N)\n    )\n\n\n  \n\n\n\nIf you want to base the proportions on the grand total, the order in group_by() doesn’t matter, but you need to explicitly ungroup() prior to computing the proportions.\n\n# Use grand total in proportion\ngrad |&gt;\n  group_by(degree, first_gen) |&gt;\n  summarize(N = n()) |&gt;\n  ungroup() |&gt;\n  mutate(\n    Prop = N / sum (N)\n    )\n\n\n  \n\n\n\n\nYou need to select the the denominator to compute your proportions based on your research questions. You also need to make sure that if you are using R (or another software) to compute proportions that the proportions you want are being outputted."
  },
  {
    "objectID": "notes/11-more-logistic-regression.html#back-to-the-rq",
    "href": "notes/11-more-logistic-regression.html#back-to-the-rq",
    "title": "More Logistic Regression",
    "section": "Back to the RQ",
    "text": "Back to the RQ\nWe wanted to know whether first generation students have a different predicted probability of obtaining a degree than non-first generation students. Thus, we want to use the row totals in our denominator. So we are comparing the proportion of all first generation students who got a degree to the proportion of all non-first generation students who got a degree.\n\n# Proportions to answer our RQ\ngrad |&gt;\n  group_by(first_gen, degree) |&gt;\n  summarize(N = n()) |&gt;\n  mutate(\n    Prop = N / sum (N)\n    ) |&gt;\n  ungroup() |&gt;\n  filter(degree == \"Yes\")\n\n\n  \n\n\n\nBased on the sample proportions, the proportion of first generation students who obtain a degree (0.780) is higher by 0.158 than the proportion of non-first generation students who obtain a degree (0.622). We could also plot this using a bar chart. The argument position=position_dodge() creates the side-by-side plot. Otherwise the plot would be a stacked bar chart which gives the same information.\n\n# Side-by-side bar chart\ngrad |&gt;\n  group_by(first_gen, degree) |&gt;\n  summarize(N = n()) |&gt;\n  mutate(\n    Prop = N / sum (N)\n    ) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = first_gen, y = Prop, fill = degree)) +\n    geom_bar(stat = \"Identity\", position = position_dodge()) +\n    theme_light() +\n    xlab(\"First Generation\") +\n    ylab(\"Proportion\") +\n    scale_fill_manual(\n      name = \"Obtained\\n Degree\",\n      values = c(\"#2EC4B6\", \"#E71D36\")\n      )\n\n\n\n\nSide-by-side barplot showing the proportion of first generation and non-first generation students in the sample who obtained and did not obtain a degree.\n\n\n\n\nTo evaluate whether this sample difference is more than we expect because of chance, we could create a dummy-coded indicator for first generation status and use that as a predictor in our logistic regression model.\n\n# Dummy code first_gen\ngrad = grad |&gt;\n  mutate(\n    is_firstgen = if_else(first_gen == \"Yes\", 1, 0)\n  )\n\n# Fit  model\nglm.2 = glm(got_degree ~ 1 + is_firstgen, data = grad, family = binomial(link = \"logit\"))\n\n# Evaluate first generation predictor\naictab(\n  cand.set = list(glm.0, glm.2),\n  modnames = c(\"Intercept-Only\", \"Effect of First Gen\")\n)\n\n\n  \n\n\n\nGiven the data and candidate models, the model that includes first generation status in the model has more evidence. Since we are comparing to the intercept-only model in this output, We can also use the information in the residual deviance column to compute a pseudo-\\(R^2\\) value.\n\n# Compute pseudo R2\n(2722.6 - 2662.1) / 2722.6\n\n[1] 0.02222141\n\n\nThe pseudo-\\(R^2\\) value of 0.022 is close to 0, which suggests that first generation status although statistically relevant, is not incredibly predictive of the log-odds (or odds or probability) of obtaining a degree. Next we turn to the fitted equation:\n\ncoef(glm.2)\n\n(Intercept) is_firstgen \n  0.4995261   0.7666388 \n\n\nBased on this output, the fitted equation for the model is:\n\\[\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) = 0.50+ 0.77(\\mathrm{First~Generation}_i)\n\\]\nInterpreting these coefficients in the log-odds metric:\n\nNon-first generation students’ predicted log-odds of obtaining a degree is 0.50.\nFirst generation students’ predicted log-odds of obtaining a degree is 0.767 higher than non-first generation students.\n\nTo determine first generation students’ log-odds of obtaining a degree, we substitute 1 into the fitted equation:\n\\[\n\\begin{split}\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) &= 0.50+ 0.77(1) \\\\[1ex]\n&= 1.27\n\\end{split}\n\\]\nWe can also transform the fitted equation into the odds metric and interpret.\n\\[\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} = e^{0.50} \\times e^{0.77(\\mathrm{First~Generation}_i)}\n\\]\n\n# Transform coefficients to odds metric\nexp(coef(glm.2))\n\n(Intercept) is_firstgen \n   1.647940    2.152519 \n\n\nInterpreting these coefficients in the odds metric:\n\nNon-first generation students’ predicted odds of obtaining a degree is 1.65. That is their probability of obtaining a degree is 1.64 times that of not obtaining a degree.\nFirst generation students’ predicted odds of obtaining a degree is 2.15 times higher than non-first generation students’ odds of obtaining a degree.\n\nTo determine first generation students’ odds of obtaining a degree, we can either substitute 1 into the transformed fitted equation:\n\\[\n\\begin{split}\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) &= e^{0.50} \\times e^{0.77(1)} \\\\[1ex]\n&= 3.56\n\\end{split}\n\\]\nOr we could have transformed their log-odds directly (\\(e^{1.27}=3.56\\)). Finally, we could also transform the equation into the probability metric.\n\\[\n\\hat\\pi_i = \\frac{e^{0.50+ 0.77(\\mathrm{First~Generation}_i)}}{1 + e^{0.50+ 0.77(\\mathrm{First~Generation}_i)}}\n\\]\n\n# Prob non-first gen students\nexp(0.50) / (1 + exp(0.50))\n\n[1] 0.6224593\n\n# Prob first gen students\nexp(0.50 + 0.77) / (1 + exp(0.50 + 0.77))\n\n[1] 0.7807427\n\n\nIn the probability metric we just compute the predicted probabilities and report/compare them. There is no easy conversion of the slope coefficient to interpret how much higher the probability of obtaining a degree for first generation students is than for non-first generation students."
  },
  {
    "objectID": "notes/11-more-logistic-regression.html#evaluating-assumptions",
    "href": "notes/11-more-logistic-regression.html#evaluating-assumptions",
    "title": "More Logistic Regression",
    "section": "Evaluating Assumptions",
    "text": "Evaluating Assumptions\nWhen the only predictor in a logistic model is categorical, the only thing we need to pay attention to is whether the average residual for each level of the predictor is close to 0. We can again use the binned_residuals() function to evaluate this. When the predictor is categorical, the binning will be done on the levels of the predictor.\n\n# Obtain average fitted values and average residuals\nout.2 = binned_residuals(glm.2)\n\n# View data frame\nas.data.frame(out.2)\n\n\n\n\n\n\n\n\nFigure 1: Binned residuals versus the binned fitted values for the model that includes the effect of first generation status to predict the log-odds of obtaining a degree.\n\n\n\n\nHere we see the average residual (ybar) for both groups is near zero. This satisfies the linearity assumption. If you examined the binned residual plot, you would also see this, albeit in a graphical form.\nThe results from glm.2 indicated that there were differences between first and non-first generation students in the probability of obtaining a degree. Based on this model’s result, we found that the predicted probability of obtaining a degre for first generation students was 0.780 while that for non-first generation students was 0.622. Does this difference persist after controlling for other covariates?\nTo examine this, we want to fit a set of models that systematically include potential covariates along with first generation status. Our strategy for this is to start with the most important covariate. We can compare the model that includes first generation status and this covariate to a model that just includes first generation status. If the evidence indicates that the model with only first generation status is important we are done. If the evidence points toward the model that also includes the most important covariate, then we will continue by comparing that model to one that includes first generation status and the two most important covariates. We will continue with these comparisons until we adopt a final model or models.\nTo determine the order of importance, we can compute the correlations between each covariate and the dummy-coded outcome.\n\ngrad |&gt;\n  select(got_degree, act, scholarship, ap_courses,is_nontrad) |&gt;\n  correlate()\n\n\n  \n\n\n\nThe rank ordering of covariates (based on the absolute value of the correlation coefficients) will be:\n\nACT scores (\\(r = 0.195\\), most important)\nNumber of AP courses (\\(r = 0.149\\))\nScholarship amount (\\(r = 0.142\\))\nNon-traditional status (\\(r = -0.084\\))"
  },
  {
    "objectID": "notes/11-more-logistic-regression.html#quadratic-effect-of-act",
    "href": "notes/11-more-logistic-regression.html#quadratic-effect-of-act",
    "title": "More Logistic Regression",
    "section": "Quadratic Effect of ACT",
    "text": "Quadratic Effect of ACT\nWe will start our interpretation by examining the coef() output.\n\ncoef(glm.1_quad)\n\n (Intercept)          act     I(act^2) \n-4.566663084  0.365752506 -0.005464371 \n\n\nThe fitted model is:\n\\[\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) = -4.57 + 0.37(\\mathrm{ACT}_i) - 0.005(\\mathrm{ACT}_i^2)\n\\]\nSince this is an interaction model, we interpret the effect of ACT as: The effect of ACT on the log-odds of obtaining a degree depends on ACT. (We could also graph this effect if we were interested in the effect of ACT on the log-odds of obtaining a degree.)\nTransforming the fitted equation to the odds metric:\n\\[\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} = e^{-4.57} \\times e^{0.37(\\mathrm{ACT}_i)} \\times e^ {-0.005(\\mathrm{ACT}_i^2)}\n\\]\nBecause changing ACT now impacts two exponent terms, one of which includes the quadratic of ACT, it is super essential that we create a plot to interpret the effects of ACT on the odds of obtaining a degree.\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-4.57 + 0.37*x - 0.005*x^2)}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted odds of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 4: Predicted odds of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe odds curve is somewhat “S”-shaped (although it is not constrained between 0 and 1 like the probability curve). This is because of the quadratic effect of ACT. The overall trend is one of exponential growth, although the rate-of-change is not constant within this curve. The most rapid change in the odds of obtaining a degree occurs at ACT scores between 20 and 30.\nFinally, we could also transform the equation into the probability metric. The fitted equation is:\n\\[\n\\hat\\pi_i = \\frac{e^{-4.57 + 0.37(\\mathrm{ACT}_i) - 0.005(\\mathrm{ACT}_i^2)}}{1 + e^{-4.57 + 0.37(\\mathrm{ACT}_i) - 0.005(\\mathrm{ACT}_i^2)}}\n\\]\nAgain, it is worthwhile to create a plot to interpret the effects of ACT on the probability of obtaining a degree.\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-4.57 + 0.37*x - 0.005*x^2) / (1 + exp(-4.57 + 0.37*x - 0.005*x^2))}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted probability of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 5: Predicted probability of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe probability curve is “S”-shaped and constrained between 0 and 1. The left-hand side of the “S” is compressed, probably due to the range of ACT scores. ACT has the largest effect on the probability of obtaining a degree for scores less than 25, and then the effect begins to plateau (although it is still growing as the logistic curve is monotonic)."
  },
  {
    "objectID": "notes/11-more-logistic-regression.html#log-linear-effect-of-act",
    "href": "notes/11-more-logistic-regression.html#log-linear-effect-of-act",
    "title": "More Logistic Regression",
    "section": "Log-Linear Effect of ACT",
    "text": "Log-Linear Effect of ACT\nWe will again start our interpretation by examining the coef() output.\n\ncoef(glm.1_log)\n\n(Intercept)    log(act) \n  -6.938715    2.500968 \n\n\nThe fitted model is:\n\\[\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) = -6.94 + 2.50\\bigg[\\ln(\\mathrm{ACT}_i)\\bigg]\n\\]\nWe interpret the effect of ACT as: Each 1% difference in ACT score is associated with a .025-unit change in the log-odds of obtaining a degree.\nTransforming the fitted equation to the odds metric:\n\\[\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} = e^{-6.94} \\times e^{2.50\\bigg[\\ln(\\mathrm{ACT}_i)\\bigg]}\n\\]\nIt is again easier to interpret the effects of ACT on the odds of obtaining a degree by creating a plot .\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-6.94 + 2.50*log(x))}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted odds of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 6: Predicted odds of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe effect of ACT on the probability of obtaining a degree is positive and non-linear (exponential growth). The predicted odds of obtaining a degree increases exponentially for higher ACT scores.\nFinally, we could also transform the equation into the probability metric. The fitted equation is:\n\\[\n\\hat\\pi_i = \\frac{e^{-6.94 + 2.50\\bigg[\\ln(\\mathrm{ACT}_i)\\bigg]}}{1 + e^{-6.94 + 2.50\\bigg[\\ln(\\mathrm{ACT}_i)\\bigg]}}\n\\]\nAgain, it is worthwhile to create a plot to interpret the effects of ACT on the probability of obtaining a degree.\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-6.94 + 2.50*log(x)) / (1 + exp(-6.94 + 2.50*log(x)))}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted probability of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 7: Predicted probability of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe effect of ACT on the probability of obtaining a degree is positive and non-linear. ACT has a larger effect on the probability of obtaining a degree for smaller ACT scores and this effect, while still positive, diminishes for higher ACT scores.\n\nOnce you include non-linear effects in the model, always create a plot of the fitted equation to try and interpret odds or probabilities."
  },
  {
    "objectID": "notes/11-more-logistic-regression.html#back-transforming-to-odds",
    "href": "notes/11-more-logistic-regression.html#back-transforming-to-odds",
    "title": "More Logistic Regression",
    "section": "Back-Transforming to Odds",
    "text": "Back-Transforming to Odds\nIf we back-transform the coefficients to facilitate interpretations using the odds metric, the fitted equation is:\n\\[\n\\begin{split}\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-5.91 + 0.51(\\mathrm{First~Generation}_i) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[1ex]\n&=e^{-5.91} \\times e^{0.51(\\mathrm{First~Generation}_i)} \\times e^{2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}\n\\end{split}\n\\]\nWe can interpret the intercept and the effect of first generation status directly:\n\nNon-first generations students with an ACT score of 1 have a predicted odds of graduating of \\(e^{-5.91}=.003\\).\nFirst generation college students, have a predicted odds of graduating that is \\(e^{0.51}=1.66\\) times higher than that for non-first generations students, after controlling for differences in ACT scores.\nEach one-point difference in ACT score is associated with improving the odds of graduating 1.09 times, on average, after controlling for whether or not the students are first generation college students.\n\nWhile it is not pertinent to the research question, if you wanted to interpret the effect of ACT, you could plot the fitted model. As always with main effects models, we find the fitted equation for first generation and non-first generation students, and plot the two curves separately using multiple geom_function() layers.\n\\[\n\\begin{split}\n\\mathbf{Non\\mbox{-}First~Generation:}\\quad\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-5.91 + 0.51(0) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[1ex]\n&= e^{-5.91 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[5ex]\n\\mathbf{First~Generation:}\\quad\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-5.91 + 0.51(1) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[1ex]\n&= e^{-5.40 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[1ex]\n\\end{split}\n\\]\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  # Non-first generation students\n  geom_function(\n    fun = function(x) {exp(-5.91 + 2.07*log(x))},\n    linetype = \"dashed\",\n    color = \"blue\"\n    ) +\n  # First generation students\n  geom_function(\n    fun = function(x) {exp(-5.40 + 2.07*log(x))},\n    linetype = \"solid\",\n    color = \"red\"\n    )+\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted odds of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 8: Predicted odds of obtaining a degree as a function of ACT score first generation (solid, red line) and non-first generation (dashed, blue line) students.\n\n\n\n\n\nHere we see that the odds of graduating increase exponentially at higher ACT scores for both first generation and non-first generation students, on average. This rate of increase, however, is higher for first generation students. Moreover, first generation students have higher odds of graduating than non-first generation students, regardless of ACT score."
  },
  {
    "objectID": "notes/11-more-logistic-regression.html#back-transforming-to-probability",
    "href": "notes/11-more-logistic-regression.html#back-transforming-to-probability",
    "title": "More Logistic Regression",
    "section": "Back-Transforming to Probability",
    "text": "Back-Transforming to Probability\nAgain, while not related to the research question, we can also plot the predicted probability of graduating as a function of ACT score. Algebraically manipulating the fitted equation,\n\\[\n\\hat\\pi_i = \\frac{e^{-5.91 + 0.51(\\mathrm{First~Generation}_i) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.91 + 0.51(\\mathrm{First~Generation}_i) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}\n\\]\nWe can then produce the fitted equations for non-first generation and first generation students by substituting either 0 or 1, respectively, into the firstgen variable. These equations are:\nNon-First Generation Students\n\\[\n\\begin{split}\n\\hat\\pi_i &= \\frac{e^{-5.91 + 0.51(0) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.91 + 0.51(0) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}} \\\\[3ex]\n&= \\frac{e^{-5.91 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.91 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}\n\\end{split}\n\\]\nFirst Generation Students\n\\[\n\\begin{split}\n\\hat\\pi_i &= \\frac{e^{-5.91 + 0.51(1) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.91 + 0.51(1) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}} \\\\[1em]\n&= \\frac{e^{-5.40 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.40 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}\n\\end{split}\n\\]\nWe can include each of these in a geom_function() layer in our plot.\n\n# Plot the fitted equations\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  # Non-first generation students\n  geom_function(\n    fun = function(x) {exp(-5.91 + 2.07*log(x)) / (1 + exp(-5.91 + 2.07*log(x)))},\n    linetype = \"dashed\",\n    color = \"blue\"\n    ) +\n  # First generation students\n  geom_function(\n    fun = function(x) {exp(-5.40 + 2.07*log(x)) / (1 + exp(-5.40 + 2.07*log(x)))},\n    linetype = \"solid\",\n    color = \"red\"\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted probability of obtaining a degree\") +\n  ylim(0, 1)\n\n\n\n\n\n\n\nFigure 9: Predicted probabilities of obtaining a degree as a function of ACT score first generation (solid, red line) and non-first generation (dashed, blue line) students.\n\n\n\n\n\nHere we see that the probability of graduatingobtaining a degree is positively associated with ACT score for both first generation and non-first generation students. The magnitude of the effect of ACT depends on ACT score for both groups. Although first generation students have higher probability of obtaining a degree than non-first generation students regardless of ACT score, the magnitude of this difference decreases at higher ACT scores."
  },
  {
    "objectID": "notes/11-more-logistic-regression.html#footnotes",
    "href": "notes/11-more-logistic-regression.html#footnotes",
    "title": "More Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnother analyst might choose to use the non-traditional group or to show the effect by displaying both groups, perhaps in different panels.↩︎"
  },
  {
    "objectID": "notes/10-logistic-regression.html",
    "href": "notes/10-logistic-regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "In this set of notes, you will learn how to use logistic regression models to model dichotomous categorical outcome variables (e.g., dummy coded outcome). We will use data from the file graduation.csv to explore predictors of college graduation.\n\nCSV File\nData Codebook\n\n\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom)\nlibrary(corrr)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(performance)\nlibrary(tidyverse)\n\n# Read in data and create dummy variable for outcome\ngrad = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/graduation.csv\") |&gt;\n  mutate(\n    got_degree = if_else(degree == \"Yes\", 1, 0)\n  )\n\n# View data\ngrad\n\n\n  \n\n\n\nWe will also examine the empirical proportions of students who obtain a degree at different ACT scores to remind us about the relationship we will be modeling.\n\n# Obtain the proportion who obtain a degree for each ACT score\nprop_grad = grad |&gt; \n  group_by(act, degree) |&gt; \n  summarize(N = n()) |&gt; \n  mutate(\n    Prop = N / sum (N)\n    ) |&gt;\n  ungroup() |&gt;\n  filter(degree == \"Yes\")\n\n# Scatterplot\nggplot(data = prop_grad, aes(x = act, y = Prop)) +\n  geom_point(aes(size = N)) +\n  geom_smooth(aes(weight = N), method = \"loess\", se = FALSE) +\n  theme_bw() +\n  xlab(\"ACT score\") +\n  ylab(\"Proportion who obtained a degree\")\n\n\n\n\n\n\n\nFigure 1: Proportion of students who obtain a degree conditional on ACT score. Size of the dot is proportional to sample size. The loess smoother is based on the raw data.\n\n\n\n\n\nBecause the sample sizes differs across ACT scores, we need to account for that by weighting the loess smoother. To do this we include aes(weight=) in the geom_smooth() layer.\nIn the last set of notes, we saw that using the linear probability model leads to direct violations of the linear model’s assumptions. If that isn’t problematic enough, it is possible to run into severe issues when we make predictions. For example, given the constant effect of X in these models it is possible to have an X value that results in a predicted proportion that is either greater than 1 or less than 0. This is a problem since proportions are constrained to the range of \\(\\left[0,~1\\right]\\)."
  },
  {
    "objectID": "notes/10-logistic-regression.html#a-transformation-to-fit-the-s-shaped-curve",
    "href": "notes/10-logistic-regression.html#a-transformation-to-fit-the-s-shaped-curve",
    "title": "Logistic Regression",
    "section": "A Transformation to Fit the “S”-shaped Curve",
    "text": "A Transformation to Fit the “S”-shaped Curve\nWe need to identify a mathematical function that relates X to Y in such a way that we produce this “S”-shaped curve. To do this we apply a transformation function, call it \\(\\Lambda\\) (Lambda), that fits the criteria for such a function (monotonic, nonlinear, maps to \\([0,~1]\\) space). There are several mathematical functions that do this. One commonly used function that meets these specifications is the logistic function. Mathematically, the logistic function is:\n\\[\n\\Lambda(x) = \\frac{1}{1 + e^{-x}}\n\\]\nwhere x is the value fed into the logistic function. For example, to logistically transform \\(x=3\\), we use\n\\[\n\\begin{split}\n\\Lambda(3) &= \\frac{1}{1 + e^{-3}} \\\\[1ex]\n&= 0.953\n\\end{split}\n\\]\nBelow we show how to transform many such values using R.\n\n# Create w values and transformed values\nexample = tibble(\n  x = seq(from = -4, to = 4, by = 0.01)  # Set up values\n  ) |&gt;\n  mutate(\n    Lambda = 1 / (1 + exp(-x))  # Transform using logistic function\n  )\n\n# View data\nexample\n\n\n\n\n\n\n\n\nFigure 3: Plot of the logistically transformed values for a sequence of values from -4 to 4.\n\n\n\n# Plot the results\nggplot(data = example, aes(x = x, y = Lambda)) +\n  geom_line() +\n  theme_light()\n\n\n\n\n\n\n\nFigure 4: Plot of the logistically transformed values for a sequence of values from -4 to 4.\n\n\n\n\n\nYou can see that by using this transformation we get a monotonic “S”-shaped curve. Now try substituting a really large value of x into the function. This gives an asymptote at 1. Also substitute a really “large”” negative value in for x. This gives an asymptote at 0. So this function also bounds the output between 0 and 1."
  },
  {
    "objectID": "notes/10-logistic-regression.html#re-expressing-a-logistic-transformation",
    "href": "notes/10-logistic-regression.html#re-expressing-a-logistic-transformation",
    "title": "Logistic Regression",
    "section": "Re-Expressing a Logistic Transformation",
    "text": "Re-Expressing a Logistic Transformation\nThe logistic model expresses the proportion of 1s (\\(\\pi_i\\)) as a function of the predictor \\(X\\). It can be mathematically expressed as:\n\\[\n\\pi_i = \\frac{1}{1 + e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]}}\n\\]\nWe can re-express this using algebra and rules of logarithms.\n\\[\n\\begin{split}\n\\pi_i &= \\frac{1}{1 + e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]}} \\\\[2ex]\n\\pi_i \\times (1 + e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]} ) &= 1 \\\\[2ex]\n\\pi_i + \\pi_i(e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]}) &= 1 \\\\[2ex]\n\\pi_i(e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]}) &= 1 - \\pi_i \\\\[2ex]\ne^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]} &= \\frac{1 - \\pi_i}{\\pi_i} \\\\[2ex]\ne^{\\big[\\beta_0 + \\beta_1(X_i)\\big]} &= \\frac{\\pi_i}{1 - \\pi_i} \\\\[2ex]\n\\ln \\bigg(e^{\\big[\\beta_0 + \\beta_1(X_i)\\big]}\\bigg) &= \\ln \\bigg( \\frac{\\pi_i}{1 - \\pi_i} \\bigg) \\\\[2ex]\n\\beta_0 + \\beta_1(X_i) &= \\ln \\bigg( \\frac{\\pi_i}{1 - \\pi_i}\\bigg)\n\\end{split}\n\\]\nOr,\n\\[\n\\ln \\bigg( \\frac{\\pi_i}{1 - \\pi_i}\\bigg) = \\beta_0 + \\beta_1(X_i)\n\\]\n\nThe logistic model expresses the natural logarithm of \\(\\frac{\\pi_i}{1 - \\pi_i}\\) as a linear function of X. Note that there is no error term on this model. This is because the model is for the mean structure only (the proportions), we are not modeling the actual \\(Y_i\\) values (i.e., the 0s and 1s) with the logistic regression model."
  },
  {
    "objectID": "notes/10-logistic-regression.html#back-transforming-to-odds",
    "href": "notes/10-logistic-regression.html#back-transforming-to-odds",
    "title": "Logistic Regression",
    "section": "Back-Transforming to Odds",
    "text": "Back-Transforming to Odds\nFor better interpretations, we can back-transform log-odds to odds. This is typically a better metric for interpretation of the coefficients. To back-transform to odds, we exponentiate both sides of the fitted equation and use the rules of exponents to simplify:\n\\[\n\\begin{split}\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) &= -1.61 + 0.11(\\mathrm{ACT~Score}_i) \\\\[4ex]\ne^{\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg)} &= e^{-1.61 + 0.11(\\mathrm{ACT~Score}_i)} \\\\[2ex]\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-1.61} \\times e^{0.11(\\mathrm{ACT~Score}_i)}\n\\end{split}\n\\]\nWhen ACT score = 0, the predicted odds of obtaining a degree are\n\\[\n\\begin{split}\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-1.61} \\times e^{0.11(0)} \\\\\n&= e^{-1.61} \\times 1 \\\\\n&= e^{-1.61} \\\\\n&= 0.2\n\\end{split}\n\\]\nThe odds of obtaining a degree for students with an ACT score of 0 are 0.2. That is, for these students, the probability of obtaining a degree is 0.2 times that of not obtaining a degree (Although it is extrapolating, it is far more likely these students will not obtain their degree!)\nTo interpret the effect of ACT on the odds of obtaining a degree, we will compare the odds of obtaining a degree for students that have ACT score that differ by one point. Say ACT = 0 and ACT = 1.\nWe already know the predicted odds for students with ACT = 0, namely \\(e^{-1.61}\\). For students with an ACT of 1, their predicted odds of obtaining a degree are:\n\\[\n\\begin{split}\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-1.61} \\times e^{0.11(1)} \\\\\n&= e^{-1.61} \\times e^{0.11} \\\\\n\\end{split}\n\\]\nThese students odds of obtaining a degree are \\(e^{0.11}\\) times greater than students with an ACT score of 0. Moreover, this increase in the odds, on average, is the case for every one-point difference in ACT score. In general,\n\nThe predicted odds for \\(X=0\\) are \\(e^{\\hat\\beta_0}\\).\nEach one-unit difference in \\(X\\) is associated with a \\(e^{\\hat\\beta_1}\\) times increase (decrease) in the odds.\n\nWe can obtain these values in R by using the coef() function to obtain the fitted model’s coefficients and then exponentiating them using the exp() function.\n\nexp(coef(glm.1))\n\n(Intercept)         act \n  0.1997102   1.1135342 \n\n\n\nFrom these values, we interpret the coefficients in the odds metric as:\n\nThe predicted odds of obtaining a degree for students with an ACT score of 0 are 0.20.\nEach one-point difference in ACT score is associated with 1.11 times greater odds of obtaining a degree.\n\n\nTo even further understand and interpret the fitted model, we can plot the predicted odds of obtaining a degree for a range of ACT scores. Recall, the general fitted equation for the logistic regression model is written as:\n\\[\n\\ln\\bigg[\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg] = \\hat\\beta_0 + \\hat\\beta_1(x_i)\n\\]\nWe need to predict odds rather than log-odds on the left-hand side of the equation. To do this we exponentiate both sides of the equation:\n\\[\n\\begin{split}\ne^{\\ln\\bigg[\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg]} &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} \\\\[1em]\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}\n\\end{split}\n\\]\nWe include the right-side of this in the argument fun= of the geom_function() layer, substituting in the values for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). Below we plot the results from our fitted logistic model.\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-1.611 + 0.108*x)}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted odds of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 6: Predicted odds of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe monotonic increase in the curve indicates the positive effect of ACT score on the odds of obtaining a degree. The exponential growth curve indicates that students with higher ACT scores have increasingly higher odds of obtaining a degree."
  },
  {
    "objectID": "notes/10-logistic-regression.html#back-transforming-to-probability",
    "href": "notes/10-logistic-regression.html#back-transforming-to-probability",
    "title": "Logistic Regression",
    "section": "Back-Transforming to Probability",
    "text": "Back-Transforming to Probability\nWe can also back-transform from odds to probability. To do this, we will again start with the logistic fitted equation and use algebra to isolate the probability of obtaining a degree (\\(\\pi_i\\)) on the left-hand side of the equation.\n\\[\n\\begin{split}\n\\ln\\bigg[\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg] &= \\hat\\beta_0 + \\hat\\beta_1(x_i) \\\\[1em]\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} \\\\[1em]\n\\hat\\pi_i &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} (1 - \\hat\\pi_i) \\\\[1em]\n\\hat\\pi_i &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} - e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}(\\hat\\pi_i) \\\\[1em]\n\\hat\\pi_i + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}(\\hat\\pi_i) &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} \\\\[1em]\n\\hat\\pi_i(1 + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}) &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} \\\\[1em]\n\\hat\\pi_i &= \\frac{e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}{1 + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}} \\\\[1em]\n\\hat\\pi_i &= \\frac{e^{\\hat Y_i}}{1 + e^{\\hat Y_i}}\n\\end{split}\n\\]\nThat is, to obtain the probability of obtaining a degree, we can transform the fitted values (i.e., the predicted log-odds) from the logistic model. For example, the intercept from the logistic fitted equation, \\(-1.61\\) was the predicted log-odds of obtaining a degree for students with an ACT of 0. To obtain the predicted probability of obtaining a degree for students with an ACT of 0:\n\nexp(-1.61) / (1 + exp(-1.61))\n\n[1] 0.1665886\n\n\nFor students with ACT of 0, the predicted probability of obtaining a degree is 0.17.\nThis transformation from log-odds to probability, is non-linear, which means that there is not a clean interpretation of the effects of ACT (i.e., the slope) on the probability of obtaining a degree To understand this effect we can plot the probability of obtaining a degree across the range of ACT scores. To do this, we use geom_function() and input the transformation to probability with the fitted equation:\n\\[\n\\hat\\pi_i = \\frac{e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}{1 + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}\n\\]\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-1.611 + 0.108*x) / (1 + exp(-1.611 + 0.108*x))}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted probability of obtaining a degree\") +\n  ylim(0, 1)\n\n\n\n\n\n\n\nFigure 7: Predicted probability of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe effect of ACT on the probability of obtaining a degree follows a monotonic increasing “S”-curve. While there is always an increasing effect of ACT on the probability of obtaining a degree, the magnitude of this effect depends on ACT score. For lower ACT scores there is a larger effect of ACT score on the probability of obtaining a degree than for higher ACT scores.\nOne interesting point on the plot is the ACT score where the probability of obtaining a degree is 0.5. In our example, this is at an ACT score of approximately 15. This implies that students who score less than 15 are more likely to not obtain a degree than to obtain a degree (on average), and those that score higher than 15 are more likely to obtain a degree than not (on average).\n\n\nRough Interpretation of the Slope\nOne rough interpretation that is often used is to divide the slope coefficient by 4 to get an upper bound of the predictive difference in probability of Y per unit increase in X. In our example, each one-point difference in ACT score is associated with, at most, a \\(0.108/4=.027\\) difference in the probability of obtaining a degree, on average.\n\nThe mathematics behind this rough interpretation is based on maximizing the rate-of-change, which is based on setting the first derivative of the logistic function to zero and solving. Recall that the logistic function relates the probabilities to the fitted equation as:\n\\[\n\\hat\\pi_i = \\frac{e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}{1 + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}\n\\]\nThe first derivative, with respect to x is:\n\\[\n\\frac{\\hat\\beta_1e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}{\\bigg[e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} + 1\\bigg]^2}\n\\]\nThis is maximized when \\(\\hat\\beta_0 + \\hat\\beta_1(x_i)=0\\), which means we can substitute 0 into the derivative to determine the maximum rate-of-change:\n\\[\n\\begin{split}\n&= \\frac{\\hat\\beta_1e^{0}}{\\bigg[e^{0} + 1\\bigg]^2} \\\\[1ex]\n&= \\frac{\\hat\\beta_1}{\\bigg[1 + 1\\bigg]^2} \\\\[1ex]\n&= \\frac{\\hat\\beta_1}{4} \\\\[1ex]\n\\end{split}\n\\]\nThus the maximum rate-of-change is the based on the slope coefficient divided by four."
  },
  {
    "objectID": "notes/10-logistic-regression.html#footnotes",
    "href": "notes/10-logistic-regression.html#footnotes",
    "title": "Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe logistic regression model is from a family of models referred to as Generalized Linear Regression models. The General Linear Model (i.e., fixed-effects regression model) is also a member of the Generalized Linear Model family.↩︎"
  },
  {
    "objectID": "notes/08-logarithmic-transformations-predictor.html",
    "href": "notes/08-logarithmic-transformations-predictor.html",
    "title": "Log-Transforming the Predictor",
    "section": "",
    "text": "In this set of notes, you will learn another method of dealing with nonlinearity. Specifically, we will look at log-transforming the predictor in a linear model. To do so, we will use the mn-schools.csv dataset:\n\nCSV File\nData Codebook\n\nOur goal will be to re-visit if (and how) academic “quality” of the student-body (measured by SAT score) is related to institutional graduation rate. Within this work, we will use information theoretic approcaches (namely the AICc and related measures) to evaluate any fitted models.\n\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom)\nlibrary(educate)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Read in data\nmn = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/mn-schools.csv\")\n\n# View data\nmn"
  },
  {
    "objectID": "notes/08-logarithmic-transformations-predictor.html#logarithmic-function-yet-another-non-linear-function",
    "href": "notes/08-logarithmic-transformations-predictor.html#logarithmic-function-yet-another-non-linear-function",
    "title": "Log-Transforming the Predictor",
    "section": "Logarithmic Function: Yet Another Non-Linear Function",
    "text": "Logarithmic Function: Yet Another Non-Linear Function\nOne way to model this non-linearity is to fit a model that included a quadratic polynomial effect. Remember that the quadratic function is ‘U’-shaped. In our example, the quadratic term was negative, which creates an upside-down ‘U’-shape. This negative quadratic function is shown in the left-had side of the figure below.\nWe do not have to use a quadratic function to model curvilinear relationships. We can use any mathematical function that mimics the curvilinear pattern observed in the data.1 One mathematical function that is useful for modeling curvilinearity is the logarithmic function. The logarithmic function is shown in the right-hand side of the figure below.\n\n\nCode\nfig_01 = data.frame(\n  x = seq(from = -5, to = 3, by = 0.1)\n  ) |&gt;\n  mutate(y =  -(x^2))\n\np1 = ggplot(data = fig_01, aes(x = x, y = y)) +\n    geom_line() +\n    theme_light() +\n  scale_x_continuous(name = \"X\", breaks = NULL) +\n  scale_y_continuous(name = \"Y\", breaks = NULL) +\n  ggtitle(\"Quadratic Function\")\n\n\nfig_02 = data.frame(\n  x = seq(from = 0.1, to = 5, by = 0.1)\n  ) |&gt;\n  mutate(y =  log2(x))\n\np2 = ggplot(data = fig_02, aes(x = x, y = y)) +\n    geom_line() +\n    theme_light() +\n  scale_x_continuous(name = \"X\", breaks = NULL) +\n  scale_y_continuous(name = \"Y\", breaks = NULL) +\n  ggtitle(\"Logarithmic Function\")\n\n# Layout the plots\np1 | p2\n\n\n\n\n\n\n\n\nFigure 2: LEFT: Negative quadratic function of X. RIGHT: Logarithmic function of X.\n\n\n\n\n\nThese two functions have some similarities. They both model non-linear growth (continuous and diminishing growth). However, in the quadratic function, this growth eventually peaks and then is followed by continuous and increasing loss (parabola; the function changes direction). The logarithmic function, on the other hand, does not change direction—it continues to grow, albeit ever diminishing in the amount of growth.2\nAs you consider different mathematical functions, it is important to consider the substantive knowledge in an area. For example, while empirically both the quadratic polynomial model and the logarithmic model might both suggest reasonable fit to the data, in this context, the logarithmic model might be a more substantively sound model than the quadratic polynomial model. We would probably expect that the effect of SAT on graduation rate would diminish for schools with higher median SAT scores, but that it wouldn’t actually change direction. (This would mean that at some value, an increase in SAT would be associated with a lower graduation rate!)"
  },
  {
    "objectID": "notes/08-logarithmic-transformations-predictor.html#fitting-a-logarithmic-function-using-lm",
    "href": "notes/08-logarithmic-transformations-predictor.html#fitting-a-logarithmic-function-using-lm",
    "title": "Log-Transforming the Predictor",
    "section": "Fitting a Logarithmic Function Using lm()",
    "text": "Fitting a Logarithmic Function Using lm()\nThe mathematical expression of the logarithmic function is:\n\\[\nY = \\log(X)\n\\]\nThis implies that, to fit a logarithmic function we need to log-transform the predictor and use that in the lm() function. (Note that we do not transform the Y-value.) The choice of base for the logarithm is irrelevant statistically, but does impact the interpretation of the slope. To illustrate, we will examine fitting models using base-2, base-10, and base-e. Rather than mutating the log-transformed variables into the data, we will include the log-transformation directly in the lm() function."
  },
  {
    "objectID": "notes/08-logarithmic-transformations-predictor.html#using-base-2-log-transformed-sat-scores-in-the-regression-model",
    "href": "notes/08-logarithmic-transformations-predictor.html#using-base-2-log-transformed-sat-scores-in-the-regression-model",
    "title": "Log-Transforming the Predictor",
    "section": "Using Base-2 Log-Transformed SAT Scores in the Regression Model",
    "text": "Using Base-2 Log-Transformed SAT Scores in the Regression Model\nTo fit the model, we use the lm() function and input the log-transformed SAT scores as the predictor.\n\n# Fit regression model\nlm.log2 = lm(grad ~ 1 + log(sat, base = 2), data = mn)\n\n\nWe can now look at the regression output and interpret the results.\n\n# Model-level output\nglance(lm.log2)\n\n\n  \n\n\n\nExamining the model-level output, we see that differences in \\(\\log_2(\\mathrm{SAT})\\) explain 81.13% of the variation in graduation rates. Since differences in \\(\\log_2(\\mathrm{SAT})\\) imply that there are differences in the raw SAT scores, we would typically just say that “differences in SAT scores explain 81.13% of the variation in graduation rates.”\nMoving to the coefficient-level output,\n\n# Coefficient-level output\ntidy(lm.log2)\n\n\n  \n\n\n\nWe can write the fitted equation as,\n\\[\n\\hat{\\mathrm{Graduation~Rate}_i} = -306.7 + 106.4\\bigg[\\log_2(\\mathrm{SAT}_i)\\bigg]\n\\]\nWe can interpret the coefficients as we always do, recognizing that these interpretation are based on the log-transformed predictor.\n\nThe intercept value of \\(-306.7\\) is the predicted average graduation rate for all colleges/universities with a \\(\\log_2(\\mathrm{SAT})\\) value of 0.\nThe slope value of 106.4 indicates that each one-unit difference in \\(\\log_2(\\mathrm{SAT})\\) is associated with a 106.4-unit difference in graduation rate, on average.\n\n\n\nBetter Interpretations: Back-transforming to the Raw Metric\nWhile these interpretations are technically correct, it is more helpful to your readers (and more conventional) to interpret any regression results in the raw metric of the variable rather than log-transformed metric. This means we have to back-transform the interpretations. To back-transform a logarithm, we use its inverse function; exponentiation.\nWe interpreted the intercept as, “the predicted average graduation rate for all colleges/universities with a \\(\\log_2(\\mathrm{SAT})\\) value of 0”. To interpret this using the raw metric of our SAT attribute, we have to understand what \\(\\log_2(\\mathrm{SAT}) = 0\\) is equivalent to in the original SAT variable’s scale. Mathematically,\n\\[\n\\log_2 (\\mathrm{SAT}) = 0 \\quad \\mathrm{is~equivalent~to} \\quad 2^{0} = \\mathrm{SAT} \\\\\n\\]\nIn this computation, \\(\\mathrm{SAT}=1\\). Thus, rather than using the log-transformed interpretation, we can, instead, interpret the intercept as,\n\nThe predicted average graduation rate for all colleges/universities with a median SAT value of 1 (which since this is measures in hundreds corresponds to a median SAT of 100) is \\(-306.7\\). Since there are no colleges/universities in our data that have a median SAT value of 1, this is extrapolation.\n\nWhat about the slope? Our interpretation was that “each one-unit difference in \\(\\log_2(\\mathrm{SAT})\\) is associated with a 106.4-unit difference in graduation rate, on average.” Working with the same idea of back-transformation, we need to understand what a one-unit difference in \\(\\log_2(\\mathrm{SAT})\\) means. Consider four values of \\(\\log_2(\\mathrm{SAT})\\) that are each one-unit apart:\n\\[\n\\begin{split}\n\\log_2(\\mathrm{SAT}) &= 1\\\\\n\\log_2(\\mathrm{SAT}) &= 2\\\\\n\\log_2(\\mathrm{SAT}) &= 3\\\\\n\\log_2(\\mathrm{SAT}) &= 4\n\\end{split}\n\\]\nIf we back-transform each of these, then we can see how the four values of the raw SAT variable would differ.\n\\[\n\\begin{split}\n\\mathrm{SAT} &= 2^1 = 2\\\\\n\\mathrm{SAT} &= 2^2 = 4\\\\\n\\mathrm{SAT} &= 2^3 = 8\\\\\n\\mathrm{SAT} &= 2^4 = 16\n\\end{split}\n\\]\nWhen \\(\\log_2(\\mathrm{SAT})\\) is increased by one-unit, the raw SAT value is doubled. We can use this in our interpretation of slope:\n\nA doubling of the SAT value is associated with a 106.4 percentage point difference in graduation rate, on average.\n\nThe technical language for doubling is a “two-fold difference”. So we would conventionally interpret this as:\n\nEach two-fold difference in SAT value is associated with a 106.4 percentage point difference in graduation rate, on average.\n\nTo understand this further, consider a specific school, say Augsburg. Their measurement on the raw SAT variable is 10.3, and their log-transformed SAT score is 3.36. Using the fitted regression equation (which employs the log-transformed SAT),\n\n-306.7 + 106.4 * 3.36\n\n[1] 50.804\n\n\nAugsburg’s predicted graduation rate would be 50.8. If we increase the l2sat score by 1 to 4.36 (which is equivalent to a raw SAT measurement of 20.6; double 10.3), their predicted graduation rate would be,\n\n-306.7 + 106.4 * 4.36\n\n[1] 157.204\n\n\nThis is an increase of 106.4 percentage points from the predicted value of 50.8 when the l2sat value was 3.36."
  },
  {
    "objectID": "notes/08-logarithmic-transformations-predictor.html#using-base-e-log-transformed-sat-scores-in-the-regression-model",
    "href": "notes/08-logarithmic-transformations-predictor.html#using-base-e-log-transformed-sat-scores-in-the-regression-model",
    "title": "Log-Transforming the Predictor",
    "section": "Using Base-e Log-Transformed SAT Scores in the Regression Model",
    "text": "Using Base-e Log-Transformed SAT Scores in the Regression Model\nIn our example, neither of the bases we examined is satisfactory in terms of interpreting the effect of median SAT score on graduation rate. Two-fold differences in median SAT scores are very unlikely, to say anything of ten-fold differences. One base that is commonly used for log-transformations because it offers a reasonable interpretation is base-e. Below we regress graduation rates on the log-transformed SAT scores, using the natural logarithm.\n\n# Fit model\nlm.ln = lm(grad ~ 1 + log(sat), data = mn)\n\n# Model-level output\nglance(lm.ln)\n\n\n  \n\n\n# Coefficient-level output\ntidy(lm.ln)\n\n\n  \n\n\n\nAs with any base, using base-e results in the same model-level evidence (\\(R^2=.811\\), AIC values). The fitted equation is,\n\\[\n\\hat{\\mathrm{Graduation~Rate}_i} = -306.7 + 153.6\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg]\n\\]\nThe intercept has the same coefficient (\\(\\hat\\beta_0=-306.7\\)), SE, t-value, and p-value as the intercept from the models using base-2 and base-10 log-transformations of SAT. (This is, again, because \\(2^0=10^0=e^0=1\\).) And, although the coefficient and SE for the effect of SAT is again different (a one-unit change in the three different log-scales does not correspond to the same amount of change in raw SAT for the three models).\nSo how can we interpret the model’s coefficients?\n\nThe intercept can be interpreted exactly the same as in the previous models in which we used base-2 or base-10; namely that the predicted average graduation rate for colleges/universities with a SAT value of one (median SAT score of 100) is \\(-306.7\\).\nInterpreting the slope, we could say that an e-fold difference in SAT value is associated with a 153.6-unit difference in graduation rates, on average.\n\nWe can aslo verify that this model’s residuals are identical to the other log-transformed models’ residuals.\n\n# Obtain residuals for base-2 log-transformed SAT\nresidual_plots(lm.ln)\n\n\n\n\n\n\n\nFigure 4: Plot of the standardized residuals versus the fitted values for a model fitted using the natural log-transformed median SAT scores. The reference line of Y=0 is displayed along with the 95% confidence envelope (grey shaded area). The loess smoother (solid, blue line) showing the empirical relationship is also displayed.\n\n\n\n\n\n\n\nInterpretation Using Percent Change\nConsider three schools, each having a median SAT values that differs by 1%; say these schools have median SAT values of 10, 10.1, 10.201. Using the fitted equation, we can compute the predicted graduation rate for each of these hypothetical schools:\n\\[\n\\hat{\\mathrm{Graduation~Rate}} = -306.7 + 153.6 \\bigg[\\ln (\\mathrm{SAT})\\bigg]\n\\]\nThe SAT values and predicted graduation rates for these schools are given below:\n\n\nCode\n# Create data\ntab_05 = data.frame(\n  sat = c(10, 10.1, 10.201)\n  ) %&gt;%\n  mutate(\n    grad = predict(lm.ln, newdata = .)\n  )\n\n# Create table\ntab_05 |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(sat, grad),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    sat = md(\"*SAT*\"),\n    grad = md(\"Predicted Graduation Rate\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(40)\n  )\n\n\n\n\nTable 1: Median SAT Values and Graduation Rates for Three Hypothetical Schools that have Median SAT Values that Differ by One Percent.\n\n\n\n\n\n\n\n\n\n\nSAT\nPredicted Graduation Rate\n\n\n\n\n10.000\n46.87784\n\n\n10.100\n48.40581\n\n\n10.201\n49.93378\n\n\n\n\n\n\n\n\n\n\n\nThe difference between each subsequent predicted graduation rate is 1.53.\n\n# Difference between predicted value for 10.1 and 10\n48.4058 - 46.8778\n\n[1] 1.528\n\n# Difference between predicted value for 10.201 and 10.1\n49.9338 - 48.4058\n\n[1] 1.528\n\n\nIn other words, schools that have a SAT value that differ by 1%, have predicted graduation rates that differ by 1.53, on average.\n\nBe very careful when you are using “percent difference” and “percentage points difference”. These are two very different things. For example, increasing a graduation rate from 50% to 60% represents a difference of 10 percentage points, but it is a 20% increase!\n\n\n\n\nMathematically Calculating the Size of the Effect\nTo understand how we can directly compute the effect for a 1% change in X, consider the predicted values for two x-values that differ by 1%, if we use symbolic notation:\n\\[\n\\begin{split}\n\\hat{y}_1 &= \\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(x)\\right] \\\\\n\\hat{y}_2 &= \\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(1.01x)\\right]\n\\end{split}\n\\]\nThe difference in their predicted values is:\n\\[\n\\begin{split}\n\\hat{y}_2 - \\hat{y}_1 &= \\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(1.01x)\\right] - \\left(\\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(x)\\right]\\right) \\\\[1ex]\n&=\\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(1.01x)\\right] - \\hat\\beta_0 - \\hat\\beta_1\\left[\\ln(x)\\right] \\\\[1ex]\n&=\\hat\\beta_1\\left[\\ln(1.01x)\\right] - \\hat\\beta_1\\left[\\ln(x)\\right] \\\\[1ex]\n&=\\hat\\beta_1\\left[\\ln(1.01x) - \\ln(x)\\right]\\\\[1ex]\n&=\\hat\\beta_1\\left[\\ln(\\frac{1.01x}{1x})\\right]\n\\end{split}\n\\]\nIf we substitute in any value for \\(x\\), we can now directly compute this constant difference. Note that a convenient value for X is 1. Then this reduces to:\n\\[\n\\hat\\beta_1\\left[\\ln(1.01)\\right]\n\\]\nSo now, we can interpret this as: a 1% difference in X is associated with a \\(\\hat\\beta_1\\left[\\ln(1.01)\\right]\\)-unit difference in Y, on average.\nIn our model, we can compute this difference using the fitted coefficient \\(\\hat\\beta_1=153.6\\) as\n\\[\n153.6\\left[\\ln(1.01)\\right] = 1.528371\n\\]\nThe same computation using R is\n\n153.6 * log(1.01)\n\n[1] 1.528371\n\n\nThis gives you the constant difference exactly. So you can interpret the effect of SAT as, each 1% difference in SAT score is associated with a difference in graduation rates of 1.53 percentage points, on average.\n\n\n\nApproximating the Size of the Effect\nWe can get an approximation for the size of the effect by using the mathematical shortcut of:\n\\[\n\\mathrm{Effect} \\approx \\frac{\\hat\\beta_1}{100}\n\\]\nUsing our fitted results, we could approximate the size of the effect as,\n\\[\n\\frac{153.6}{100} = 1.536\n\\]\nWe could then interpret the effect of SAT by saying a 1% difference in median SAT score is associated with a 1.53 percentage point difference in predicted graduation rate, on average."
  },
  {
    "objectID": "notes/08-logarithmic-transformations-predictor.html#plot-of-the-fitted-equation",
    "href": "notes/08-logarithmic-transformations-predictor.html#plot-of-the-fitted-equation",
    "title": "Log-Transforming the Predictor",
    "section": "Plot of the Fitted Equation",
    "text": "Plot of the Fitted Equation\nTo further help interpret these effects, we can plot the curves resulting from this fitted equation. Now we will have two curves one for public schools and one for private schools. To determine the exact equations for these curves, we will substitute either 0 (for private schools) or 1 (for public schools) into the fitted equation for the public variable.\nPrivate\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -286.1 - 8.5(0) + 146.0\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] \\\\\n&= -286.1 + 146.0\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] \\\\\n\\end{split}\n\\]\nPublic\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -286.1 - 8.5(1) + 146.0\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] \\\\\n&= -294.6 + 146.0\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] \\\\\n\\end{split}\n\\]\nWe can input each of these fitted equations into a geom_function() layer of our ggplot() syntax.\n\n# Plot\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0) +\n    geom_function( # Private schools\n      fun = function(x) {-286.1 + 146.0*log(x)},\n      color = \"#6d92ee\",\n      linetype = \"solid\"\n      ) +\n  geom_function( # Public schools\n    fun = function(x) {-294.6 + 146.0*log(x)},\n    color = \"#fe932d\",\n    linetype = \"dashed\"\n    ) +\n    theme_light() +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\")\n\n\n\n\n\n\n\nFigure 7: Plot of predicted graduation rate as a function of median SAT score for public (blue, solid line) and private (orange, dashed line) institutions.\n\n\n\n\n\nThe plot shows the nonlinear, diminishing positive effect of median SAT score on graduation rate for both public and private schools. For schools with lower median SAT scores, there is a larger effect on graduation rates than for schools with higher median SAT scores (for both private and public schools). Note that the curves are parallel (the result of fitting a main effects model) suggesting that the effect of median SAT score on graduation rates is the same for both public and private institutions.\nThe plot also shows the controlled effect of sector. For schools with the same median SAT score, private schools have a higher predicted graduation rate than public schools, on average. This difference is the same, regardless of median SAT score; again a result of fitting the main effects model."
  },
  {
    "objectID": "notes/08-logarithmic-transformations-predictor.html#footnotes",
    "href": "notes/08-logarithmic-transformations-predictor.html#footnotes",
    "title": "Log-Transforming the Predictor",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo long as we can ultimately write this function as a linear model, namely, \\(Y_i=\\beta_0+\\beta_1(x_i) + \\epsilon_i\\).↩︎\nMathematically, the instantaneous derivative of the logarithmic function is is always positive, but it approaches 0 for larger values of x, while that for the quadratic function eventually becomes negative.↩︎\nHere 🦒 is simply a placeholder for the expression \\(2^{\\hat\\beta_0} \\times X_i^{\\hat\\beta_1}\\).↩︎\nIn order to determine this we would need more data at higher values of SAT to determine if the graduation rate gets lower or essentially planes off. The lack of data in this range is why we have this model selection uncertainty.↩︎"
  },
  {
    "objectID": "notes/06-information-criteria-and-model-selection.html",
    "href": "notes/06-information-criteria-and-model-selection.html",
    "title": "Information Criteria and Model Selection",
    "section": "",
    "text": "In this set of notes, you will use information theoretic approaches (e.g., information criteria) to select one (or more) empirically supported model from a set of candidate models. the usnwr-2024.csv dataset (see the data codebook) to fit a set of models that explain variation in peer ratings of graduate schools of education.\n\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom)\nlibrary(educate)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(tidyr)\n\n# Import data\nusnwr = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/usnwr-2024.csv\") |&gt;\n  drop_na()\n\n# View data\nusnwr"
  },
  {
    "objectID": "notes/06-information-criteria-and-model-selection.html#empirical-support-is-for-the-working-hypotheses",
    "href": "notes/06-information-criteria-and-model-selection.html#empirical-support-is-for-the-working-hypotheses",
    "title": "Information Criteria and Model Selection",
    "section": "Empirical Support is for the Working Hypotheses",
    "text": "Empirical Support is for the Working Hypotheses\nBecause the models are proxies for the scientific working hypotheses, the AIC ends up being a measure of empirical support for any particular working hypothesis. Using the AIC, we can rank order the models (and subsequently the working hypotheses) based on their level of empirical support. Ranked in order of empirical support, the four scientific working hypotheses are:\n\nH3: Institutional prestige is a function of the institution’s characteristics, student- and faculty-resources, and student- and faculty-outcomes.\nH2: Institutional prestige is a function of the institution’s characteristics, and student- and faculty-resources.\nH1: Institutional prestige is a function of the institution’s characteristics.\nH0: Institutional prestige is not a function of anything other than the individual institution.\n\nIt is important to remember that the phrase given the data and other candidate models is highly important. The ranking of models/working hypotheses is a relative ranking of the models’ level of empirical support contingent on the candidate models we included in the comparison and the data we used to compute the AIC.\nAs such, this method is not able to rank any hypotheses that you did not consider as part of the candidate set of scientific working hypotheses. Moreover, the AIC is a direct function of the likelihood which is based on the actual model fitted as a proxy for the scientific working hypothesis. If the predictors used in any of the models had been different, it would lead to different likelihood and AIC values, and potentially a different rank ordering of the hypotheses.\nThe ranking of models is also based on the data we have. If we had different ranges of the data or a different set of variables, the evidence might support a different model. This is very important. Model 3 is the most empirically supported candidate model GIVEN the four candidate models we compared and the data we used to compute the AIC metric.\n\nThe model selection and ranking is contingent on both the set of candidate models you are evaluating, and the data you have.\n\nBased on the AIC values for the four candidate models we ranked the hypotheses based on the amount of empirical support:\n\n\nCode\ncand_models = data.frame(\n  Model = c(\n    \"Model 3\",\n    \"Model 2\",\n    \"Model 1\",\n    \"Model 0\"\n    ),\n  k = c(13, 11, 6, 2),\n  AIC = c(AIC(lm.3), AIC(lm.2), AIC(lm.1), AIC(lm.0))\n) \n\ncand_models |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Model),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(k, AIC),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Model = md(\"*Model*\"),\n    k = md(\"*k*\"),\n    AIC = md(\"*AIC*\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(40)\n  )\n\n\n\n\nTable 1: Models rank-ordered by the amount of empirical support as measured by the AIC.\n\n\n\n\n\n\n\n\n\n\nModel\nk\nAIC\n\n\n\n\nModel 3\n13\n0.6810194\n\n\nModel 2\n11\n67.8027454\n\n\nModel 1\n6\n102.8983425\n\n\nModel 0\n2\n120.3346639"
  },
  {
    "objectID": "notes/06-information-criteria-and-model-selection.html#pretty-printing-tables-of-model-evidence-for-quarto-documents",
    "href": "notes/06-information-criteria-and-model-selection.html#pretty-printing-tables-of-model-evidence-for-quarto-documents",
    "title": "Information Criteria and Model Selection",
    "section": "Pretty Printing Tables of Model Evidence for Quarto Documents",
    "text": "Pretty Printing Tables of Model Evidence for Quarto Documents\nWe can format the output from aictab() to be used in the gt() function. Because there are multiple classes associated with the output from the aictab() function, we first pipe model_evidence object into the data.frame() function. Viewing this, we see that the data frame, also includes an additional column that gives the relative likelihoods (ModelLik).\n\n# Create data frame to format into table\ntab_01 = model_evidence |&gt;\n  data.frame()\n\n# View table\ntab_01\n\n\n  \n\n\n\nThen we can use the select() function to drop the LL and Cum.Wt columns from the data frame. The log-likelihood is redundant to the information in the AICc column, since AICc is a function of log-likelihood and the other information in the table. The cumulative weight can also easily be computed from the information in the AICcWt column.\n\n# Drop columns\ntab_01 = tab_01 |&gt;\n  select(-LL, -Cum.Wt)\n\n# View table\ntab_01\n\n\n  \n\n\n\nWe can then pipe the tab_01 data frame into the gt() function to format the table for pretty-printing in Quarto. For some column labels, I use the html() function in order to use HTML symbols to create the Greek letter Delta and the scripted “L”.\n\n# Create knitted table\ntab_01 |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Modnames),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(K, AICc, Delta_AICc, ModelLik, AICcWt),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Modnames = md(\"*Model*\"),\n    K = md(\"*k*\"),\n    AICc = md(\"*AICc*\"),\n    Delta_AICc = html(\"&#916;AICc\"),\n    ModelLik = html(\"Rel(&#8466;)\"),\n    AICcWt = md(\"*AICc Wt.*\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(50)\n  ) |&gt;\n  tab_footnote(\n    footnote = html(\"Rel(&#8466;) = Relative likelihood\"),\n    locations = cells_column_labels(columns = ModelLik)\n  )\n\n\n\nTable 7: Models rank-ordered by the amount of empirical support as measured by the AICc after removing Model 1 from the candidate set. Other evidence includes the ΔAICc, relative likelihood, and model probability (AICc weight) for each model.\n\n\n\n\n\n\n\n\n\n\nModel\nk\nAICc\nΔAICc\nRel(ℒ)1\nAICc Wt.\n\n\n\n\nModel 3\n13\n5.956382\n0.00000\n1.000000e+00\n1.000000e+00\n\n\nModel 2\n11\n71.521055\n65.56467\n5.791780e-15\n5.791780e-15\n\n\nModel 1\n6\n104.003606\n98.04722\n5.120541e-22\n5.120541e-22\n\n\nModel 0\n2\n120.484664\n114.52828\n1.350515e-25\n1.350515e-25\n\n\n\n1 Rel(ℒ) = Relative likelihood"
  },
  {
    "objectID": "notes/06-information-criteria-and-model-selection.html#footnotes",
    "href": "notes/06-information-criteria-and-model-selection.html#footnotes",
    "title": "Information Criteria and Model Selection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnalyses not shown.↩︎"
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html",
    "href": "notes/04-optional-likelihood-estimation.html",
    "title": "Likelihood: A Framework for Estimation",
    "section": "",
    "text": "In this set of notes, you will learn about the method of maximum likelihood to estimate model parameters.\n\n# Load libraries\nlibrary(broom)\nlibrary(gt)\nlibrary(tidyverse)"
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html#likelihood-profile-for-multiple-parameters",
    "href": "notes/04-optional-likelihood-estimation.html#likelihood-profile-for-multiple-parameters",
    "title": "Likelihood: A Framework for Estimation",
    "section": "Likelihood Profile for Multiple Parameters",
    "text": "Likelihood Profile for Multiple Parameters\nWe could also plot the profile of the likelihood for our search space, but this time there would be three dimensions: one dimension for \\(\\mu\\) (x-axis), one dimension for \\(\\sigma\\) (y-axis), and one dimension for the likelihood (z-axis). When we plot the likelihood profile across both \\(\\mu\\) and \\(\\sigma\\), the profile looks like an asymmetrical mountain. The highest likelihood value is at the summit of the mountain and corresponds to \\(\\mu=25.2\\) and \\(\\sigma=3.7\\).\n\n\nCode\n# Load library\nlibrary(plot3D)\n\nscatter3D(x = example_02$mu, y = example_02$sigma, z = example_02$L, \n          pch = 18, cex = 2, theta = 45, phi = 20, ticktype = \"detailed\",\n          xlab = expression(mu), ylab = expression(sigma), zlab = \"Likelihood\",\n          colkey = FALSE,\n          colvar = example_02$L,\n          col = ramp.col(col = c(\"#f6eff7\", \"#bdc9e1\", \"#67a9cf\", \"#1c9099\", \"#016c59\"), n = 100, alpha = 1)\n)\n\n\n\n\n\n\n\n\nFigure 4: Likelihood profile for the search space of both \\(\\mu\\) and \\(\\sigma\\) assuming a normal distribution.\n\n\n\n\n\nIf we extend our estimation to three or more parameters, we can still use the computational search to find the maximum likelihood estimates (MLEs), but it would be difficult to plot (there would be four or more dimensions). In general, the profile plots are more useful as a pedagogical tool rather than as a way of actually finding the MLEs."
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html#complications-with-grid-search",
    "href": "notes/04-optional-likelihood-estimation.html#complications-with-grid-search",
    "title": "Likelihood: A Framework for Estimation",
    "section": "Complications with Grid Search",
    "text": "Complications with Grid Search\nIn practice, there are several issues with the grid search methods we have employed so far. The biggest is that you would not have any idea which values of \\(\\beta_0\\) and \\(\\beta_1\\) to limit the search space to. Essentially you would need to search an infinite number of values unless you could limit the search space in some way. For many common methods (e.g., linear regression) finding the ML estimates is mathematically pretty easy (if we know calculus; see the section Using Calculus to Determine the MLEs). For more complex methods (e.g., mixed-effect models) there is not a mathematical solution. Instead, mathematics is used to help limit the search space and then a grid search is used to hone in on the estimates.\nAlthough not a complication, we made an assumption about the value of the residual standard error, that it was equivalent to sigma(errors). In practice, this value would also need to be estimated, along with the coefficients."
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html#estimating-the-residual-variation-maximum-likelihood-vs.-ordinary-least-squares",
    "href": "notes/04-optional-likelihood-estimation.html#estimating-the-residual-variation-maximum-likelihood-vs.-ordinary-least-squares",
    "title": "Likelihood: A Framework for Estimation",
    "section": "Estimating the Residual Variation: Maximum Likelihood vs. Ordinary Least Squares",
    "text": "Estimating the Residual Variation: Maximum Likelihood vs. Ordinary Least Squares\nThe estimates of the residual standard error differ because the two estimation methods use different criteria to optimize over; OLS estimation finds the estimates that minimize the sum of squared errors, and ML finds the estimates that maximize the likelihood. Because of the differences, it is important to report how the model was estimated in any publication.\nBoth estimation methods have been well studied, and the resulting residual standard error from these estimation methods can be computed directly once we have the coefficient estimates (which are the same for both methods). Namely, the residual standard error resulting from OLS estimation is:\n\\[\n\\hat\\sigma_{\\epsilon}= \\sqrt{\\frac{\\left(Y_i - \\hat{Y}_i\\right)^2}{n-p-1}}\n\\]\nwhere p is the number of predictors in the model. And the residual standard error resulting from ML estimation is:\n\\[\n\\hat\\sigma_{\\epsilon}=\\sqrt{\\frac{\\left(Y_i - \\hat{Y}_i\\right)^2}{n}},\n\\]\nThe smaller denominator from the OLS estimate produces a higher overall estimate of the residual variation (more uncertainty). When n is large, the differences between the OLS and ML estimates of the residual standard error are minimal and can safely be ignored. When n is small, however, these differences can impact statistical results. For example, since the residual standard error is used to compute the standard error estimates for the coefficients, the choice of ML or OLS will have an effect on the size of the t- and p-values for the coefficients. (In practice, it is rare to see the different estimation methods producing substantively different findings, especially when fitting general linear models.)\nLastly, we note that the value of log-likelihood is the same for both the ML and OLS estimated models. The result from the ML output was:\n\\[\n\\begin{split}\n-2 \\ln(\\mathrm{Likelihood}) &= 78.91 \\\\[1ex]\n\\ln(\\mathrm{Likelihood}) &= -39.45\n\\end{split}\n\\]\nThe log-likelihood for the OLS estimated model is:\n\n# Log-likelihood for OLS model\nlogLik(lm(y ~ 1 + x))\n\n'log Lik.' -39.45442 (df=3)\n\n\nThis is a very useful result. It allows us to use lm() to estimate the coefficients from a model and then use its log-likelihood value in the same way as if we had fitted the model using ML. This will be helpful when we compute measure such as information criteria later in the course.\n\nIn many applications of estimation, it is useful to use a criterion which is modified variant of the likelihood. This variant omits “nuisance parameters” (parameters which are not of direct interest and subsequently not needed in the estimation method) from the computation of the likelihood. This restricted version of the likelihood is then maximized and the estimation method using this modified likelihood is called Restricted Maximum Likelihood (REML).\nWhen REML is used to estimate parameters, the residual standard error turns out to be the same as that computed in the OLS estimation. As such, sometimes this estimate is referred to as the REML estimate of the residual standard error."
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html#footnotes",
    "href": "notes/04-optional-likelihood-estimation.html#footnotes",
    "title": "Likelihood: A Framework for Estimation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe could also compute the log-likelihood directly using sum(dnorm(c(30, 20, 24, 27), mean = mu, sd = 4.272, log = TRUE)).↩︎\nThis is because taking the logarithm of a set of numbers keeps the same ordination of values as the original values.↩︎\nAlternatively, x and y could be included as additional inputs into the function.↩︎"
  },
  {
    "objectID": "notes/03-more-quarto.html",
    "href": "notes/03-more-quarto.html",
    "title": "More Quarto",
    "section": "",
    "text": "This document contains some additional instruction for EPsy 8252. Note that I might add to this as students hit me with questions over the course of the semester."
  },
  {
    "objectID": "notes/03-more-quarto.html#creating-.bib-files-using-zotero",
    "href": "notes/03-more-quarto.html#creating-.bib-files-using-zotero",
    "title": "More Quarto",
    "section": "Creating .bib Files Using Zotero",
    "text": "Creating .bib Files Using Zotero\nMost reference managers (e.g., Papers, Zotero, Mendelay) can produce BibTex files. Here I will illustrate the process using Zotero.\n\nCreate a New Collection.\nDrag the references you want in your BibTeX database into this collection. For today, drag the Carmichael (1954) and Ross, Winterhalder, & McElreath (2020) references into this collection.\nRight-click the collection and select Export Collection.\nIn the pop-up window, change the format of the exported collection to BibTex.\nClick OK.\n\nName the BibTex file (here I named it my-bibliography.bib) and save it in the assets folder.\n\n\n\n\n\n\n\n\n\nWhen you call the BibTex file in the bibliography: key of the YAML in your QMD document, you will need to give the location of the BibTex (relative to the QMD document) and the name you just gave it. For example if you are calling a bibliography in the quarto document assignment-01.qmd that has the following directory/folder structure:\nassignment-01\n    ├── README\n    ├── assets\n       └── my-bibliography.bib\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       └── fertility.csv\n    ├── figs\n    └── scripts"
  },
  {
    "objectID": "notes/03-more-quarto.html#including-the-bib-file-in-your-yaml",
    "href": "notes/03-more-quarto.html#including-the-bib-file-in-your-yaml",
    "title": "More Quarto",
    "section": "Including the BIB File in your YAML",
    "text": "Including the BIB File in your YAML\nYou would then include the following in your Quarto document’s YAML:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat: html\neditor: visual\nbibliography: \"assets/my-bibliography.bib\"\n---"
  },
  {
    "objectID": "notes/03-more-quarto.html#bibtex-files",
    "href": "notes/03-more-quarto.html#bibtex-files",
    "title": "More Quarto",
    "section": "BibTex Files",
    "text": "BibTex Files\nBibTeX files are essentially databases that store bibliographic information in a plain-text (style-independent) file. The database includes a set of references and their metadata. Here is the raw text inside an example BibTex file that includes two articles written by Carmichael (1954) and Ross et al. (2020).\n@article{carmichael_1954,\n    title = {Laziness and the Scholarly Life},\n    volume = {78},\n    number = {4},\n    journal = {The Scientific Monthly},\n    author = {Carmichael, Leonard},\n    year = {1954},\n    pages = {208--213}\n}\n\n@article{ross_2020,\n    title = {Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates},\n    issn = {1948-5506, 1948-5514},\n    url = {http://journals.sagepub.com/doi/10.1177/1948550620916071},\n    doi = {10.1177/1948550620916071},\n    language = {en},\n    urldate = {2020-06-24},\n    journal = {Social Psychological and Personality Science},\n    author = {Ross, Cody T. and Winterhalder, Bruce and McElreath, Richard},\n    month = jun,\n    year = {2020},\n    pages = {194855062091607}\n}\nThe citation identifiers in this example are carmichael_1954 and ross_2020. These identifiers are the first bit of text after the initial curly brace in the reference. This is how we will refer to the citations in our QMD document. (Typically these are auto-generated from our reference manager.) To determine the citiation identifiers, you can open your BibTeX file in RStudio. (Do this by using Open File... within RStudio; double-clicking on the BibTeX file will likely open it in a different application.)"
  },
  {
    "objectID": "notes/03-more-quarto.html#including-citations-in-your-quarto-document",
    "href": "notes/03-more-quarto.html#including-citations-in-your-quarto-document",
    "title": "More Quarto",
    "section": "Including Citations in Your Quarto Document",
    "text": "Including Citations in Your Quarto Document\nCitations go inside square brackets and are separated by semicolons. Each citation must have a key, composed of @citation_identifier from the database (no spaces between them). For example to cite the Carmichael article:\nHere is some text and a citation [@carmichael_1954].\nThis will create a citation where you included it in the text and also add the reference at the end of the document. If you want a section header for your references, include a level-1 heading called “References” at the end of your document.\nThe rendered document now includes a citation where you added this is the QMD document. The associated reference is also included at the end of the document.\n\n\n\n\n\n\n\n\n\nCitations may also include. additional text before and after the citation. In this example we have prefixed the citation with the word “see” and added a page number after the citation. The citation identifier and the text following the citation identifier are separated by a comma.\nHere is some text and a citation [see @carmichael_1954, p. 208].\nThe output of this is:\n\nHere is some text and a citation (see Carmichael, 1954, p. 208).\n\nYou can also include multiple citations. To do this we include the different citation identifiers separated by a semicolon.\nHere are multiple citations [@ross_2020; @carmichael_1954].\nThe output of this is:\n\nHere are multiple citations (Carmichael, 1954; Ross et al., 2020).\n\nWe can also change the format of the citation. For example, here we use a format common to starting a sentence with a citation. To do that we omit the square brackets.\n@carmichael_1954 suggests something is true.\nThe output of this is:\n\nCarmichael (1954) suggests something is true.\n\n\nYou can learn more on the Citations and Footnotes Help Page in the Quarto documentation."
  },
  {
    "objectID": "notes/03-more-quarto.html#use-apa-formatted-citations-and-references",
    "href": "notes/03-more-quarto.html#use-apa-formatted-citations-and-references",
    "title": "More Quarto",
    "section": "Use APA Formatted Citations and References",
    "text": "Use APA Formatted Citations and References\nBy default, citations and references are formatted using the Chicago style. To use another style, you will need to:\n\nDownload the appropriate citation style language (CSL) file. (Find many at https://zotero.org/styles)\nPlace the CSL file in the assets folder of your project’s directory.\nSpecify the name of the CSL style file in the csl: key in the QMD file’s YAML.\n\nFor example, say you have downloaded and saved the apa-single-spaced.csl file and placed it in the assets directory, giving the following directory/folder structure:\nassignment-01\n    ├── README\n    ├── assets\n       ├── apa-single-spaced.csl\n       └── my-bibliography.bib\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       └── fertility.csv\n    ├── figs\n    └── scripts\nYou would include the following in your YAML.\nIf, for example, you had named the BIB file my-bibliography.bib and had put this file in the assets directory, your YAML would be:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat: html\neditor: visual\nbibliography: \"assets/my-bibliography.bib\"\ncsl: \"assets/apa-single-spaced.csl\"\n---\nUse of the APA CSL file not only formats the references according to APA format, but it also fixed the order of the citations in the text itself! (Even though we included the Ross citation identifier prior to the Carmichael citation identifier in the multiple references example, the APA CSL file put them in the text alphabetically!)"
  },
  {
    "objectID": "notes/03-more-quarto.html#citation-extras",
    "href": "notes/03-more-quarto.html#citation-extras",
    "title": "More Quarto",
    "section": "Citation Extras",
    "text": "Citation Extras\nOne R package that is extremely useful, especially if you are using the visual editor in RStudio for writing QMD documents, is {citr}. This package provides functionality and an RStudio Add-In to search a BibTeX-file to create and insert formatted Markdown citations into a QMD document.\n\n\n\n\n\n\n\n\n\nThe {citr} package is only available via GitHub, so you will need to install it using{remotes}or{devtools}. (See the Computational Toolkit book for a reminder on how to do this.) The directions for installing{citr}` are also on the citr webpage along with instructions for using the package."
  },
  {
    "objectID": "notes/03-creating-tables.html",
    "href": "notes/03-creating-tables.html",
    "title": "Creating Tables using the {gt} Package",
    "section": "",
    "text": "Any table is essentially a rectangular layout (rows and columns) of information. Below I show two common tables of statistical output taken from the Regression Review notes. I have also added guide-lines to show the rectangular display of information within each of these tables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo illustrate how to create a table in the QMD document, we will attempt to recreate Table 1 from the Regression Review notes. We can use the fact that data frames are also rectangular displays of information to create a table in the QMD document. The data.frame() function is used to enter the cell information from each column. This is, of course, done in a code chunk.\n\n# Input cell information\ntab_01 = data.frame(\n  variable = c(\"Environmental policy strength\", \"Corruption\", \"Wealth\", \"Toxic waste severity\", \n               \"Democratic control\", \"Interparty competition\", \"Public environmentalism\"),\n  m = c(17.6, .32, 28.15, 3.53, .63, 39.03, 2.49),\n  sd = c(8.23, .22, 36.38, 1.14, .26, 11.40, .10),\n  min = c(4, 0, 12.78, 0, 0, 9.26, 2.31),\n  max = c(37, .98, 278.01, 5.76, 1, 56.58, 2.7)\n)\n\n# Show output\ntab_01\n\n\n  \n\n\n\n\nHere we entered the cell data in manually, however, most of the time these values will be generated from functions that produce data frames. For example, the output from tidy(), glance(), correlate(), and {dplyr} functionality is a data frame. This output can be used directly when we create our table."
  },
  {
    "objectID": "notes/03-creating-tables.html#column-labels",
    "href": "notes/03-creating-tables.html#column-labels",
    "title": "Creating Tables using the {gt} Package",
    "section": "Column Labels",
    "text": "Column Labels\nColumn labels can be changed from the names of the columns used in the data frame. To change them we will use the cols_label() function. This function takes as many arguments as there are columns, each mapping a label to the original column name. Below, we change our column names to match those in Table 1.\n\n# Change Column labels\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = \"Variable\",\n    m = \"M\",\n    sd = \"SD\",\n    min = \"Min.\",\n    max = \"Max.\"\n  )\n\n\n\n\n\n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\n\nWe can also use the md() function to include Markdown syntax to further format our labels. For example, to make the column labels italics we use the following.\n\n# Change Column labels to italics\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) \n\n\n\n\n  \n  \n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\n\nThe names we just gave to the variables are only labels. As we refer to the columns in additional functions, we need to use their original names from the data frame."
  },
  {
    "objectID": "notes/03-creating-tables.html#column-alignment",
    "href": "notes/03-creating-tables.html#column-alignment",
    "title": "Creating Tables using the {gt} Package",
    "section": "Column Alignment",
    "text": "Column Alignment\nTo change the column alignment, we use the cols_align() function. We provide this with two arguments. The columns= argument takes a vector of column names using the c() function, and the align= argument takes a character string of \"left\", \"right\", or \"center\". Following typical formatting rules, we left align text columns and center numerical columns.\n\n# Change Column labels to italics\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  )\n\n\n\n\n  \n  \n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70"
  },
  {
    "objectID": "notes/03-creating-tables.html#adding-a-title-and-subtitle-using-quarto",
    "href": "notes/03-creating-tables.html#adding-a-title-and-subtitle-using-quarto",
    "title": "Creating Tables using the {gt} Package",
    "section": "Adding a Title and Subtitle (Using Quarto)",
    "text": "Adding a Title and Subtitle (Using Quarto)\nIf you are creating a table within a QMD document, we will use the code chunk options to add a table number, title, and subtitle. To create these use the following code chunks:\n\nThe label: field is used to label the table and give it a table number. The key here is that the lable name must start withtbl-. In the example below, the label name I gave the table is tbl-summary-measures.\nThe tbl-cap: field is used to provide a table caption, similar to how fig-cap: is used to give a figure caption.\nThe tbl-subcap: field is used to provide a table subcaption. Not all tables need a subcaption!\nThe tbl-cap-location: field can be use to change the table caption location from top to bottom, or even have it placed in the margin.\n\nHere we show the R code chunk and chunk options to produce the table numbering and caption. (I do not use a subcaption on this table.) The resulting table, along with its numbering (which is autopopulated by Quarto), and caption is shown below.\n\n```{r}\n#| label: tbl-summary-measures\n#| tbl-cap: \"Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |&gt;\n  opt_align_table_header(\"left\")\n```\n\n\n\nTable 1. Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n\n\n\n\n\n\n\n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\n\n\n\n\nThe advantage to including a label: and naming it with a tbl- prefix is that it makes your tables cross-referenceable in the document. In the document’s text you can link to that table using the @ and giving the table name. For example, see Table 1. To do this I wrote the following in my Quarto document:\nFor example, see @tbl-summary-measures.\nSee here for more detail about cross-referencing tables, substables, figures, equations, and more!\n\nChanging the Formatting of the Table Numbering\nWe can change how Quarto formats the table labeling in the YAML part of the document. The default is: “Table X: Table Caption”. Here we make “Table X.” bold with a period after it, where “X” is an arabic number.\ncrossref:\n  tbl-title: \"**Table**\"\n  tbl-labels: arabic\n  title-delim: \".\""
  },
  {
    "objectID": "notes/03-creating-tables.html#adding-a-title-and-subtitle-not-using-quarto",
    "href": "notes/03-creating-tables.html#adding-a-title-and-subtitle-not-using-quarto",
    "title": "Creating Tables using the {gt} Package",
    "section": "Adding a Title and Subtitle (Not Using Quarto)",
    "text": "Adding a Title and Subtitle (Not Using Quarto)\nThe tab_header() function can be used to add a title or subtitle to your table. Here we again use the md() function to allow us to use Markdown syntax directly in the title. I also use the opt_align_table_header() function to left align the title and subtitle per APA.\n\n# Add title and subtitle\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |&gt;\n    tab_header(\n    title = md(\"**Table 1.** Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"),\n  ) |&gt;\n  opt_align_table_header(\"left\")\n\n\n\n\n  \n  \n\n\n\nTable 1. Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\n\nSince the title and subtitle appear on separate lines, you can take advantage of that to use the title to provide the table number and the subtitle provides the table caption if you are trying to format in APA style."
  },
  {
    "objectID": "notes/03-creating-tables.html#fine-tuning-the-table",
    "href": "notes/03-creating-tables.html#fine-tuning-the-table",
    "title": "Creating Tables using the {gt} Package",
    "section": "Fine-Tuning the Table",
    "text": "Fine-Tuning the Table\nThe table is very close to matching the original Table 1. But, there are still a couple of things (if you are an Enneagram One) that we need to attend to. For example, we could remove the horizontal lines in the table. These lines are called “borders” and we can modify them in the tab_style() function. This is a general function that allows us to customize many parts of the table (akin to theme() in ggplot()).\nTo do this we use the style= argument and call the cell_borders() function within tab_style(). Here we remove all borders (top, bottom, left, and right) by using sides=\"all\" and setting style=NULL. The tab_style() function also requires the argument locations=. We give this argument the function cell_body() which we provide the column and row numbers that we want to remove the borders from. Since we want to keep the horizontal border associated with the first and last rows, we omit those row numbers from the rows= argument.\n\n# Add title and subtitle\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |&gt;\n    tab_header(\n    title = md(\"**Table 1.** Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"),\n  ) |&gt;\n  opt_align_table_header(\"left\")  |&gt;\n  tab_style(\n    style = cell_borders(\n      sides = \"all\", \n      style = NULL\n      ),\n    locations = cells_body(\n      columns = 1:5,\n      rows = 2:6\n    )\n  )\n\n\n\n\n  \n  \n\n\n\nTable 1. Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70"
  },
  {
    "objectID": "notes/03-creating-tables.html#formatting-decimal-places",
    "href": "notes/03-creating-tables.html#formatting-decimal-places",
    "title": "Creating Tables using the {gt} Package",
    "section": "Formatting Decimal Places",
    "text": "Formatting Decimal Places\nIn these tables it is typical to format the number of decimal places to 2 for coefficients, standard errors, and t-values and to three decimal places for p-values. To do this, we will use the fmt_number() function from {gt}.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  )\n\n\n\nTable 3. Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\n(Intercept)\n20.49\n3.37\n6.07\n0.000\n\n\ncorrupt\n−10.02\n5.26\n−1.91\n0.063\n\n\ntoxic_waste\n0.08\n0.82\n0.09\n0.927\n\n\n\n\n\n\n\n\n\n\n\nThe p-value for the intercept is now “0.000” after rounding to three decimal places. It is conventional in this case to report it as “&lt;0.001” rather than “0.000”. To do this we will use the sub_values() function from {gt} to search the p.value column for any value less than .001 and replace it with the text “&lt;.001”.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  ) |&gt;\n  sub_values(columns = p.value, fn = function(x) x &lt; .001, replacement = \"&lt;.001\")\n\n\n\nTable 4. Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\n(Intercept)\n20.49\n3.37\n6.07\n&lt;.001\n\n\ncorrupt\n−10.02\n5.26\n−1.91\n0.063\n\n\ntoxic_waste\n0.08\n0.82\n0.09\n0.927\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the sub_values() function to change the text in the term column to make our vasriable names more human-friendly. To do this we again specify the column name, and then indicate the values we want to replace along with the replacement text.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  ) |&gt;\n  sub_values(columns = p.value, fn = function(x) x &lt; .001, replacement = \"&lt;.001\") |&gt;\n  sub_values(columns = term, values = \"(Intercept)\", replacement = \"Constant\") |&gt;\n  sub_values(columns = term, values = \"corrupt\", replacement = \"Political Corruption\") |&gt;\n  sub_values(columns = term, values = \"toxic_waste\", replacement = \"Toxic Waste\")\n\n\n\nTable 5. Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\nConstant\n20.49\n3.37\n6.07\n&lt;.001\n\n\nPolitical Corruption\n−10.02\n5.26\n−1.91\n0.063\n\n\nToxic Waste\n0.08\n0.82\n0.09\n0.927"
  },
  {
    "objectID": "mission-statements.html",
    "href": "mission-statements.html",
    "title": "Mission Statements",
    "section": "",
    "text": "Quantitative Methods in Education Mission Statement\nQME strives to be a premier program recognized for leadership, innovation, and excellence, and to enable human potential through the advancement of education. QME prepares students to become cutting-edge professionals in educational measurement, evaluation, statistics, and statistics education, through excellence in teaching, research, and service; and through investigating and developing research methodology in education.\n\n\n\nDepartment of Educational Psychology Mission Statement\nEducational psychology involves the study of cognitive, emotional, and social learning processes that underlie education and human development across the lifespan. Research in educational psychology advances scientific knowledge of those processes and their application in diverse educational and community settings. The department provides training in the psychological foundations of education, research methods, and the practice and science of counseling psychology, school psychology, and special education. Faculty and students provide leadership and consultation to the state, the nation, and the international community in each area of educational psychology. The department’s scholarship and teaching enhance professional practice in schools and universities, community mental health agencies, business and industrial organizations, early childhood programs, and government agencies. Adopted by the Department. of Educational Psychology faculty October 27, 2004\n\n\n\nCollege of Education + Human Development Mission Statement\nThe mission of the University of Minnesota College of Education and Human Development is to contribute to a just and sustainable future through engagement with the local and global communities to enhance human learning and development at all stages of the life span."
  },
  {
    "objectID": "index.html#welcome-to-epsy-8252-statistical-methods-in-education-ii",
    "href": "index.html#welcome-to-epsy-8252-statistical-methods-in-education-ii",
    "title": "EPsy 8252",
    "section": "Welcome to EPsy 8252: Statistical Methods in Education II",
    "text": "Welcome to EPsy 8252: Statistical Methods in Education II\nEPsy 8252: Statistical Methods in Education II is the second course in an entry-level, doctoral sequence for students in education. The course content for EPsy 8252 builds on the content of EPsy 8251 and includes: (1) non-linear transformations, (2) likelihood estimation and inference, (2) information criteria for model selection, (3) logistic models for analyzing dichotomous outcomes, and (4) mixed-effects models for analysis of longitudinal data.\n\n\n\n\n\n\nEPsy 8252 is a 3 credit course. It is expected that the academic work required of Graduate School and professional school students will exceed three hours per credit per week (see Expected Time per Course Credit Policy). In my experience, it is typical for students to spend 10–15 hours a week on this course. As with every class, some students will spend more time than that on this course, while others will spend less time than that—it all depends on your prior experiences with statistics and computing. If you find yourself consistently spending more than 20 hours a week on the course, please make an appointment to see the instructor so that we can strategize about how to best optimize how you are devoting time to the course."
  },
  {
    "objectID": "index.html#classroom",
    "href": "index.html#classroom",
    "title": "EPsy 8252",
    "section": "Classroom",
    "text": "Classroom\n\n\nTuesday/Thursday (11:15am–12:30pm): Peik Hall 28"
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "EPsy 8252",
    "section": "Textbooks",
    "text": "Textbooks\nThe course textbooks are available via the University of Minnesota library.\n\nRequired: Fox, J. (2021). A mathematical primer for social statistics. Sage.\nOptional: Anderson, D. R. (2008). Model based inference in the life sciences: A primer on evidence. Springer.\n\nAdditionally, the textbook from EPsy 8251 may be a useful reference for refreshing your memories about the prerequisite computing and statistical content:\n\nStatistical Modeling and Computation for Educational Scientists"
  },
  {
    "objectID": "index.html#statistical-computing",
    "href": "index.html#statistical-computing",
    "title": "EPsy 8252",
    "section": "Statistical Computing",
    "text": "Statistical Computing\nStatistical computing is an integral part of statistical work, and subsequently, EPsy 8251. To support your learning in this area, this course will emphasize the use of R. R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS https://www.r-project.org. It should be noted that while some R syntax and programming is taught during class time, there is also a fair amount that you may need to learn on your own outside of class. There are several tutorials and resources linked from the course website to help you learn R.\nYou can install R and RStudio onto your local machine. (There are instructions for how to do this on the course website.) You are responsible for getting things to work on your computer. While it should be straightforward, each OS and computer has their quirks. I can try to help you with this if you are having trouble.\n\n\nA Note on Inclusion and Respect\nIn this class, we will work together to develop a learning community that is inclusive and respectful, and where every student is supported in the learning process. As a class full of diverse individuals (reflected by differences in race, culture, age, religion, gender identity, sexual orientation, socioeconomic background, abilities, professional goals, and other social identities and life experiences) I expect that different students may need different things to support and promote their learning. The TAs and I will do everything we can to help with this, but as we only know what we know, we need you to communicate with us if things are not working for you or you need something we are not providing. I hope you all feel comfortable in helping to promote an inclusive classroom through respecting one another’s individual differences, speaking up, and challenging oppressive/problematic ideas. Finally, I look forward to learning from each of you and the experiences you bring to the class."
  },
  {
    "objectID": "index.html#image-attribution",
    "href": "index.html#image-attribution",
    "title": "EPsy 8252",
    "section": "Image Attribution",
    "text": "Image Attribution\n\nTaylor Swift icons from Kelsey Darbro and TheLastMayDay.\nThe icon of Tilly the Therapy Chicken in the Stress Management note is used with permissin of the PAWS program."
  },
  {
    "objectID": "codebooks/woods.html",
    "href": "codebooks/woods.html",
    "title": "woods.csv",
    "section": "",
    "text": "The data in woods.csv were collected from several sources to assesses the effect of political corruption on state environmental policy (Woods, 2008)."
  },
  {
    "objectID": "codebooks/woods.html#attributes",
    "href": "codebooks/woods.html#attributes",
    "title": "woods.csv",
    "section": "Attributes",
    "text": "Attributes\n\nstate: Two-letter state postal code\nenvprogstr: Environmental program strength is measured via Hall and Kerr’s (1991) Green Policy Index, a composite score that represents 67 state policy initiatives in a variety of environmental arenas, including air, water, and hazardous waste. The index includes indicators such as the sanctions available to the appropriate agencies in each state, the size of the state’s pollution monitoring program, and the size of the state’s program budget, as well as a variety of specific policy indicators.\ncorrupt: Aggregate number of convictions (per 100 officials) during the Reagan Administration (between 1981 and 1987). Aggregating across several years helps eliminate spikes that occur because a single investigation may result in multiple convictions. The average state had 1.42 convictions per 100 officials over that period.\nwealth: Gross state product per capita, in thousands of dollars. This is an indicator of a state’s financial resources.\ntoxicwaste: Natural logarithm of the total tons of toxic waste emitted in air, water based on the U.S. EPA’s Toxic Release Inventory. Higher values indicate a more severe problem.\npublicenv: Indication of public attitudes toward environmental protection in the state. It is calculated from the 1988–1992 NES Senate Election Study, which asked the following question: “Should federal spending on the environment be increased, decreased, or stay the same?” Individual responses are coded 1 (decrease), 2 (same), and 3 (increase) and then averaged.\ndemcontrol: Represents the average amount of Democratic Party control of state political institutions in the 1980s measured as a proportion.\ninterpartycomp: Holbrook and Van Dunk’s (1993) district-level measure is included as an indicator of interparty competition.\nenvgroups: The number of environmental groups registered to lobby in the state in 1990 standardized by gross state product to account for the fact that the density of organized interests is greater in states with larger economies.\nmanufgroups: The number of manufacturing groups registered to lobby in the state in 1990 standardized by gross state product to account for the fact that the density of organized interests is greater in states with larger economies."
  },
  {
    "objectID": "codebooks/woods.html#preview",
    "href": "codebooks/woods.html#preview",
    "title": "woods.csv",
    "section": "Preview",
    "text": "Preview\n\n# Import data\nwoods = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/woods.csv\")\n\n# View data\nwoods |&gt;\n    print(width = Inf)\n\n# A tibble: 50 × 11\n   state envprogstr corrupt legprof wealth toxicwaste publicenv demcontrol\n   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 AK             8   0.586   0.311   44.3       1.31      2.42      0.625\n 2 AL             6   0.630   0.158   18.9       4.70      2.37      0.800\n 3 AR             5   0.132   0.105   18.4       3.71      2.42      0.900\n 4 AZ            13   0.318   0.25    19.3       4.19      2.52      0.300\n 5 CA            34   0.498   0.625   25.5       4.69      2.7       0.600\n 6 CO            17   0.212   0.300   23.8       2.75      2.5       0.5  \n 7 CT            29   0.213   0.233   30.2       3.63      2.59      0.900\n 8 DE            15   0.301   0.192   34.3       2.15      2.60      0.300\n 9 FL            24   0.604   0.255   19.9       4.12      2.61      0.800\n10 GA            12   0.546   0.133   22.7       4.46      2.49      1    \n   interpartycomp envgroups manufgroups\n            &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1          53.5     0.346        0.654\n 2          27.3     0.064        0.538\n 3           9.26    0.0455       0.636\n 4          33.9     0.148        0.783\n 5          47.3     0.014        0.259\n 6          40.2     0.242        0.521\n 7          52.8     0.192        0.586\n 8          39.7     0.127        1.05 \n 9          31.1     0.246        0.875\n10          16.2     0.143        0.651\n# ℹ 40 more rows"
  },
  {
    "objectID": "codebooks/vocabulary.html",
    "href": "codebooks/vocabulary.html",
    "title": "vocabulary.csv",
    "section": "",
    "text": "The data in vocabulary.csv, adapted from data provided by Bock (1975), come from the Laboratory School of the University of Chicago and include scaled test scores across four grades from the vocabulary section of the Cooperative Reading Test for \\(n=64\\) students. The attributes in the dataset include:\n\nid: The student ID number\nvocab_08: The scaled vocabulary test score in 8th grade\nvocab_09: The scaled vocabulary test score in 9th grade\nvocab_10: The scaled vocabulary test score in 10th grade\nvocab_11: The scaled vocabulary test score in 11th grade\nfemale: Dummy coded sex variable (0 = Not female, 1 = Female)\n\n\nPreview\n\n# Import data\nvocab = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/vocabulary.csv\")\n\n# View data\nvocab\n\n# A tibble: 64 × 6\n      id vocab_08 vocab_09 vocab_10 vocab_11 female\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1.75     2.6      3.76     3.68      1\n 2     2     0.9      2.47     2.44     3.43      0\n 3     3     0.8      0.93     0.4      2.27      0\n 4     4     2.42     4.15     4.56     4.21      1\n 5     5    -1.31    -1.31    -0.66    -2.22      0\n 6     6    -1.56     1.67     0.18     2.33      0\n 7     7     1.09     1.5      0.52     2.33      0\n 8     8    -1.92     1.03     0.5      3.04      0\n 9     9    -1.61     0.29     0.73     3.24      0\n10    10     2.47     3.64     2.87     5.38      1\n# ℹ 54 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBock, R. D. (1975). Multivariate statistical methods in behavioral research. New York: McGraw-Hill."
  },
  {
    "objectID": "codebooks/same-sex-marriage.html",
    "href": "codebooks/same-sex-marriage.html",
    "title": "same-sex-marriage.csv",
    "section": "",
    "text": "The data in same-sex-marriage.csv were collected from a poll taken by Pew Research Center in 2015. These particular data consist of 500 American’s responses. The attributes are:\n\nsupport: Dummy-coded variable indicating whether the respondent supports same sex marriage? (1=Yes; 0=No)\nfriends: Number of respondent’s friends that are gay or lesbian\n\n0 = None\n1 = Only 1 or 2\n2 = Some\n3 = A lot\n\neducation: Respondent’s level of education\n\n0 = High school or less\n1 = Some college (no degree) or two year associate degree from a college or university\n2 = Four year college or university degree/Bachelor’s degree (e.g., BS, BA, AB)\n3 = Some postgraduate or professional schooling or postgraduate or professional degree\n\nfemale: Dummy-coded variable indicating whether the respondent is female (1=Yes; 0=No)\nage: Respondent’s age, in years\nattend: Frequency the respondent attends religious services\n\n0 = Never\n1 = Seldom\n2 = A few times a year;\n3 = Once or twice a month;\n4 = Once a week\n5 = More than once a week\n\nideology: Respondent’s description of their political views\n\n1 = Very conservative\n2 = Conservative\n3 = Moderate\n4 = Liberal\n5 = Very liberal\n\nparty: Respondent’s self-identified political party affiliation (Democrat, Republican, Independent)\n\n\nPreview\n\n# Import data\nsame_sex = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/same-sex-marriage.csv\")\n\n# View data\nsame_sex\n\n# A tibble: 500 × 8\n   support friends education female   age attend ideology party      \n     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1       1       2         0      0    30      2        3 Democrat   \n 2       1       2         0      1    35      3        3 Independent\n 3       0       1         0      0    78      4        2 Republican \n 4       0       1         0      0    60      4        2 Independent\n 5       1       1         0      0    19      2        2 Republican \n 6       1       1         0      0    56      2        4 Independent\n 7       0       3         0      0    99      2        2 Republican \n 8       1       3         0      0    50      2        2 Democrat   \n 9       1       3         0      0    33      2        4 Independent\n10       1       3         0      1    18      2        3 Independent\n# ℹ 490 more rows"
  },
  {
    "objectID": "codebooks/pew.html",
    "href": "codebooks/pew.html",
    "title": "pew.csv",
    "section": "",
    "text": "The data in pew.csv come from a telephone survey conducted by The Pew Research Center for The People & The Press in February 2007. The data represent a probability sample of 1,502 adults in the U.S. The attributes in the dataset include:\n\nid: The respondent ID number\nknowledge: Score on the Political News Knowledge Test; on a scale from 0–100, with higher values indicating more political knowledge.\nnews: Respondent’s level of news exposure; on a scale from 0–100, with higher values indicating more news exposure\nage: Respondent’s age\neduc: Respondent’s education level; indicates highest grade completed from 8 to 18 (a post masters degree)\nmale: Dummy coded sex variable (0 = Not male, 1 = Male)\nideology: Respondent’s political ideology; on a scale from 0–100, where 0 is as liberal as possible and 100 is as conservative as possible\nparty: Respondent’s party affiliation\nstate_blue: Does the respondent live in a “blue state” (voted Democrat) or a “red state” (voted Republican) as classified by voting in the 2004 presidential election\nengagement: Index of individual political engagement; on a scale from 0–100, with higher values indicating more political engagement\n\n\nPreview\n\n# Import data\npew = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/pew.csv\")\n\n# View data\npew\n\n# A tibble: 1,502 × 10\n      id knowledge  news   age  educ  male ideology party  state_blue engagement\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1     1        50    63  44.4    14     0     39.3 Democ…          0       79.5\n 2     5        79    54  59.5    15     1     69.5 Democ…          0       80.8\n 3     6        31    65  43.8    12     0     18   Democ…          0       99.6\n 4     8        93    50  29.6    16     1     29.7 Democ…          0       88.6\n 5     9        23    12  45.1    12     0     52.5 Democ…          0       82.4\n 6    11        23    43  47.5     9     0     62   Democ…          0       67  \n 7    13        44    34  18.3    10     1     28.3 Democ…          0       69.4\n 8    14        97    56  79.1    17     1     30.1 Democ…          0       78.2\n 9    16        54    81  32.5    13     0     31.7 Democ…          0       97.8\n10    18        58    66  30.8    16     0     11   Democ…          0       78.3\n# ℹ 1,492 more rows\n\n\n\n\nReferences"
  },
  {
    "objectID": "codebooks/mn-schools.html",
    "href": "codebooks/mn-schools.html",
    "title": "mn-schools.csv",
    "section": "",
    "text": "The data in mnSchools.csv were collected from http://www.collegeresults.org and contain 2011 institutional data for \\(n=33\\) Minnesota colleges and universities. The attributes include:\n\nname: College/university name\ngrad: Six-year graduation rate (as a percentage)\npublic: Sector (1 = public college/university, 0 = private college/university)\nsat: Estimated median composite SAT score (in hundreds)\ntuition: Amount of tuition and required fees covering a full academic year for a typical student (in thousands of U.S. dollars)\n\n\nPreview\n\n# Import data\nmn = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/mn-schools.csv\")\n\n# View data\nmn\n\n# A tibble: 33 × 6\n      id name                               grad public   sat tuition\n   &lt;dbl&gt; &lt;chr&gt;                             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 Augsburg College                   65.2      0  10.3    39.3\n 2     3 Bethany Lutheran College           52.6      0  10.6    30.5\n 3     4 Bethel University, Saint Paul, MN  73.3      0  11.4    39.4\n 4     5 Carleton College                   92.6      0  14      54.3\n 5     6 College of Saint Benedict          81.1      0  11.8    43.2\n 6     7 Concordia College at Moorhead      69.4      0  11.4    36.6\n 7     8 Concordia University-Saint Paul    47.9      0   9.9    37.8\n 8     9 Crossroads College                 26.9      0   9.7    25.3\n 9    10 Crown College                      51.3      0  10.3    33.2\n10    11 Gustavus Adolphus College          81.7      0  12.2    43.8\n# ℹ 23 more rows"
  },
  {
    "objectID": "codebooks/mammals.html",
    "href": "codebooks/mammals.html",
    "title": "mammals.csv",
    "section": "",
    "text": "The data in mammals.csv come from Allison & Cicchetti (1976) and contain data on 62 species of mammals. The attributes include:\n\nspecies: Common name of species\nbody_weight: Body weight, in kg\nbrain_weight: Brain weight, in g\nslow_wave: Average slow wave (non-dreaming) sleep per day, in hours\nparadox: Average paradoxical (dreaming) sleep per day, in hours\ntotal_sleep: Average sleep per day, in hours\nlifespan: Average lifespan, in years\ngestation: Average gestation period, in days\npredation: Rating of degree to which species are preyed upon, 1 (low)–5 (high)\nexposure: Rating of sleep exposure from 1 (low exposure; e.g., sleep in a burrow or den)–5 (maximally exposed sleep)\ndanger: Rating of predatory danger based on predation and exposure scales, 1 (low danger)–5 (high degreee of danger)\n\n\nPreview\n\n# Import data\nmammals = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/mammals.csv\")\n\n# View data\nmammals\n\n# A tibble: 62 × 11\n   species       body_weight brain_weight slow_wave paradox total_sleep lifespan\n   &lt;chr&gt;               &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 African elep…    6654           5712        NA      NA           3.3     38.6\n 2 African gian…       1              6.6       6.3     2           8.3      4.5\n 3 Arctic fox          3.38          44.5      NA      NA          12.5     14  \n 4 Arctic groun…       0.92           5.7      NA      NA          16.5     NA  \n 5 Asian elepha…    2547           4603         2.1     1.8         3.9     69  \n 6 Baboon             10.6          180.        9.1     0.7         9.8     27  \n 7 Big brown bat       0.023          0.3      15.8     3.9        19.7     19  \n 8 Brazilian ta…     160            169         5.2     1           6.2     30.4\n 9 Cat                 3.3           25.6      10.9     3.6        14.5     28  \n10 Chimpanzee         52.2          440         8.3     1.4         9.7     50  \n# ℹ 52 more rows\n# ℹ 4 more variables: gestation &lt;dbl&gt;, predation &lt;dbl&gt;, exposure &lt;dbl&gt;,\n#   danger &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAllison, T., & Cicchetti, D. V. (1976). Sleep in mammals: Ecological and constitutional correlates. Science, 194(4266), 732–734. doi:10.1126/science.982039"
  },
  {
    "objectID": "codebooks/fertility.html",
    "href": "codebooks/fertility.html",
    "title": "fertility.csv",
    "section": "",
    "text": "Human overpopulation is a growing concern and has been associated with depletion of Earth’s natural resources (water is a big one that ) and degredation of the environment. This, in turn, has social and economic consequences such as global tension over resources such as water and food, higher cost of living and higher unemployment rates. The data in fertility.csv were collected from several sources (e.g., World Bank) and are thought to correlate with fertility rates, a measure directly linked to population. The variables are:\n\ncountry: Country name\nregion: Region of the world\nfertility_rate: Average number of children that would be born to a woman if she were to live to the end of her childbearing years and bear children in accordance with age-specific fertility rates.\neduc_female: Average number of years of formal education (schooling) for females\ninfant_mortality: Number of infants dying before reaching one year of age, per 1,000 live births in a given year.\ncontraceptive: Percentage of women who are practicing, or whose sexual partners are practicing, any form of contraception. It is usually measured for women ages 15–49 who are married or in union.\ngni_class: Categorization based on country’s gross national income per capita (calculated using the World Bank Atlas method)\n\nLow: Low-income economies; GNI per capita of $1,025 or less;\nLow/Middle: Lower-middle-income economies; GNI per capita between $1,026 and $3,995;\nUpper/Middle: Upper middle-income economies; GNI per capita between $3,996 and $12,375;\nUpper: High-income economies; GNI per capita of $12,376 or more.\n\nhigh_gni: Dummy variable indicating if the country is has an upper-middle or high income economy (low- or low/middle-income = 0; upper/middle or upper income = 1)\n\n\nPreview\n\n# Import data\nfertility = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/fertility.csv\")\n\n# View data\nfertility\n\n# A tibble: 124 × 7\n   country      region fertility_rate educ_female infant_mortality contraceptive\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Albania      Europ…           1.49         9.1             15              46\n 2 Algeria      Middl…           2.78         5.9             17.2            57\n 3 Armenia      Europ…           1.39        10.8             14.7            57\n 4 Austria      Europ…           1.42         8.9              3.3            66\n 5 Azerbaijan   Europ…           1.92        10.5             30.8            55\n 6 Bahamas, The Latin…           1.97        11.1             13.9            45\n 7 Bangladesh   South…           2.5          4.6             33.1            62\n 8 Belgium      Europ…           1.65        10.5              3.4            67\n 9 Belize       Latin…           3.08         9.2             15.7            51\n10 Benin        Sub-S…           5.13         2               58.5            16\n# ℹ 114 more rows\n# ℹ 1 more variable: gni_class &lt;chr&gt;\n\n\n\n\nReferences\nRoser, M. (2017). Fertility rate. Our world in data.\nUNICEF. (2016). State of the world’s children 2016. United Nations Population Division’s World Contraceptive Use, household surveys including Demographic and Health Surveys and Multiple Indicator Cluster Surveys.\nWorld Bank (2019). World Bank open data."
  },
  {
    "objectID": "class-policies.html",
    "href": "class-policies.html",
    "title": "Class Policies",
    "section": "",
    "text": "Policy for Missing Class and Making up Missed/Late Work\nStudents are responsible for planning their schedules to avoid excessive conflicts with course requirements and must notify the instructor of unavoidable scheduling conflicts as early as possible. For circumstances where absences are unavoidable, accommodations for makeup work will be made according to University Policy. If you miss class:\n\nEmail the instructor as soon as you know you will be missing class.\nStudents are expected to obtain notes from a classmate of class material missed.\nPlease note that I will not be recording class sessions at the request of individual students, nor will I be Zooming students in to the class. Although, if you can arrange it with a classmate, they can Zoom you in.\n\nIf you are zooming in a classmate, please let the instructor know.\n\nIf you will be gone the day an assignment is due, you will need to make arrangements with the instructor about when you will turn in the assignment.\n\nIf you do not communicate with the instructor and make arrangements for turning in work when you are absent, the assignment will receive a 0.\n\n\n\nChatGPT and Other AI Policy\nArtificial intelligence (AI) language models, such as ChatGPT, are emerging tools that we will all need to consider in the next few years. These tools may be used to help you write R syntax, but not for answering any of the other questions on the assignment. If you are in doubt as to whether you are using AI language models appropriately in this course, I encourage you to discuss your situation with me or the TA. If you do use any AI to help write syntax for any of the assignment questions you need to do the following:\n\nCite this in the assignment; failure to do so is in violation of the academic integrity policy at UMN. Please include a paragraph at the end of any assignment that uses AI explaining what you used the AI for and what prompts you used to get the results. More examples of citing AI language models are available at: https://libguides.umn.edu/chatgpt.\nInclude a statement about what you will do to offset the environmental impact of the energy use incurred by using the AI. Be specific in this statement. For example: I will bike to school for the next week (Jan. 01–Jan 07) rather than drive or bus.\n\nYou are also responsible for ensuring any syntax composed by AI is correct. I would not trust anything AI says. If it gives you a number, fact, or code, assume it is wrong unless you either know the answer or can check in with another source. For this reason, using AI works best for topics you understand and can quality check. Here are a couple of other things to be aware of if you decide to use AI:\n\nThe environmental impact of AI should not be ignored. The building and usage of AI tools consumes a lot of energy (see here and here).\nThe development of AI has been associated with labor abuse and other dicey ethical issues (see here and here).\n\nIf you are caught using AI to answer questions on any of the assignments, you will be given a 0 on that assignment and a Scholastic Dishonesty Report Form will be filled out and reported to the Office of Community Standards. Depending on the severity of the offense, the instructor also reserves the right to give you an “F” in the course.\n\n\n\nTechnology Policy\nThe course uses technology on a regular basis during both instruction and assessments (e.g., homework assignments, exams, etc.). Student difficulty with obtaining or operating the various software programs and technologies—including printer trouble—will not be acceptable as an excuse for late work. Due to the variation in computer types and systems, the instructor or TA may not be able to assist in trouble shooting all problems you may have.\n\n\n\n\nCEHD Policy on Recording Classes\nAll class sessions may be recorded by the instructor using the procedures in the CEHD Policy on Recording Classes, with or without prior notice. Students should assume that a class session is being recorded unless otherwise notified. No person (student or otherwise) may record a class without express written permission from the instructor or an authorized administrator implementing a disability accommodation. All permitted recordings are governed by this policy’s limits on distribution and redistribution of recordings."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments and Grading",
    "section": "",
    "text": "Students will complete six homework assignments. The homework assignments and due dates are posted below. These assignments include problems that will help you learn the course material through reflection and practice. Submit each assignment as a PDF file via email to the TA.\nTo foster cooperation and collaboration, you are permitted to form groups of no larger than three to work on the homework. Submit only one assignment per group, and list the names of each group member on the assignment. Each assignment will be scored and this score will be given to all individuals in the group. From past experience, student collaborations work most fluidly when everyone in the group has chosen the same grading option for the course (e.g., A/F, S/N, etc.).\nIf you work alone on the assignments, you need to truly work alone. To protect against running afoul of the scholastic dishonesty policy, students working alone are not permitted to interact with any other student in regards to the assignment, including discussion, obtaining help, etc."
  },
  {
    "objectID": "assignments.html#assignment-due-dates",
    "href": "assignments.html#assignment-due-dates",
    "title": "Assignments and Grading",
    "section": "Assignment Due Dates",
    "text": "Assignment Due Dates\nBelow are the due dates for the assignments, as well as links to each assignment. All assignments are due by 5:00 PM on the due date. The due dates may change at the instructor’s discretion. Any revised due dates will be announced in class and posted to the website.\n\n\n\n\n\nAssignment\n\n\nDue Date\n\n\nQMD\n\n\nHTML\n\n\n\n\n\n\nAssignment #1: Introduction to Quarto\n\n\n\nFeb. 08 [Closed]\n\n\n \n\n\n \n\n\n\n\nAssignment #2: Polynomial Effects\n\n\n\nFeb. 22 [Closed]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment #3: Evidence and Model Selection\n\n\n\nMar. 12 [Closed]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment #4: Logarithmic Transformations\n\n\nMar. 26 [Closed]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment #5: Logistic Regression\n\n\n\nApr. 11 [Closed]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment #6: Linear Mixed-Effects Regression\n\n\nApr. 30 (due by 10:00am) [Closed]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStress Management\nStress management is an important piece of the skill set needed for success in graduate school. Pet Away Worry & Stress (PAWS) is one of the many resources available to students. Find out more at https://boynton.umn.edu/paws.\n\nYou can follow Tilly the Therapy Chicken on Twitter (@TherapyChicken)."
  },
  {
    "objectID": "assignments.html#evaluation-of-student-performance",
    "href": "assignments.html#evaluation-of-student-performance",
    "title": "Assignments and Grading",
    "section": "Evaluation of Student Performance",
    "text": "Evaluation of Student Performance\nCourse grades will be based entirely on performance on the homework assignments. The points from the six homework assignment will be pooled to compute a percentage in the class, which will be converted to a final course grade using:\n\n\n\n\n\n\n\n\nCutoff\nGrade\nDefinition for Graduate Credit\n\n\n\n\n93%–100%\nA\nFor exceptional work, well above the minimum criteria\n\n\n90%–92%\nA–\nFor outstanding work, well above the minimum criteria\n\n\n87%–89%\nB+\nFor excellent work, significant above the minimum criteria\n\n\n83%–86%\nB\nFor work above the minimum criteria\n\n\n80%–82%\nB–\n\n\n\n77%–79%\nC+\n\n\n\n73%–76%\nC\nFor work which meets the course requirements in every respect\n\n\n70%–72%\nC–\n\n\n\n63%–69%\nD\nWorthy of credit even though it fails to meet the course requirements\n\n\n0%–62%\nF\nFailed to meet minimum course requirements\n\n\n\nIf you are taking the course S/N, the minimum criterion to receive an S is 80% (the equivalent of a B– letter grade). The S grade does not carry grade points and is not part of the GPA calculation, but the credits will count toward the student’s degree program if allowed by the college, campus, or program.\nAny student who does not complete all homework assignments without making prior arrangements with the instructor will receive a grade of F/N.\n\n\nIncomplete\nInstructors may assign the registration symbol “I” for Incomplete if, at the time the incomplete is requested: (1) the student has successfully completed a substantial portion of the work of the course; and (2) due to extraordinary circumstances (as determined by the instructor), the student was prevented from completing the work of the course on time. The assignment of an “I” requires a written agreement with the student specifying the time and manner in which the student will complete the course requirements. For more information see Grading and Transcripts.\n\n\n\nAccessing Course Grades\nShortly after the course, you may access your grades online at myU. Assignments will be handed back in class or during office hours. Uncollected assignments will be retained for six weeks after the course and then discarded."
  },
  {
    "objectID": "assignments.html#assignment-faqs",
    "href": "assignments.html#assignment-faqs",
    "title": "Assignments and Grading",
    "section": "Assignment FAQs",
    "text": "Assignment FAQs\nHow do I submit the assignment?\nCreate a PDF of your responses and submit the PDF via email to both the instructor and TA. Also cc any group members. Before you submit the assignment check that:\n\nAll group members’ names are on the assignment.\nAll tables are numbered and have a caption.\nAll figures are numbered and have a caption.\nAll figures are re-sized to not take up more page space than is necessary to read them.\nNo R syntax is included unless the question specifically asked for the syntax to be included. If there is R syntax included, be sure that it is typeset in a monospaced font (e.g., Courier, Inconsolata).\nDo not submit the script file you used unless the directions specifically ask you to submit it.\n\nDo I need to turn in the script file?\nNo…unless the directions specifically ask you to submit the script file. The script file is for your reference. Future assignments will sometimes have questions that reference older assignments, so an organized and well-commented script file is a good idea. Moreover, the TA or myself may ask you to submit your syntax after you submit the assignment to help us interpret mistakes you made on the assignment so that we can provide more thorough feedback.\nWill you provide answer keys to the assignments?\nNo. You will get back several comments on your assignments that address what you did wrong. The educational research is pretty clear that feedback is more helpful to student learning than providing correct answers. If you have further questions or need additional clarification, you are welcome to come to office hours or make an appointment with the instructor or TA.\nWill you go over the answers in class?\nNo. I will address broad concepts if several students/groups made the same mistake, but otherwise, since each student/group makes unique mistakes going over the assignments more broadly is not a good use of our limited class time. The feedback should be helpful in understanding any mistakes you made, but if it isn’t, you are welcome to come to office hours or make an appointment with the instructor or TA.\nWill you look at our assignment prior to us submitting it?\nThe instructor or TA will not “pre-grade” your assignment. If there is a “Preparation” part of the assignment we can ensure that you did that part correctly, and we can also give you feedback about syntax, but we will not tell you if an answer is correct or not. We can also answer clarifying questions (e.g., ‘I know Question 8 is asking about […], but I’m not sure what this question is looking for. Is there another way to restate this question?’). If you completed the assignment without any trouble, then be confident in your work and simply submit the assignment! 😄 If you are unsure about something specific, then ask about that specific thing rather than a general ‘did I do this right’!\nWill we be able to re-do the assignment?\nGenerally no. Since you have the opportunity to work in groups on the assignments, you should be able offset any of the issues that come up that would necessitate a re-submission. The exception to this is that we may allow you to re-do Assignment 1. If we do this, we will send you an email requesting that you resubmit part, or all of Assignment 1. You will not receive much feedback on your assignment prior to your re-submission. This is because during the first assignment you are learning not only content, but also expectations around how to respond to the questions we ask."
  },
  {
    "objectID": "codebooks/carbon.html",
    "href": "codebooks/carbon.html",
    "title": "carbon.csv",
    "section": "",
    "text": "Carbon dioxide emissions are the primary driver of global climate change. Some of the biggest predictors of these emissions include economic growth, industrialization, and urbanization. The data in carbon.csv are from 2017 and include some predictors of carbon dioxide emissions for several countries around the world. The variables are:\n\ncountry: Country name\nregion: Region of the world\nco2: Carbon dioxide measure from the burning of fossil fuels (metric tons per person)\nwealth: A measure of a country’s wealth based on its GDP; higher values indicate wealthier countries\nurbanization: Annual urban population growth (as a percentage)\n\n\nPreview\n\n# Import data\ncarbon = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/carbon.csv\")\n\n# View data\ncarbon\n\n# A tibble: 189 × 5\n   country             region      co2 wealth urbanization\n   &lt;chr&gt;               &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan         Asia      0.254   1.07        3.35 \n 2 Albania             Europe    1.59    3.75        1.32 \n 3 Algeria             Africa    3.69    3.54        2.81 \n 4 Angola              Africa    1.12    2.71        4.31 \n 5 Antigua and Barbuda Americas  5.88    4.36        0.432\n 6 Argentina           Americas  4.41    4.49        1.15 \n 7 Armenia             Europe    1.89    3.72        0.309\n 8 Australia           Oceania  16.9     5.60        1.66 \n 9 Austria             Europe    7.75    5.82        0.836\n10 Azerbaijan          Europe    3.7     3.79        1.47 \n# ℹ 179 more rows\n\n\n\n\nReferences\nWorld Bank (2019). World Bank open data."
  },
  {
    "objectID": "codebooks/graduation.html",
    "href": "codebooks/graduation.html",
    "title": "graduation.csv",
    "section": "",
    "text": "The data in graduation.csv include student-level attributes for \\(n=2,344\\) randomly sampled students who were first-year, full-time students from the 2002 cohort at a large, midwestern research university. Any students who transferred to another institution were removed from the data. The source of these data is Jones-White, Radcliffe, Lorenz, & Soria (2014). The attributes, collected for these students are:\n\nstudent: Student ID number in the dataset\ndegree: Did the student obtain a degree (i.e., graduate) from the institution? (No; Yes)\nact: Student’s ACT score (If the student reported a SAT score, a concordance table was used to transform the score to a comparable ACT score.)\nscholarship: Amount of scholarship offered to student (in thousands of dollars)\nap_courses: Number of Advanced Placement credits at time of enrollment\nfirst_gen: Is the student a first generation college student? (No; Yes)\nnon_traditional: Is the student a non-traditional student (older than 19 years old at the time of enrollment)? (No; Yes)\n\n\nPreview\n\n# Import data\ngraduation = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/graduation.csv\")\n\n# View data\ngraduation\n\n# A tibble: 2,344 × 7\n   student degree   act scholarship ap_courses first_gen non_traditional\n     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;          \n 1       1 Yes       21         0            0 No        No             \n 2       2 Yes       19         0            0 No        No             \n 3       3 Yes       27         0            0 Yes       No             \n 4       4 Yes       25         0.5          0 Yes       No             \n 5       5 No        28         0           17 Yes       No             \n 6       6 Yes       21         0            0 No        Yes            \n 7       7 Yes       27         0            8 Yes       No             \n 8       8 No        20         0            0 No        No             \n 9       9 Yes       26         0            0 Yes       No             \n10      10 Yes       25         0            4 Yes       No             \n# ℹ 2,334 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nJones-White, D. R., Radcliffe, P. M., Lorenz, L. M., & Soria, K. M. (2014). Priced out?: The influence of financial aid on the educational trajectories of first-year students starting college at a large research university. Research in Higher Education, 55(4), 329–350."
  },
  {
    "objectID": "codebooks/minneapolis.html",
    "href": "codebooks/minneapolis.html",
    "title": "minneapolis.csv",
    "section": "",
    "text": "The data in minneapolis.csv were provided in Long (2012). They constitute a sample of \\(n=22\\) students taken from a much larger dataset collected by the Minneapolis Public School District. The data were collected to comply with the No Child Left Behind Act of 2001 and began during the 2004-05 school year. The variables included in the data are:\n\nstudent_id: De-identified student ID number\nreading_score: Reading achievement score\\(^\\dagger\\).\ngrade: Grade-level.\nspecial_ed: Is the student recieving special education services?\nattendance: Proportion of attendance based on the number of school days the student attended school during the four year of the study.\n\n\\(^\\dagger\\)The reading achievement scores are based on the reading section of th Northwest Achievement Levels Test (NALT), a multiple-choice, adaptive assessment of students’ academic achievement. The NALT raw scores were converted to vertically equated scaled scores using an IRT model. Higher scale scores indicate more reading achievement.\n\nPreview\n\n# Import data\nmpls = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/minneapolis.csv\")\n\n# View data\nmpls\n\n# A tibble: 88 × 5\n   student_id reading_score grade special_ed attendance\n        &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1          1           172     5 No               0.94\n 2          1           185     6 No               0.94\n 3          1           179     7 No               0.94\n 4          1           194     8 No               0.94\n 5          2           200     5 No               0.91\n 6          2           210     6 No               0.91\n 7          2           209     7 No               0.91\n 8          2           210     8 No               0.91\n 9          3           191     5 No               0.97\n10          3           199     6 No               0.97\n# ℹ 78 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLong, J. D. (2012). Longitudinal data analysis for the behavioral sciences using R. Thousand Oaks, CA: Sage Publications, Inc."
  },
  {
    "objectID": "codebooks/nhl.html",
    "href": "codebooks/nhl.html",
    "title": "nhl.csv",
    "section": "",
    "text": "Each season, Team Marketing Report (TMR) computes the cost of taking a family of four to a professional sports contest for each of the major sporting leagues. Costs are determined by telephone calls with representatives of the teams, venues and concessionaires. Identical questions were asked in all interviews. Prices for Canadian teams were converted to U.S. dollars and comparison prices were converted using a recent exchange rate.\nThe data in nhl.csv includes data on the cost of attending an NHL game over nine seasons for the current 31 NHL teams. The attributes include:\n\nteam: NHL team name\nfci: Fan cost index (FCI) for each season. There are no data for 2012, since that year the NHL was locked out. The FCI comprises the prices of four (4) average-price tickets, two (2) small draft beers, four (4) small soft drinks, four (4) regular-size hot dogs, parking for one (1) car, two (2) game programs and two (2) least-expensive, adult-size adjustable caps. Costs were determined by telephone calls with representatives of the teams, venues and concessionaires. Identical questions were asked in all interviews.\nyear: NHL season (e.g., 2002 indicates the 2002–2003 NHL season)\nhs_hockey: An dummy coded variable that indicates whether there is state organized high school hockey in the team’s location (0 = no; 1 = yes). This is a proxy for whether there is a hockey tradition in the team’s location.\n\n\nPreview\n\n# Import data\nnhl = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/nhl.csv\")\n\n# View data\nnhl\n\n# A tibble: 279 × 4\n   team              fci  year hs_hockey\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 Anaheim Ducks    212.  2002         0\n 2 Anaheim Ducks    229.  2003         0\n 3 Anaheim Ducks    211.  2006         0\n 4 Anaheim Ducks    260.  2007         0\n 5 Anaheim Ducks    274.  2008         0\n 6 Anaheim Ducks    285.  2010         0\n 7 Anaheim Ducks    239.  2011         0\n 8 Anaheim Ducks    285.  2013         0\n 9 Anaheim Ducks    289.  2014         0\n10 Arizona Coyotes  214.  2002         0\n# ℹ 269 more rows"
  },
  {
    "objectID": "codebooks/riverview.html",
    "href": "codebooks/riverview.html",
    "title": "riverview.csv",
    "section": "",
    "text": "The data in riverview.csv come from Lewis-Beck & Lewis-Beck (2016) and contain five attributes collected from a random sample of \\(n=32\\) employees working for the city of Riverview, a hypothetical midwestern city. The attributes include:\n\neducation: Years of formal education\nincome: Annual income (in thousands of U.S. dollars)\nseniority: Years of seniority\ngender: Employee’s gender\nmale: Dummy coded gender variable (0 = Female, 1 = Male)\nparty: Political party affiliation\n\n\nPreview\n\n# Import data\ncity = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/riverview.csv\")\n\n# View data\ncity\n\n# A tibble: 32 × 5\n   education income seniority gender     party      \n       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n 1         8   26.4         9 female     Independent\n 2         8   37.4         7 Not female Democrat   \n 3        10   34.2        16 female     Independent\n 4        10   25.5         1 female     Republican \n 5        10   47.0        14 Not female Democrat   \n 6        12   46.5        11 female     Democrat   \n 7        12   52.5        16 female     Independent\n 8        12   37.7        14 Not female Democrat   \n 9        12   50.3        24 Not female Democrat   \n10        14   32.6         5 female     Independent\n# ℹ 22 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLewis-Beck, C., & Lewis-Beck, M. (2016). Applied regression: An introduction (2nd ed.). Thousand Oaks, CA: Sage."
  },
  {
    "objectID": "codebooks/usnwr-2024.html",
    "href": "codebooks/usnwr-2024.html",
    "title": "usnwr-2024.csv",
    "section": "",
    "text": "The data in usnwr-2024.csv includes 13 attributes collected from the n=111 graduate schools of education ranked in the U.S. News and World Report’s 2023–2024 Best Education Graduate Schools."
  },
  {
    "objectID": "codebooks/usnwr-2024.html#attributes",
    "href": "codebooks/usnwr-2024.html#attributes",
    "title": "usnwr-2024.csv",
    "section": "Attributes",
    "text": "Attributes\n\nrank: Rank in USNWR\nschool: Graduate program of Education\npeer_rating: Peer assessment score from deans’, program directors’, and senior faculty’s judgement of the academic quality of the program on a scale of 1 (marginal) to 5 (outstanding).\nenroll: Total enrollment of part-time and full-time graduate students in 2022–23\nft_students: Total number of full-time graduate students enrolled in 2022–23\nft_faculty: Total number of full-time faculty employed in 2022–23\nnonres_tuition: Amount of yearly non-resident graduate tuition\nsf_ratio: Adjusted ratio of graduate students to faculty in 2022–2023\npct_doc: Percentage of students enrolled in a Ph.D./Ed.D. in 2022–23\nug_gpa: Average undergraduate GPA for Ph.D. students admitted in 2022–23\ngre: Averge total GRE score (based on verbal and quantitative) for Ph.D. students admitted in 2022–23\ndoc_accept: Acceptance rate for doctoral students in 2016\ntot_pubs: Total number of publication attributed to faculty in the program from 2019–23 based on SCOPUS\ntot_res: Total amount of funded research (in millions of dollars)"
  },
  {
    "objectID": "codebooks/usnwr-2024.html#preview",
    "href": "codebooks/usnwr-2024.html#preview",
    "title": "usnwr-2024.csv",
    "section": "Preview",
    "text": "Preview\n\n# Import data\nusnwr = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/usnwr-2024.csv\")\n\n# View data\nusnwr |&gt;\n    print(width = Inf)\n\n# A tibble: 111 × 14\n    rank school                                peer_rating enroll ft_students\n   &lt;dbl&gt; &lt;chr&gt;                                       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1     1 Teachers College, Columbia University         4.2   4471        3192\n 2     1 University of Michigan--Ann Arbor             4.4    455         303\n 3     3 Northwestern University                       4.1    382         228\n 4     3 University of Pennsylvania                    4.4   1654        1091\n 5     3 University of Wisconsin--Madison              4.3   1136         729\n 6     6 Vanderbilt University (Peabody)               4.5   1007         809\n 7     7 Stanford University                           4.6    343         343\n 8     7 University of California--Los Angeles         4.3    701         701\n 9     9 Harvard University                            4.4    868         707\n10     9 New York University (Steinhardt)              4.1   2163        1747\n   ft_fac nonres_tuition sf_ratio pct_doc ug_gpa   gre doc_acc tot_pubs tot_res\n    &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1    131          45912      8.4    23.9    3.5   313    12.1      789    65.2\n 2     45          53178      4.2    30.8    3.3   316    13.7     1802    40.9\n 3     39          56067      2.2    22.2    3.7   321     7.8      719    29.2\n 4     75          42864      4.8    23.8    3.3   312    14.6     1176    50.4\n 5    123          24054      5.4    51.1    3.5   305    15.9     1396    64.5\n 6     97          53160      2.6    24.9    3.6   321     5.4     1195    49.9\n 7     51          56487      4.3    46.6    3     327     3.9     1415    25.2\n 8     43          26802      7.1    51.6    3.4   310    28.1     1058    46.5\n 9     42          54032      3.6    14.1    3.9   320     5.7     2148    36.7\n10     79          48480      4.6    14.5    3.7   314     4.7      886    71.4\n# ℹ 101 more rows"
  },
  {
    "objectID": "codebooks/wine.html",
    "href": "codebooks/wine.html",
    "title": "wine.csv",
    "section": "",
    "text": "The data in wine.csv includes data on 200 different wines. These data are a subset of a larger database (\\(n = 6,613\\)) from wine.com, one of the biggest e-commerce wine retailers in the U.S. It allows customers to buy wine according to any price range, grape variety, country of origin, etc. The data were made available at http://insightmine.com/. The attributes include:\n\nwine: Wine name\nvintage: Year the wine was produced (centered so that 0 = 2008, 1 = 2009, etc.)\nregion: Region of the world where the wine was produced. These data include seven regions (Australia, California, France, Italy, New Zealand, South Africa, South America).\nvarietal: Grape varietal These data include nine varietals (Cabernet Sauvignon, Chardonnay, Merlot, Pinot Noir, Sauvignon Blanc, Syrah/Shiraz, Zinfandel, Other Red, Other Whites).\nrating: Wine rating on a 100-pt. scale (these are from sources such as Wine Spectator, the Wine Advocate, and the Wine Enthusiast)\nprice: Price in U.S. dollars\n\n\nPreview\n\n# Import data\nwine = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/wine.csv\")\n\n# View data\nwine\n\n# A tibble: 200 × 6\n   wine                                     vintage region varietal rating price\n   &lt;chr&gt;                                      &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Rust en Vrede Cabernet Sauvignon               4 South… Caberne…     91  30.0\n 2 Bonterra Organically Grown Merlot              4 Calif… Merlot       90  15.0\n 3 Allegrini Palazzo della Torre                  2 Italy  Other R…     90  20.0\n 4 Marcarini Barolo Brunate                       2 Italy  Other R…     94  56.0\n 5 Chateau Beausejour Duffau                      3 France Other R…     96  98.0\n 6 Clos du Marquis                                2 France Other R…     96  60.0\n 7 Altocedro Ano Cero Malbec                      4 South… Other R…     92  19.0\n 8 Stag's Leap Wine Cellars Artemis Cabern…       4 Calif… Caberne…     91  33.0\n 9 Duckhorn Three Palms Merlot                    3 Calif… Merlot       95  89  \n10 Migration Russian River Pinot Noir (375…       4 Calif… Pinot N…     93  20.0\n# ℹ 190 more rows"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Below are the links to the data sets and data codebooks used in the notes, scripts, and assignments."
  },
  {
    "objectID": "data.html#how-to-download-a-csv-file-to-your-computer",
    "href": "data.html#how-to-download-a-csv-file-to-your-computer",
    "title": "Data",
    "section": "How to Download a CSV File to Your Computer",
    "text": "How to Download a CSV File to Your Computer\nTo download a data (CSV) file, click on the data set. Then click the RAW button. Now you should be able to right-click the dataset and save it. If you are using Safari on a Mac, make sure that the downloaded data does not have an extra .txt appended to it when it downloads. If it does, just delete the extra .txt suffix."
  },
  {
    "objectID": "instructor.html",
    "href": "instructor.html",
    "title": "Instructor/TA",
    "section": "",
    "text": "Instructor: Andrew Zieffler  Email: zief0002@umn.edu  Office: Education Sciences Building 178  Office Hours: Wednesday 11:00 AM–12:00 PM; and by appointment  Virtual Office: If you want to meet virtually, send me a Google calendar invite and include a Zoom link in the invite.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTA: Shelby Weisen  Email: weise207@umn.edu  Office: Education Sciences Building 197 or by Zoom  Office Hours: Wednesday 12:00 PM–1:00 PM; and by appointment  Virtual Office: During office hours, Shelby will be available via Zoom. If you want to meet Shelby virtually outside of office hours, send her a Google calendar invite and include a Zoom link in the invite."
  },
  {
    "objectID": "notes/02-project-organization.html",
    "href": "notes/02-project-organization.html",
    "title": "Project Organization",
    "section": "",
    "text": "In this set of notes, you will learn some basic tips and tricks for organizing directories and files for project management. At the end of it, you will have an organized project directory to begin work on Assignment 1."
  },
  {
    "objectID": "notes/02-project-organization.html#naming-conventions",
    "href": "notes/02-project-organization.html#naming-conventions",
    "title": "Project Organization",
    "section": "Naming Conventions",
    "text": "Naming Conventions\nThe naming conventions for the directories and files in our project are as follows:\n\nFile names should be short but descriptive (less than 25 characters)\nAll lowercase letters\nAvoid special characters and spaces in a file name\n\nUse hyphens instead of spaces to separate words (e.g., assignment-01)\n\nAny names that include the date will use the ISO 8601 date format (YYYYMMDD)\nAny names that include a number will include at least two digits (e.g., assignment-01 rather than assignment-1)\n\nAgain, while there is no one best naming convention, it is important that you have one, and that you are consistent throughout the project. That being said, as you develop naming conventions for your projects, all the conventions should be documented! This documentation helps onboard collaborators to your project.\n\nThere are several guides available to help you establish naming conventions including here and here."
  },
  {
    "objectID": "notes/02-project-organization.html#adding-content-to-readme",
    "href": "notes/02-project-organization.html#adding-content-to-readme",
    "title": "Project Organization",
    "section": "Adding Content to README",
    "text": "Adding Content to README\nSince README files are plain text files, they cannot include formatting like bold or italic. However, they do typically include Markdown syntax (which is itself plain text). The plain text nature of these files keeps them small in size and accessible to anyone with any type of computer.\n\nThere are several online guides for what to include in a README file, including here and here. There is also a pretty good template for a README for data science oriented projects here.\n\nSince the README file is informational, you can include any type of information in this file that is useful to the project. For example, you could add your naming conventions to this file.\n# assignment-01\n\nThis directory contains all of the files necessary to complete Assignment 1.\n\n\n# Naming Conventions\n\n- File names should be short but descriptive (less than 25 characters)\n- All lowercase letters\n- Avoid special characters and spaces in a file name\n  + Use hyphens instead of spaces to separate words (e.g., `assignment-01`)\n- Any names that include the date will use the ISO 8601 date format (YYYYMMDD)\n- Any names that include a number will include at least two digits (e.g., `assignment-01` rather than `assignment-1`)\n\nWhile you should only have one README file per directory, you can have different README files in other directories. For example, you could create a README file in the data directory that includes the codebook information for your data files."
  },
  {
    "objectID": "notes/02-project-organization.html#footnotes",
    "href": "notes/02-project-organization.html#footnotes",
    "title": "Project Organization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR projects have the same name as the root directory you associated it with and have the file suffix .Rproj. Our directory tree is now:↩︎"
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html",
    "href": "notes/03-introduction-to-quarto.html",
    "title": "📖 Introduction to Quarto",
    "section": "",
    "text": "In this set of notes, you will continue your Quarto journey. One important note before we begin:"
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#add-a-code-chunk-to-load-libraries-and-import-data",
    "href": "notes/03-introduction-to-quarto.html#add-a-code-chunk-to-load-libraries-and-import-data",
    "title": "📖 Introduction to Quarto",
    "section": "Add a Code Chunk to Load Libraries and Import Data",
    "text": "Add a Code Chunk to Load Libraries and Import Data\nThe first question on Assignment 1 is:\n\nImport the data and display the first several rows of data (not all of it). Use one of the paged table options in your YAML to ensure that this is printed nicely. All syntax for these commands should be hidden.\n\n\n\nCreate a Level-2 Heading in your QMD document that is called “Question 1”.\nThen create a new R chunk. In that chunk load the {tidyverse} library. Don’t forget to add a comment.\nIn the same R chunk, import the woods.csv data into an object called woods.\n\n\nTo import the data (which you put in the data directory of your project last class) use the following syntax:\n\nwoods = read_csv(\"data/woods.csv\")\n\nThe path inside the quotation marks gives the location (relative to the QMD file) of the woods.csv data. Here, the data are in the directory called data. Thus the path data/woods.csv indicates go to the data folder and inside of that locate woods.csv.\n\nWhat would the path be inside the quotation marks if our project had the following tree?\nassignment-01\n    ├── README\n    ├── assets\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       ├── assignment-data\n          └── woods.csv\n       └── notes-data\n    ├── figs\n    └── scripts\n\n\nUse the first code chunk in your Quarto document to load all the packages and datasets used in the document. Label this chunk setup. This has the added advantage that others can immediately see what packages and datasets are needed to run the document. Labelling it setup also runs that chunk when you try to run other chunks, so your code works!"
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#displaying-data",
    "href": "notes/03-introduction-to-quarto.html#displaying-data",
    "title": "📖 Introduction to Quarto",
    "section": "Displaying Data",
    "text": "Displaying Data\n\nIn the same R chunk display the data by typing the data object name after you import the data.\n\n\nRendering the document should display the entire data set in your outputted HTML file! In general, we do not want to take the space (especially when data sets are large) to do this. One option is to change the way that data frames/tibbles are printed in the Quarto document. To do this we need to update our YAML.\n\nUpdate the YAML of your QMD to the following:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat:\n  html:\n    df-print: paged\neditor: visual\n---\nRe-render the document.\n\n\nThis sets the printing option for data frames into an HTML table that has clickable paging when the data are large. The Quarto documentation Data Frames, includes other options for printing data frames.\n\nPay attention to the spacing in the YAML!!!!"
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#add-a-code-chunk-to-create-a-plot",
    "href": "notes/03-introduction-to-quarto.html#add-a-code-chunk-to-create-a-plot",
    "title": "📖 Introduction to Quarto",
    "section": "Add a Code Chunk to Create a Plot",
    "text": "Add a Code Chunk to Create a Plot\n\nCreate another Level-1 header in your document called “Scatterplot”. Then create a code chunk in which you create a scatterplot of infant mortality rates (y-axis) versus female education level (x-axis).\n\n\nIt is a good idea to include a label in the chunk options for each plot you create. A label is the name of this code chunk. (Any code chunk, even if it is not a plot, can have a label.) In the plot below, the chunk’s label is fig-scatterplot.\n\n```{r}\n#| label: fig-scatterplot\n#| fig-cap: \"Scatterplot showing the relationship between political corruption and environmental program strength.\"\nggplot(data = woods, aes(x = envprogstr, y = corrupt)) +\n   geom_point()\n```\n\n\n\n\n\n\n\nFigure 1: Scatterplot showing the relationship between political corruption and environmental program strength.\n\n\n\n\n\nNotice that by labeling this chunk, the figure is numbered in the document. We also added a caption using the fig-cap: option.\n\n\nFigure Size\nThere are a couple other chunk options for figures that help set the aspect ratio and size of the figure in your document. The aspect ratio is the ratio of a figure’s width to its height. We set this using the fig-width: and fig-height: chunk options. (The default value for both is 6; thus the figure appears square.) Below we change these so that the figure appears wider than it is tall.\n\n```{r}\n#| label: fig-scatterplot-2\n#| fig-cap: \"Scatterplot showing the relationship between political corruption and environmental program strength.\"\n#| fig-width: 9\n#| fig-height: 5\nggplot(data = woods, aes(x = envprogstr, y = corrupt)) +\n   geom_point()\n```\n\n\n\n\n\n\n\nFigure 2: Scatterplot showing the relationship between political corruption and environmental program strength.\n\n\n\n\n\n\nIt is important to change the aspect ratio when you have legends or when you are using {patchwork} to stack or put multiple figures next to each other. Many times you need to find a good aspect ratio though trial-and-error of trying different values.\n\nThe actual size of the figure in the document is independent of the aspect ratio and can be set using the out-width: or out-height: chunk option. These options take a character string of the direct size (in HTML documents this is typically in pixels) or of the percentage of the output width/height. Here we keep the same aspect ratio, but make the figure smaller by setting the figure width to 40% of the document’s width.\n\n```{r}\n#| label: fig-scatterplot-3\n#| fig-cap: \"Scatterplot showing the relationship between political corruption and environmental program strength.\"\n#| fig-width: 9\n#| fig-height: 5\n#| out-width: \"40%\"\nggplot(data = woods, aes(x = envprogstr, y = corrupt)) +\n   geom_point()\n```\n\n\n\n\n\n\n\nFigure 3: Scatterplot showing the relationship between political corruption and environmental program strength.\n\n\n\n\n\n\nIf you set the aspect ratio of the figure, you need only set out-width: or out-height:. You don’t need to set both as the other will be determined by the aspect ratio."
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#inline-code-chunks-for-better-reproducibility",
    "href": "notes/03-introduction-to-quarto.html#inline-code-chunks-for-better-reproducibility",
    "title": "📖 Introduction to Quarto",
    "section": "Inline Code Chunks for Better Reproducibility",
    "text": "Inline Code Chunks for Better Reproducibility\nIn writing papers where there are results from data analyses being reported in the text, inline code chunks are boss! For example, consider writing the following sentence in an analysis of the woods.csv data.\n\nThe mean environmental program strength is 17.6 (SD = 8.23).\n\nRather than computing these values and then transposing the values into the sentence, we can use inline code chunks to directly compute and write the values in the sentence. Here is some syntax we could use:\nThe mean environmental program strength is `r mean(woods$envprogstr)` (SD = `r sd(woods$envprogstr)`).\nThis produces the following sentence in the rendered document:\n\nThe mean environmental program strength is 17.6 (SD = 8.2263886).\n\n\n\nRounding\nThere are several ways to set the rounding to two decimal places. One is to embed the computation in the round() function. For example, in the first inline chunk we could use: round(mean(woods$envprogstr), 2).\nYou can also do this in a separate code chunk and then call the values in the inline computation (as suggested in the Quarto Computations Tutorial)."
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#better-typesetting-of-equations",
    "href": "notes/03-introduction-to-quarto.html#better-typesetting-of-equations",
    "title": "📖 Introduction to Quarto",
    "section": "Better Typesetting of Equations",
    "text": "Better Typesetting of Equations\nWhen we typeset equations, there are a couple things we should do:\n\nUse variable names that make sense to our reader, rather than their name in our dataset/R. For example “Environmental Program Strength” is a better name than “envprogstr”.\nVariable names should also be typeset using normal text rather than italic (the default in mathematical expressions).\n\nWe can fix both issues by including the variable names in a text environment (\\text{}). So to get the name “Environmental Program Strength” we need to use the following in our equation:\n$$\n\\text{Environmental Program Strength}\n$$\nOther useful environments include: \\mathit{} (italics), \\mathbf{} (bold), \\mathtt{} (typewriter text), and \\mathcal{} (caligraphy; this is useful for the “N” we use to indicate a normal distribution). If you want a space when using these, you include a tilde (~) to denote a space.\n\nHyphens need special syntax since a hyphen would be interpreted as a minus sign. In typesetting, the minus sign is longer than the hyphen symbol.\n\nIf you want to include a hyphen, we need to include it in \\mbox{}. For example, to add a hyphen in our variable name to get “Check-In”, we use:\n$$\n\\text{Check\\mbox{-}In}\n$$\n\nUpdate your fitted equation from before to include spaces in variable names. Also be sure the variable names are written in normal font."
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#spacing-out-the-lines",
    "href": "notes/03-introduction-to-quarto.html#spacing-out-the-lines",
    "title": "📖 Introduction to Quarto",
    "section": "Spacing Out the Lines",
    "text": "Spacing Out the Lines\nYou can add space between the lines of your multiline equation by including a unit of measurement between square brackets after the double backslashes.\n$$\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\[1em]\n&= 7\n\\end{split}\n$$\nHere the resulting display equation is:\n\\[\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\[1em]\n&= 7\n\\end{split}\n\\]\nHere we have added line space of 1 em. An em is a unit for measuring the width of printed work, equal to the height of the type size being used (typically the width of the letter “m”). Other common printing units include the en (the width of the letter “n”), and the ex (the width of the letter “x”).\n\nQuestion #8 actually asks:\n\nUse a display equation to write the equation for the underlying regression model (including error and assumptions) using Greek letters, subscripts, and variable names. Also write the equation for the fitted least squares regression equation based on the output from lm(). Type these two equations in the same display equation, each on a separate line of text in your document, and align the equals signs.\n\nTry writing this multiline equation."
  },
  {
    "objectID": "notes/04-likelihood-evidence.html",
    "href": "notes/04-likelihood-evidence.html",
    "title": "Likelihood: A Framework for Evidence",
    "section": "",
    "text": "In this set of notes, you will learn about the law of likelihood, and the use of likelihood ratios as statistical evidence for model selection. To do so, we will use the usnwr-2024.csv dataset (see the data codebook) to fit a set of models that explain variation in peer ratings of graduate schools of education.\n\n# Load libraries\nlibrary(broom)\nlibrary(educate)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(tidyr)\n\n# Import data\nusnwr = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/usnwr-2024.csv\")\n\n# View data\nusnwr"
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#an-example-of-computing-and-evaluating-likelihood",
    "href": "notes/04-likelihood-evidence.html#an-example-of-computing-and-evaluating-likelihood",
    "title": "Likelihood: A Framework for Evidence",
    "section": "An Example of Computing and Evaluating Likelihood",
    "text": "An Example of Computing and Evaluating Likelihood\nThe likelihood allows us to answer probability questions about a set of parameters. For example, what is the likelihood (probability) that the data (\\(x = \\{30, 20, 24, 27\\}\\)) were generated from a normal distribution with a mean of 20 and standard deviation of 4? To compute the likelihood we compute the joint probability density of the data under that particular set of parameters.\n\nprod(dnorm(x = c(30, 20, 24, 27), mean = 20, sd = 4))\n\n[1] 0.0000005702554\n\n\nWhat is the likelihood (probability) that the same set of data (\\(x = \\{30, 20, 24, 27\\}\\)) were generated from a normal distribution with a mean of 25 and standard deviation of 4?\n\nprod(dnorm(x = c(30, 20, 24, 27), mean = 25, sd = 4))\n\n[1] 0.00001774012\n\n\nGiven the data and the model, there is more empirical support that the parameters are \\(\\mathcal{N}(25,4^2)\\) rather than \\(\\mathcal{N}(20, 4^2)\\), because the likelihood is higher for the former set of parameters. We can compute a ratio of the two likelihoods to quantify the amount of additional support for the \\(\\mathcal{N}(25,4^2)\\).\n\\[\n\\begin{split}\n\\mathrm{Likelihood~Ratio} &= \\frac{0.00001774012}{0.0000005702554} \\\\[1ex]\n&= 31.11\n\\end{split}\n\\]\nThe empirical support for the \\(\\mathcal{N}(25,4^2)\\) parameterization is 31 times that of the \\(\\mathcal{N}(20, 4^2)\\) parameterization! In a practical setting, this would lead us to adopt a mean of 25 over a mean of 20."
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#log-likelihood",
    "href": "notes/04-likelihood-evidence.html#log-likelihood",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\nThe likelihood values are quite small since we are multiplying several probability densities (values between 0 and 1) together. Since it is hard to work with these smaller values, in practice, we often compute and work with the natural logarithm of the likelihood. So in our example, Model 0 (\\(\\mathcal{L}_0 = 5.452323 \\times 10^{-26}\\)) has a log-likelihood of:\n\n# Log-likelihood for Model 0\nlog(5.452323e-26)\n\n[1] -58.17117\n\n\nSimilarly, we can compute the log-likelihood for Model 1 as:\n\n# Log-likelihood for Model 1\nlog(1.680643e-20)\n\n[1] -45.53253\n\n\nWe typically denote log-likelihood using a scripted lower-case “l” (\\(\\mathcal{l}\\)). Here,\n\\[\n\\begin{split}\n\\mathcal{l}_0 &= -58.17117 \\\\[1ex]\n\\mathcal{l}_1 &= -45.53253 \\\\[1ex]\n\\end{split}\n\\]\nNote that the logarithm of a decimal will be negative, so the log-likelihood will be a negative value. Less negative log-likelihood values correspond to higher likelihood values, which indicate more empirical support. Here Model 1 has a log-likelihood value (\\(-58.2\\)) that is less negative than Model 0’s log-likelihood value (\\(-45.5\\)), which indicates there is more empirical support for Model 1 than Model 0.\nWe can also express the likelihood ratio using log-likelihoods. To do so we take the natural logarithm of the likelihood ratio. We also re-write it using the rules of logarithms from algebra.\n\\[\n\\begin{split}\n\\ln(\\mathrm{LR}) &= \\ln \\bigg(\\frac{\\mathcal{L}_2}{\\mathcal{L}_1}\\bigg) \\\\[2ex]\n&= \\ln \\big(\\mathcal{L}_2\\big) - \\ln \\big(\\mathcal{L}_1\\big)\n\\end{split}\n\\]\nThat is, we can find an equivalent relative support metric to the LR based on the log-likelihoods by computing the difference between them. For our example:\n\n# Difference in log-likelihoods\nlog(1.680643e-20) - log(5.452323e-26)\n\n[1] 12.63865\n\n# Equivalent to ln(LR)\nlog(1.680643e-20 / 5.452323e-26)\n\n[1] 12.63865\n\n\nUnfortunately, this difference doesn’t have the same interpretational value as the LR does, bcause this difference is in log-units. In order to get that interpretation back, we need to exponentiate (the reverse function of the logarithm) the difference:\n\n# Exponentiate the difference in log-likelihoods\nexp(12.63865)\n\n[1] 308244.9\n\n\nAgain, Model 1 has 308244.9 times the empirical support than Model 0.\n\nMathematics of Log-Likelihood\nWe can express the log-likelihood of the regression residuals mathematically by taking the natural logarithm of the likelihood we computed earlier:\n\\[\n\\begin{split}\n\\ln \\Bigl(\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data})\\Bigr) &= \\ln \\Biggl( \\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times  \\\\\n&~~~~~~ \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times \\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right] \\Biggr) \\\\\n\\end{split}\n\\]\nUsing our rules for logarithms and re-arranging gives,\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\n\\]\nExamining this equation, we see that the log-likelihood is a function of \\(n\\), \\(\\sigma^2_{\\epsilon}\\) and the sum of squared residuals (SSR)1. We can of course, re-express this using the the regression parameters:\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\big[Y_i - \\beta_0 - \\beta_1(X_i)\\big]^2\n\\]\nAnd, again, since \\(\\sigma^2_{\\epsilon}\\) is a function of the regression coefficients and \\(n\\), this means that the only variables in the log-likelihood function are the coefficients."
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#shortcut-the-loglik-function",
    "href": "notes/04-likelihood-evidence.html#shortcut-the-loglik-function",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Shortcut: The logLik() Function",
    "text": "Shortcut: The logLik() Function\nThe logLik() function can be used to obtain the log-likelihood directly from a fitted model object. For example, to find the log-likelihood for Model 1, we can use:\n\n# Compute log-likelihood for Model 1\nlogLik(lm.1)\n\n'log Lik.' -45.44917 (df=6)\n\n\nThe df output tells us how many total parameters are being estimated in the model. In our case this is six (\\(\\beta_0\\), \\(\\beta_{\\mathrm{Enrollment}}\\), \\(\\beta_{\\mathrm{FT~Students}}\\), \\(\\beta_{\\mathrm{FT~Faculty}}\\), \\(\\beta_{\\mathrm{Tuition}}\\), and \\(\\sigma^2_{\\epsilon}\\)). What is more important to us currently, is the log-likelihood value; \\(\\mathcal{l}_1=-45.44917\\).\nThis value is slightly different than the log-likelihood we just computed of \\(-45.53253\\). This is not because of rounding in this case. It has to do with how the model is being estimated; the logLik() function assumes the parameters are being estimated using maximum likelihood (ML) rather than ordinary least squares (OLS). You can learn more about ML estimation in the optional set of notes, but for now, we will just use logLik() to compute the log-likelihood.\nHere we compute the log-likelihood for Model 0 using the logLik() function. We also use the output to compute the likelihood, and the likelihood ratio between Model 1 and Model 0\n\n# Compute log-likelihood for Model 0\nlogLik(lm.0)\n\n'log Lik.' -58.16733 (df=2)\n\n# Compute likelihood for Model 2\nexp(logLik(lm.0)[1])\n\n[1] 5.473293e-26\n\n# Compute difference in log-likelihoods\nlogLik(lm.1)[1] - logLik(lm.0)[1]\n\n[1] 12.71816\n\n# Compute LR\nexp( logLik(lm.1)[1] - logLik(lm.0)[1] )\n\n[1] 333754.4\n\n\nBecause the output from logLik() includes extraneous information (e.g., df), we use indexing (square brackets) to extract only the part of the output we want. In this case, the [1] extracts the log-likelihood value from the logLik() output (ignoring the df part).\nAlso of note is that the df for Model 0 is two, indicating that this model is estimating two parameters (\\(\\beta_0\\), and \\(\\sigma^2_{\\epsilon}\\)). The value of df in the logLik() output is a quantification of the model’s complexity. Here Model 1 2 (df = 6) is more complex than Model 0 (df = 2).\nAs we consider using the likelihood ratio (LR) or the difference in log-likelihoods for model selection, we also need to consider the model complexity. In our example, the likelihood ratio of 333754.4 (computed using logLik()) indicates that Model 1 has approximately 333,754 times the empirical support than Model 0. But, Model 1 is more complex than Model 0, so we would expect that it would be more empirically supported.\nIn this case, with a LR of 333,754, it seems like the data certainly support adopting Model 1 over Model 0, despite the added complexity of Model 1. But what if the LR was 10? Would that be enough additional support to warrant adopting Model 1 over Model 0? What about a LR of 5?"
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#hypothesis-test-of-the-lrt",
    "href": "notes/04-likelihood-evidence.html#hypothesis-test-of-the-lrt",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Hypothesis Test of the LRT",
    "text": "Hypothesis Test of the LRT\nWhen we have nested models we can carry out a hypothesis test to decide between the following competing hypotheses:\n\\[\n\\begin{split}\nH_0:& ~\\theta_0 = \\{\\beta_0,~\\sigma^2_{\\epsilon}\\}\\\\[1ex]\nH_A:& \\theta_1 = \\{\\beta_0,~\\beta_{\\mathrm{Enrollment}},~\\beta_{\\mathrm{FT~Students}},~\\beta_{\\mathrm{FT~Faculty}},~\\beta_{\\mathrm{Tuition}},~\\sigma^2_{\\epsilon}\\}\n\\end{split}\n\\]\nwhere \\(\\theta_0\\) refers to the simpler model and \\(\\theta_1\\) refers to the more complex model. This translates to adopting either the simpler model (fail to reject \\(H_0\\)) or the more complex model (reject \\(H_0\\)). To carry out this test, we translate our likelihood ratio to a test statistic called \\(\\chi^2\\) (pronounced chi-squared):\n\\[\n\\chi^2 = -2 \\ln \\bigg(\\frac{\\mathcal{L}({\\theta_0})}{\\mathcal{L}({\\theta_1})}\\bigg)\n\\]\nThat is we compute \\(-2\\) times the log of the likelihood ratio where the likelihood for the simpler model is in the numerator. (Note this is the inverse of how we have been computing the likelihood ratio!) Equivalently, we can compute this as:\n\\[\n\\chi^2 = -2 \\bigg(\\ln \\bigg[\\mathcal{L}({\\theta_0})\\bigg] - \\ln \\bigg[\\mathcal{L}({\\theta_1})\\bigg]\\bigg)\n\\]\nFor our example, we compute this using the following syntax:\n\n# Compute chi-squared\n-2 * (logLik(lm.0)[1] - logLik(lm.1)[1])\n\n[1] 25.43632"
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#mathematics-of-deviance",
    "href": "notes/04-likelihood-evidence.html#mathematics-of-deviance",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Mathematics of Deviance",
    "text": "Mathematics of Deviance\nWe can express the deviance mathematically by multiplying the log-likelihood by \\(-2\\).\n\\[\n\\begin{split}\n\\mathrm{Deviance} &= -2 \\times\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) \\\\[1ex]\n&= -2 \\bigg(-\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\\bigg) \\\\[1ex]\n&= -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{\\sigma^2_{\\epsilon}}\\sum \\epsilon_i^2 \\\\[1ex]\n&= -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{\\mathrm{RSS}}{\\sigma^2_{\\epsilon}}\n\\end{split}\n\\]\nRewriting this using the parameters from the likelihood:\n\\[\n\\mathrm{Deviance} = -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{\\sum_{i=1}^n \\big[Y_i-\\beta_0-\\beta_1(X_i)\\big]^2}{\\sigma^2_{\\epsilon}}\n\\]\nOnce again, we find that the only variables in the deviance function are the regression coefficients."
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#modeling-the-variation-in-the-test-statistic",
    "href": "notes/04-likelihood-evidence.html#modeling-the-variation-in-the-test-statistic",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Modeling the Variation in the Test Statistic",
    "text": "Modeling the Variation in the Test Statistic\nIf the null hypothesis is true, the difference in deviances can be modeled using a \\(\\chi^2\\)-distribution. The degrees-of-freedom for this \\(\\chi^2\\)-distribution is based on the difference in the number of parameters between the complex and simpler model. In our case this difference is four (\\(6-2=4\\)):\n\\[\n\\chi^2(4) = 25.44\n\\]\n\n\nCode\n# Create dataset\nfig_01 = data.frame(\n  X = seq(from = 0, to = 30, by = 0.01)\n  ) %&gt;%\n  mutate(\n    Y = dchisq(x = X, df = 4)\n    )\n\n# Filter out X&lt;=65\nshaded = fig_01 %&gt;%\n  filter(X &gt;=25.44)\n\n# Create plot\nggplot(data = fig_01, aes(x = X, y = Y)) +\n  geom_line() +\n  xlab(\"Chi-squared\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\n\n\n\nFigure 3: Plot of the probability density function (PDF) for a \\(\\chi^2(4)\\) distribution. The grey shaded area represents the p-value based on \\(\\chi^2=25.44\\).\n\n\n\n\n\nTo compute the p-value we use the pchisq() function.\n\n# Compute p-value for X^2 = 25.44\n1 - pchisq(q = 25.44, df = 4)\n\n[1] 0.00004103253\n\n# Alternative method\npchisq(q = 25.44, df = 4, lower.tail = FALSE)\n\n[1] 0.00004103253\n\n\nBased on the p-value, we would reject the null hypothesis for the likelihood ratio test, which suggests that we should adopt the more complex model (Model 1). This means that the model that includes the set of predictors measuring the institutional characteristics of size and wealth is more empirically supported than a model that includes no predictors. Note that we are making a holistic evaluation about the model rather than about individual predictors. Based on the LR test we conducted we cannot say which predictors are statistically relevant, only that the set of predictors included in Model 1 is more supported than no predictors.\nTo determine how much more variation Model 1 explains in peer ratingfs than Model 0 does, we look at the \\(R^2\\) values from the glance() output.\n\n# Model-level output Model 0\nglance(lm.0)\n\n\n  \n\n\n# Model-level output Model 1\nglance(lm.1)\n\n\n  \n\n\n\nModel 1 explains 26.4% of the variation in peer ratings compared to Model 0, which explains 0% of the variation in peer ratings. This difference is statistically significant based on the p-value obtained from the likelihood ratio test (\\(p=0.000041\\))."
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#footnotes",
    "href": "notes/04-likelihood-evidence.html#footnotes",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSometimes this is also referred to a the sum of squared errors (SSE).↩︎\nThis is in some sense mixing the paradigms of likelihood-based evidence and classical hypothesis test-based evidence. In a future set of notes we will learn about information criteria which eliminate the need to mix these two paradigms.↩︎\nThe use of the term “residual deviance” is not universal. Some textbooks omit the “residual” part and just refer to it as the “deviance”. Others use the term “model deviance”.↩︎"
  },
  {
    "objectID": "notes/05-polynomial-effects.html",
    "href": "notes/05-polynomial-effects.html",
    "title": "Polynomial Effects",
    "section": "",
    "text": "In this set of notes, you will learn one method of dealing with nonlinearity. Specifically, we will look at the inclusion of polynomial effects into a model. To do so, we will use the mn-schools.csv dataset (see the data codebook) to examine if (and how) academic “quality” of the student-body (measured by SAT score) is related to institutional graduation rate.\n\n# Load libraries\nlibrary(broom)\nlibrary(educate) \nlibrary(lmtest)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Read in data\nmn = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/mn-schools.csv\")\n\n# View data\nhead(mn)"
  },
  {
    "objectID": "notes/05-polynomial-effects.html#residual-plot-another-way-to-spot-nonlinearity",
    "href": "notes/05-polynomial-effects.html#residual-plot-another-way-to-spot-nonlinearity",
    "title": "Polynomial Effects",
    "section": "Residual Plot: Another Way to Spot Nonlinearity",
    "text": "Residual Plot: Another Way to Spot Nonlinearity\nSometimes, the nonlinear relationship is difficult to detect from the scatterplot of Y versus X. Often it helps to fit the linear model and then examine the assumption of linearity in the residuals. It is sometimes easier to detect nonlinearity in the scatterplot of the residuals versus the fitted values.\n\n# Fit linear model\nlm.1 = lm(grad ~ 1 + sat, data = mn)\n\n# Residual plot: Scatterplot\nresidual_plots(lm.1, type = \"s\")\n\n\n\n\n\n\n\nFigure 2: Standardized residuals versus the fitted values for a model regressing six-year graduation rates on median SAT scores. The line \\(Y=0\\) (black) and the loess smoother (blue) are also displayed.\n\n\n\n\n\nThis plot suggests that the assumption of linearity may be violated; the average residual is not zero at each fitted value. For low fitted values it appears as though the average residual may be less than zero, for moderate fitted values it appears as though the average residual may be more than zero, and for high fitted values it appears as though the average residual may be less than zero.\nNotice that the pattern displayed in the residuals is consistent with the pattern of the observed data in the initial scatterplot (Figure 1). If we look at the data relative to the regression smoother we see that there is not even vertical scatter around this line. At low and high SAT scores the observed data tends to be below the regression line (the regression is over-estimating the average graduation rate), while for moderate SAT scores the observed data tends to be above the regression line (the regression is under-estimating the average graduation rate)."
  },
  {
    "objectID": "notes/05-polynomial-effects.html#are-the-assumptions-more-tenable-for-this-model",
    "href": "notes/05-polynomial-effects.html#are-the-assumptions-more-tenable-for-this-model",
    "title": "Polynomial Effects",
    "section": "Are the Assumptions More Tenable for this Model?",
    "text": "Are the Assumptions More Tenable for this Model?\nMore important than whether the p-value is small, is whether including the quadratic effect improved the assumption violation we noted earlier. To evaluate this, we will examine the two residual plots for the quadratic model: (1) a density plot of the standardized residuals; and (2) a plot of the standardized residuals versus the fitted values.\n\n# Residual plots\nresidual_plots(lm.2)\n\n\n\n\n\n\n\nFigure 3: Residual plots for the quadratic model regressing six-year graduation rate on median SAT scores. LEFT: Density plot of the standardized residuals. The confidence envelope for a normal reference distribution (blue shaded area) is also displayed. RIGHT: Standardized residuals versus the fitted values. The line \\(Y=0\\) (black) and confidence envelope (grey shaded area) for that line are shown, along with the loess smoother (blue) esimating the mean pattern of the residuals.\n\n\n\n\n\nThe plot of the standardized residuals versus the fitted values suggests that the residuals for the quadratic model are far better behaved; indicating much more consistency with the assumption that the average residual is zero at each fitted value than the linear model. This is the evidence that we would use to justify retaining the quadratic effect in the model. This plot also suggests that the homoskedasticity assumption is tenable. Finally, the empirical density of the residuals also seems consistent with the assumption of normality for this model.\nReminder: The assumption of independence can’t be evaluated from the plots, but is vary important given we rely on it to be able to correctly compute the likelihood. From the design of the study, we haven’t sampled the schools randomly, nor randomly assigned the schools to their levels of the predictor(s), so we can’t infer independence from that. So the question becomes, does knowing the graduation rate of a school in the population give us information about the graduation rate for other schools in the population with the same median SAT score? If the answer is “no”, then we can call the independence assumption tenable. If the answer is “yes”, the independence assumption is violated and we should not use the lm() function or believe that results from that function are valid."
  },
  {
    "objectID": "notes/05-polynomial-effects.html#graphical-interpretation",
    "href": "notes/05-polynomial-effects.html#graphical-interpretation",
    "title": "Polynomial Effects",
    "section": "Graphical Interpretation",
    "text": "Graphical Interpretation\nTo plot the fitted equation, we use the geom_function() layer to add the fitted curve. (Note that we can no longer use geom_abline() since the addition of the polynomial effect implies that a line is no longer suitable.) This layer takes the argument fun= which describes a function that will be plotted as a line. To describe a function, use the syntax function(){...}. Here we use this syntax to describe the function of x (which in the ggplot() global layer is mapped to the sat variable). The fitted equation is then also written in terms of x and placed inside the curly braces. (Note that it is best to be more exact in the coefficient values—don’t round—when you create this plot as even minor differences can grossly change the plot.)\n\n# Scatterplot\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0.3) +\n  geom_function(\n    fun = function(x) {-366.34 + 62.72*x - 2.15 * x^2},\n    color = \"#0072b2\"\n    ) +\n    theme_light() +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\")\n\n\n\n\n\n\n\nFigure 4: Scatterplot of six-year graduation rate versus median SAT score. The fitted curve from the quadratic regression model (blue line) is also displayed.\n\n\n\n\n\nThe fitted curve helps us interpret the nature of the relationship between median SAT scores and graduation rates. The effect of median SAT score on graduation rate depends on SAT score (definition of an interaction). For schools with low SAT scores, the effect of SAT score on graduation rate is positive and fairly high. For schools with high SAT scores, the effect of SAT score on graduation rate remains positive, but it has a smaller effect on graduation rates; the effect diminishes."
  },
  {
    "objectID": "notes/05-polynomial-effects.html#algebraic-interpretation",
    "href": "notes/05-polynomial-effects.html#algebraic-interpretation",
    "title": "Polynomial Effects",
    "section": "Algebraic Interpretation",
    "text": "Algebraic Interpretation\nFrom algebra, you may remember that the coefficient in front of the quadratic term (\\(-2.2\\)) informs us of whether the quadratic is an upward-facing U-shape, or a downward-facing U-shape. Since our term is negative, the U-shape is downward-facing. This is consistent with what we saw in the plot. What the algebra fails to show is that, within the range of SAT scores in our data, we only see part of the entire downward U-shape.\nThis coefficient also indicates whether the U-shape is skinny or wide. Although “skinny” and “wide” are only useful as relative comparisons. Algebraically, the comparison is typically to a quadratic coefficient of 1, which is generally not useful in our interpretation. The intercept and coefficient for the linear term help us locate the U-shape in the coordinate plane (moving it right, left, up, or down from the origin). You could work all of this out algebraically.\n\nYou can see how different values of these coefficients affect the curve on Wikipedia. Here is an interactive graph that lets you explore how changing the different coefficients changes the parabola.\n\nWhat can be useful is to find where the minimum (upward facing U-shape) or maximum (downward facing U-shape) occurs. This point is referred to as the vertex of the parabola and can be algebraically determined. To do this we determine the x-location of the vertex by\n\\[\nx_\\mathrm{Vertex} = -\\frac{\\hat{\\beta}_1}{2 \\times \\hat\\beta_2}\n\\]\nwhere, \\(\\hat{\\beta}_1\\) is the estimated coefficient for the linear term and \\(\\hat{\\beta}_2\\) is the estimated coefficient for the quadratic term. The y coordinate for the vertex can then be found by substituting the x-coordinate into the fitted equation. For our example,\n\\[\nx_\\mathrm{Vertex} = -\\frac{62.72}{2 \\times -2.15} = 14.58\n\\]\nand\n\\[\ny_\\mathrm{Vertex} = -366.34 + 62.72(14.58) - 2.15(14.58^2) = 91.08\n\\]\nThis suggests that at a median SAT score of 1458 we predict a six-year graduate rate of 91.08. This x-value also represents the value at which the direction of the effect changes. In our example recall that for higher values of SAT the effect of SAT on graduation rate was diminishing. This is true for schools with median SAT scores up to 1458. For schools with higher SAT scores the effect of SAT score on graduation rate would theoretically be negative, and would be more negative for higher values.\nThis is all theoretical as our data only includes median SAT scores up to 1400. Everything past that value (including the vertex) is extrapolation. Extrapolation is exceedingly sketchy when we start fitting non-linear models. For example, do we really think that the average graduation rate for schools with a median SAT scores higher than 1458 would actually be smaller than for schools at 1458? It is more likely that the effect just flattens out."
  },
  {
    "objectID": "notes/05-polynomial-effects.html#summarizing-the-adopted-interaction-model",
    "href": "notes/05-polynomial-effects.html#summarizing-the-adopted-interaction-model",
    "title": "Polynomial Effects",
    "section": "Summarizing the Adopted Interaction Model",
    "text": "Summarizing the Adopted Interaction Model\nAgain, we will obtain model- and coefficient-level output and use those to summarize the results of the fitted model.\n\n# Model-level output\nglance(lm.4)\n\n\n  \n\n\n\nModel 4 explains 90.6% of the variation in graduation rates, an increase of 0.9 percentage points from Model 3. The residual standard error for Model 4, \\(\\hat\\sigma_{\\epsilon}=5.64\\), has also decreased from that for Model 3 indicating that this model has less error than Model 3.\nThe coefficient-level output is:\n\n# Coefficients\ncoef(lm.4)\n\n(Intercept)         sat    I(sat^2)      public  sat:public \n-413.798015   71.654268   -2.544468   35.071344   -4.112108 \n\n\nWe can use this to obtain the fitted equation:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} = -&413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i) + \\\\\n&35.07(\\mathrm{Public}_i) - 4.11(\\mathrm{SAT}_i)(\\mathrm{Public}_i)\n\\end{split}\n\\]\nThe interaction tells us that the effect of SAT on graduation rate differs for public and private institutions. Rather than parsing this difference by trying to interpret each of the effects, we will again plot the fitted curves for private and public institutions, and use the plot to aid our interpretation.\nPrivate Institutions\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i) + 35.07(0) - 4.11(\\mathrm{SAT}_i)(0) \\\\\n&= -413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i)\n\\end{split}\n\\]\nPublic Institutions\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i) + 35.07(1) - 4.11(\\mathrm{SAT}_i)(1) \\\\\n&= -378.73 + 67.54(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i)\n\\end{split}\n\\]\nComparing these two fitted equations, we see that not only is the intercept in the two fitted models different (due to the main effect of sector), but now the coefficient for the interaction between sector and the linear effect of SAT also differs; it is lower for public institutions (by 4.11 units) than for private institutions. The quadratic effect of SAT is the same for both public and private institutions. (If we had adopted the model with both interaction terms, the quadratic effect would also be different across sectors.)\nTo visualize the effect of SAT, we can again plot each fitted curve by including each in a separate geom_function() layer.\n\n# Plot of the fitted model\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0) +\n    geom_function(\n      fun = function(x) {-378.73 + 67.54 * x - 2.54 * x^2},\n      color = \"#2ec4b6\",\n      linetype = \"dashed\"\n      ) +\n  geom_function(\n    fun = function(x) {-413.80 + 71.65 * x - 2.54 * x^2},\n    color = \"#ff9f1c\",\n    linetype = \"solid\"\n    ) +\n    theme_light() +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\")\n\n\n\n\n\n\n\nFigure 6: Six-year graduation rate as a function of median SAT score for private (orange, solid line) and public (blue, dashed line) institutions.\n\n\n\n\n\nThe curvature (linear and quadratic slopes) of the lines now looks different for the two sectors because of the interaction term. (Note that interactions with the linear effect of SAT or the quadratic effect of SAT contribute to a change in the curvature.) For both sectors, the effect of median SAT on graduation rates is positive (institutions with higher median SAT scores tend to have higher graduation rates. But, this effect diminishes for institutions with increasingly higher SAT scores. Private schools have higher graduation rates, on average, than public schools for all levels of median SAT score. Moreover, this difference in graduation rates is getting larger at higher levels of SAT (that is the interaction!)."
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html",
    "href": "notes/07-logarithmic-transformations-outcome.html",
    "title": "Log-Transforming the Outcome",
    "section": "",
    "text": "In this set of notes, you will learn another method for dealing with nonlinearity. Specifically, we will look at log-transforming the outcome in a linear model. This transformation can also help rectify violations of the normality and homogeneity of variance assumptions. To do so, we will use the carbon.csv dataset:\n\nCSV File\nData Codebook\n\nOur analytic goal will be to explain variation in worldwide carbon emissions. Specifically:\n\nWhat is the effect of a country’s wealth on carbon emissions?\nDoes this effect persist after controlling for urbanization?\nDoes this relationship vary between world regions?\n\nWithin this work, we will use information theoretic approcaches (namely the AICc and related measures) to evaluate any fitted models.\n\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom)\nlibrary(educate) \nlibrary(gt)\nlibrary(patchwork)\nlibrary(texreg)\nlibrary(tidyverse)\n\n# Read in data\ncarbon = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/carbon.csv\")\n\n# View data\ncarbon"
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#quick-refresher-on-exponents-and-logarithms",
    "href": "notes/07-logarithmic-transformations-outcome.html#quick-refresher-on-exponents-and-logarithms",
    "title": "Log-Transforming the Outcome",
    "section": "Quick Refresher on Exponents and Logarithms",
    "text": "Quick Refresher on Exponents and Logarithms\nThe logarithm is an inverse function of an exponent. Consider this example,\n\\[\n\\log_2 (32)\n\\]\nThe base-2 logarithm of 32 is the exponent to which the base, 2 in our example, must be raised to produce 32. Mathematically it is equivalent to solving the following equation for x:\n\\[\n\\begin{split}\n2^{x} &= 32 \\\\\nx &= 5\n\\end{split}\n\\]\nThus,\n\\[\n\\log_2 (32) = 5\n\\]\nTo compute a logarithm using R, we use the log() function. We also specify the argument base=, since logarithms are unique to a particular base. For example, to compute the mathematical expression \\(\\log_2 (32)\\), we use\n\nlog(32, base = 2)\n\n[1] 5\n\n\nThere is also a shortcut function to use base-2.\n\nlog2(32)\n\n[1] 5"
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#log-transforming-variables",
    "href": "notes/07-logarithmic-transformations-outcome.html#log-transforming-variables",
    "title": "Log-Transforming the Outcome",
    "section": "Log-Transforming Variables",
    "text": "Log-Transforming Variables\nFor our purposes, we need to log-transform each value of the Y attribute, namely the CO2 values in our data. Here, we will log-transform the CO2 values (using base-2). To do this we create a new column called l2co2 using the mutate() function.\n\n# Create base-2 log-transformed CO2 values\ncarbon = carbon |&gt;\n  mutate(\n    l2co2 = log(co2, base = 2)\n    )\n\n# View data\ncarbon\n\n\n  \n\n\n\nHow does the distribution of the log-transformed CO2 values compare to the distribution of raw CO2 values? We can examine the density plot of both the original and log-transformed variables to answer this.\n\n\nCode\np1 = ggplot(data = carbon, aes(x = co2)) +\n  stat_density(geom = \"line\") +\n  theme_light() +\n  xlab(\"Carbon emissions (metric tons per person)\") +\n  ylab(\"Probability density\") +\n  ggtitle(\"Raw values of predictor\")\n\np2 = ggplot(data = carbon, aes(x = l2co2)) +\n  stat_density(geom = \"line\") +\n  theme_light() +\n  xlab(\"Log2 (Carbon emissions)\") +\n  ylab(\"Probability density\") +\n  ggtitle(\"Log-2 transformed carbon emissions\")\n\n# Layout the plots\np1 | p2\n\n\n\n\n\n\n\n\nFigure 4: Density plot of the raw CO2 values (LEFT) and the base-2 log-transformed CO2 (RIGHT).\n\n\n\n\n\nComparing the shapes of the two distributions, we see that:\n\nThe original CO2 variable was right-skewed. The log-transformed variable is more symmetric. (It is perhaps even a bit left-skewed, although certainly closer to symmetric than the original.)\nThe scale is quite different between the two variables (one is, after all, log-transformed). This has greatly affected the center (mean) and the variation. Importantly, after log-transforming, the variation in the distribution is much smaller.\n\n\nLogarithmic transformations change the shape, center, and variation of a distribution! For modelers, they take (1) make a right-skewed variable more symmetric and (2) reduce the variation in the distribution."
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#relationship-between-log-transformed-co2-emissions-and-wealth",
    "href": "notes/07-logarithmic-transformations-outcome.html#relationship-between-log-transformed-co2-emissions-and-wealth",
    "title": "Log-Transforming the Outcome",
    "section": "Relationship between Log-Transformed CO2 Emissions and Wealth",
    "text": "Relationship between Log-Transformed CO2 Emissions and Wealth\nLet’s examine the relationship between the log-transformed CO2 emissions and wealth.\n\n# Scatterplot\nggplot(data = carbon, aes(x = wealth, y = l2co2)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Log2(Carbon emissions)\") +\n  annotate(geom = \"text\", x = 6.4, y = 5.25, label = \"Quatar\", size = 3, hjust = 1) +\n  annotate(geom = \"text\", x = 4.6, y = 4.97, label = \"Trinidad and Tobago\", size = 3, hjust = 1) \n\n\n\n\n\n\n\nFigure 5: Scatterplot between wealth and log base-2 transformed CO2 emissions. The loess smoother (blue, dashed line) is also displayed. The two countries with extreme carbon emissions are also identified on this plot.\n\n\n\n\n\nLog-transforming the outcome has drastically affected the scale for the outcome. The relationship between wealth and the log-transformed CO2 emissions is much more linear (although not perfect). The two countries which had extremely high CO2 emissions when we examined raw CO2, no longer seems like outliers in the transformed data. The differences in variation of log-carbon emissions between lower wealth and higher wealth countries also seems less severe.\nHas this helped us better meet the distributional assumptions for the regression model? To find out, we will fit the model using the log-transformed carbon emissions and examine the residual plots.\n\n# Fit model\nlm.log2 = lm(l2co2 ~ 1 + wealth, data = carbon)\n\n# Obtain residual plots\nresidual_plots(lm.log2)\n\n\n\n\n\n\n\nFigure 6: Density plot of the standardized residuals (LEFT) and scatterplot of the standarized residuals versus the fitted values (RIGHT) from regressing log-transformed CO2 emisssions on wealth. The reference line of Y=0 is also displayed along with the 95% confidence envelope (grey shaded area). The loess smoother (solid, blue line) shows the empirical relationship betwen the residuals and fitted values.\n\n\n\n\n\nThese plots suggest that after the transforming the outcome, there is a great deal of improvement in meeting the assumption of normality. The assumption of homoskedasticity also looks markedly improved. There still seems to be violations of the linearity assumption.\n\nFor now, we will proceed so you understand how to interpret coefficients from models where the outcome has been log-transformed, but we will come back and fix this nonlinearity later."
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#better-interpretations-back-transforming-to-the-raw-metric",
    "href": "notes/07-logarithmic-transformations-outcome.html#better-interpretations-back-transforming-to-the-raw-metric",
    "title": "Log-Transforming the Outcome",
    "section": "Better Interpretations: Back-transforming to the Raw Metric",
    "text": "Better Interpretations: Back-transforming to the Raw Metric\nWhile these interpretations are technically correct, it is more helpful to your readers (and more conventional) to interpret any regression results in the raw metric of the variable rather than log-transformed metric. This means we have to back-transform the interpretations. To think about how to do this, we first consider a more general expression of the fitted linear model:\n\\[\n\\log_2\\left(\\hat{Y}_i\\right) = \\hat\\beta_0 + \\hat\\beta_1(X_{i})\n\\]\nThe left-hand side of the equation is in the log-transformed metric, which drives our interpretations. If we want to instead, interpret using the raw metric of Y, we need to back-transform from \\(\\log_2(Y)\\) to Y. To back-transform, we use the inverse function, which is to exponentiate using the base of the logarithm, in our case, base-2.\n\\[\n2^{\\log_2(Y_i)} = Y_i\n\\]\nIf we exponentiate the left-hand side of the equation, to maintain the equality, we also need to exponentiate the right-hand side of the equation.\n\\[\n2^{\\log_2(Y_i)} = 2^{\\hat\\beta_0 + \\hat\\beta_1(X_{i})}\n\\]\nThen we use rules of exponents to simplify this.\n\\[\nY_i = 2^{\\hat\\beta_0} \\times 2^{\\hat\\beta_1(X_{i})}\n\\]\nFor our example, we exponentiate both sides of the fitted equation to get the following back-transformed fitted equation:\n\\[\n\\widehat{\\mathrm{CO2}_i} = 2^{-3.17} \\times 2^{1.19(\\mathrm{Wealth}_i)}\n\\]\n\n\nSubstituting in Values for Wealth to Interpret Effects\nTo interpret the back-transformed effects, we can substitute in the different values for wealth and solve. For example when wealth = 0:\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= 2^{-3.17} \\times 2^{1.19(0)}\\\\\n&= 0.111 \\times 1 \\\\\n&= 0.111\n\\end{split}\n\\]\nIn R, we use the ^ operator to exponentiate. In our example, to compute the predicted caron dioxide emissions, we use the following:\n\n# Compute predicted CO2 when wealth=0\n2^(-3.17) * 2^(1.19 * 0)\n\n[1] 0.1111053\n\n\nThe predicted CO2 emissions for a countries that have a wealth level of 0 is 0.111 metric tons per person, on average (extrapolation). This is how we interpret the intercept! How about countries that have a wealth level of 1 (a one-unit difference in wealth from countries that have a wealth level of 0)?\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= 2^{-3.17} \\times 2^{1.19(1)}\\\\\n&= 0.111 \\times 2.28 \\\\\n&= 0.253\n\\end{split}\n\\]\nThe predicted CO2 emission for a countries that have a wealth level of 1 is 0.253 metric tons per person. This is 2.28 TIMES the emissions of countries that have a wealth level of 0. How about countries that have a wealth level of 2 (a one-unit difference in wealth from countries that have a wealth level of 1)?\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= 2^{-3.17} \\times 2^{1.19(2)}\\\\\n&= 0.111 \\times 5.21 \\\\\n&= 0.578\n\\end{split}\n\\]\nUsing another rule of exponents we could re-write this last fitted equation as:\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= 2^{-3.17} \\times (2^{1.19})^2\\\\\n&= 0.111 \\times 2.28^2 \\\\\n&= 0.111 \\times 2.28 \\times 2.28 \\\\\n&= 0.578\n\\end{split}\n\\]\nThis is 2.28 TIMES the emissions of countries that have a wealth level of 1. Each 1-unit increase in wealth is associated with an increase in CO2 emissions of a factor of 2.28 (i.e., 2.28 times higher CO2 emissions). The technical language we use to express “2.28 times” is a “2.28-fold difference”. So we would conventionally interpret this as:\n\nEach one-unit difference in wealth level is associated with a 2.28-fold difference in carbon emissions, on average.\n\nTo summarize, when we back-transform from interpretations of log-Y to Y the rate-of-change is multiplicative rather than additive. And to determine the multiplicative rate-of-change, we take the base we chose to the power of the estimated slope, that is:\n\\[\n\\text{Rate-of-Change} = \\text{base}^{\\hat\\beta_1}\n\\]\nA computational shortcut to obtain the back-transformed interpretations is to exponentiate the output of the coef() function (which returns the coefficients from the fitted model).\n\n# Obtain back-transformed interpretations\n2 ^ coef(lm.log2)\n\n(Intercept)      wealth \n  0.1111884   2.2788237"
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#interpretation-of-the-rate-of-change-as-percent-change",
    "href": "notes/07-logarithmic-transformations-outcome.html#interpretation-of-the-rate-of-change-as-percent-change",
    "title": "Log-Transforming the Outcome",
    "section": "Interpretation of the Rate-of-Change as Percent Change",
    "text": "Interpretation of the Rate-of-Change as Percent Change\nThe results of log-transformed models are often interpreted as percent change. Rather than saying that each one-unit difference in wealth is associated with a 2.28-fold difference in carbon emissions, on average. We can also interpret this change as a percent change. To do this, we compute:\n\\[\n\\mathrm{Percent~Change} = (2^{\\hat{\\beta_1}} -1) \\times 100\n\\]\nIn our example,\n\\[\n\\begin{split}\n\\mathrm{Percent~Change} &= (2^{1.19} -1) \\times 100 \\\\[1ex]\n&= 128.15\n\\end{split}\n\\]\nThus the interpretation is:\n\nEach one-unit difference in wealth is associated with a 128% change in carbon emissions, on average.\n\n\nBe very careful when you are working with and interpreting using percents. The phrases “percent change” and “percentage point change” are two very different things. For example, increasing a graduation rate from 50% to 60% represents an increase of 10 percentage points, but it is a 20% increase or a 120% change!"
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#plotting-the-fitted-model",
    "href": "notes/07-logarithmic-transformations-outcome.html#plotting-the-fitted-model",
    "title": "Log-Transforming the Outcome",
    "section": "Plotting the Fitted Model",
    "text": "Plotting the Fitted Model\nAs always, we should plot the fitted model to aid in interpretation. To do this we will use the back-transformed expression of the fitted equation:\n\\[\n\\widehat{\\mathrm{CO2}_i} = 2^{-3.17} \\times 2^{1.19(\\mathrm{Wealth}_i)}\n\\]\nThis can be plotted by using the geom_function() layer of ggplot().\n\n# Plot\nggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point(alpha = 0) +\n  geom_function(fun = function(x) {2^(-3.17) * 2^(1.19*x)} ) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Predicted CO2 emissions (in metric tones per person)\")\n\n\n\n\n\n\n\nFigure 7: Plot of predicted CO2 emisssions as a function of wealth.\n\n\n\n\n\nBased on this plot, we see a non-linear, positive effect of wealth on carbon emissions. Wealthier countries tend to have higher carbon emissions, on average, but the increase in carbon emissions as countries get wealthier is not constant; it is exponentially increasing."
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#fitting-the-model-using-base-e-logarithms",
    "href": "notes/07-logarithmic-transformations-outcome.html#fitting-the-model-using-base-e-logarithms",
    "title": "Log-Transforming the Outcome",
    "section": "Fitting the Model Using Base-e Logarithms",
    "text": "Fitting the Model Using Base-e Logarithms\nThe fitted equation will be represented as:\n\\[\n\\hat{Y}_i = e^{\\hat\\beta_0 + \\hat\\beta_1(X_i)}\n\\]\nHere we take the natural logarithm of both sides of the equation:\n\\[\n\\begin{split}\n\\ln(\\hat{Y}_i) &= \\ln\\bigg(e^{\\hat\\beta_0 + \\hat\\beta_1(X_i)}\\bigg) \\\\[2ex]\n&= \\hat\\beta_0 + \\hat\\beta_1(X_i)\n\\end{split}\n\\]\nThat is, we can regress the natural log-transformed values of Y onto X. To do this, we initially create a new column in the data that includes the natural logs of the carbon dioxide emission values. We can then use those as the outcome in our lm().\n\n# Create natural log-transformed CO2 values\ncarbon = carbon |&gt;\n  mutate(\n    lnco2 = log(co2)\n    )\n\n# View data\ncarbon\n\n\n  \n\n\n# Fit model\nlm.ln = lm(lnco2 ~ 1 + wealth, data = carbon)\n\n# Model-level output\nglance(lm.ln)\n\n\n  \n\n\n\nThe model-level output is identical to the model-level output from the base-2 and base-10 model, which we expected. That is differences in wealth explains 83.8% of the variation in CO2 emissions.\n\n# Coefficients\ncoef(lm.ln)\n\n(Intercept)      wealth \n -2.1965295   0.8236594 \n\n\nBecause we changed the base, the coefficient estimates for the fitted equation have changed. Writing the new fitted equation:\n\\[\n\\ln(\\widehat{\\mathrm{CO2}_i}) = -2.20 + 0.824(\\mathrm{Wealth}_i)\n\\]\nOr, back-transforming the left-hand side:\n\\[\n\\widehat{\\mathrm{CO2}_i} = e^{2.20} \\times e^{0.824(\\mathrm{Wealth}_i)}\n\\]\nTo interpret these, we exponentiate the coefficients using base-e.\n\n# Exponentiate using base-e\nexp(coef(lm.ln))\n\n(Intercept)      wealth \n  0.1111884   2.2788237 \n\n\nWe get the exact same back-transformed coefficients as we did when we back-transformed the base-2 coefficients. Again, this is expected. Based on these, the interpretations are:\n\nIntercept: The predicted CO2 emissions for a countries that have a wealth level of 0 is 0.111 metric tons per person, on average (extrapolation).\nRate-of-Change: Each one-unit difference in wealth level is associated with a 2.28-fold difference in carbon emissions, on average. OR using the language of percent change, each one-unit difference in wealth level is associated with a 128% increase in carbon emissions, on average.\n\nLastly, we can plot the fitted curve.\n\n# Plot\nggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point(alpha = 0) +\n  geom_function(fun = function(x) {exp(-2.20) * exp(0.824*x)} ) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Predicted CO2 emissions (in metric tones per person)\")\n\n\n\n\n\n\n\nFigure 9: Plot of predicted CO2 emisssions as a function of wealth.\n\n\n\n\n\nThis is the exact same fitted curve we obtained from the base-2 and base-10 models. Once again, this is completely expected."
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#what-is-special-about-base-e",
    "href": "notes/07-logarithmic-transformations-outcome.html#what-is-special-about-base-e",
    "title": "Log-Transforming the Outcome",
    "section": "What is Special About Base-e",
    "text": "What is Special About Base-e\nOther than being a cool number, why do Social Scientists like using base-e? It turns out that if the slope estimate is small (e.g., \\(\\hat\\beta_k &lt; 0.20\\)), we can directly take the slope value from the coef() output, multiply it by 100, and interpret it as the percent change. That is because for small values of the slope,\n\\[\n(e^{\\hat{\\beta_1}} -1) \\approx \\hat{\\beta_1}\n\\]\nIn our example, the slope is not less than 0.20, so we have to do the math, \\((e^{0.824}-1)\\times 100=128\\), to compute the percent change."
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#interpreting-the-effect-of-wealth",
    "href": "notes/07-logarithmic-transformations-outcome.html#interpreting-the-effect-of-wealth",
    "title": "Log-Transforming the Outcome",
    "section": "Interpreting the Effect of Wealth",
    "text": "Interpreting the Effect of Wealth\nSince the quadratic term is an interaction term, we interpret the effect of wealth generally as:\n\nInterpretation\nThe effect of wealth on carbon dioxide emissions depends on the level of wealth.\n\nTo better understand the nature of this relationship we plot the fitted curve. To do so, we first back-transform the metric of log-carbon emission to the metric of raw carbon emissions. Remember, this creates a multiplicative relationship among the exponentiated coefficients. Exponentiating both sides of the fitted equation:\n\\[\n\\widehat{\\mathrm{CO2}_i} = e^{-2.90} \\times e^{1.38(\\mathrm{Wealth}_i)} \\times e^{-0.0837(\\mathrm{Wealth}_i^2)}\n\\]\nPlotting this function will allow us to understand the relationship between wealth and carbon emissions from the polynomial model. Inputting this into the geom_function() layer of our ggplot() syntax, we get:\n\n# Plot\nggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point(alpha = 0) +\n  geom_function(fun = function(x) {exp(-2.90) * exp(1.38*x) * exp(-0.0837*x^2)} ) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Predicted CO2 (in metric tons per person)\")\n\n\n\n\n\n\n\nFigure 12: Plot of predicted carbon emisssions as a function of wealth for the quadratic polynomial model.\n\n\n\n\n\n\nAnswering the First Research Question\nThe relationship between wealth and carbon emisssions is complicated. It combines the positive exponential growth we saw earlier with the change in curvature of the negative quadratic effect. In general wealthier countries have exponentially increasing CO2 emissions that begins to diminish for countries with exceptionally high wealth levels."
  },
  {
    "objectID": "notes/07-logarithmic-transformations-outcome.html#footnotes",
    "href": "notes/07-logarithmic-transformations-outcome.html#footnotes",
    "title": "Log-Transforming the Outcome",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo long as we can ultimately write this function as a linear model, namely, \\(Y_i=\\beta_0+\\beta_1(x_i) + \\epsilon_i\\).↩︎\nMathematically, the instantaneous derivative of the exponential function is is always positive, and it approaches \\(+\\infty\\) for larger values of x, while that for the quadratic function changes in sign from negative to positive.↩︎\nFun Fact: e also happens to be Andy’s favorite number. Google founders might also enjoy e, as their IPO indicated they were going to raise e-billion dollars (see this Forbes article).↩︎"
  },
  {
    "objectID": "notes/09-rule-of-the-bulge.html",
    "href": "notes/09-rule-of-the-bulge.html",
    "title": "Rule of the Bulge—An Example",
    "section": "",
    "text": "In this set of notes you will learn about power transformations, how to use power transformations to re-express data so that the re-expressed data meet the assumption of “linearity” (i.e., straighten” curvilinear data), and see this in an empirical example."
  },
  {
    "objectID": "notes/09-rule-of-the-bulge.html#caution",
    "href": "notes/09-rule-of-the-bulge.html#caution",
    "title": "Rule of the Bulge—An Example",
    "section": "Caution ⚠️",
    "text": "Caution ⚠️\nSometimes these re-expressions will not be adequate. In some cases, you might not be able to “straighten” the data enough to meet the assumption. This is because these transformations “deteriorate” or “spuriously increase” the information contained in the data. As you use re-expressions further down-the-ladder, the variation in the re-expressed data decreases (Less variation = less information). Eventually, the variation in the re-expressed data will be so small that the values become indistinguishable (no information).\nIn the other direction, as you use re-expressions further up-the-ladder, the variation in the re-expressed data increases (more variation = more information), albeit spuriously. Essentially, we are adding information that is not truly in the data. This might lead us to finding results that aren’t really there, or over-emphasizing relationships.\n\nRe-expressions that only go a little way up- or down-the-ladder are fine. Just beware if you need to go too far up- or down-the-ladder to straighten your data. In those cases you may want to use a different method of estimating the model than OLS (e.g., non-linear least squares)."
  },
  {
    "objectID": "notes/09-rule-of-the-bulge.html#fit-linear-model",
    "href": "notes/09-rule-of-the-bulge.html#fit-linear-model",
    "title": "Rule of the Bulge—An Example",
    "section": "Fit Linear Model",
    "text": "Fit Linear Model\nFitting the linear model and looking at it’s output:\n\n# Fit model\nlm.1 = lm(log(body_weight) ~ 1 + log(brain_weight), data = mammal)\n\n# Model-level output\nglance(lm.1)\n\n\n  \n\n\n# Coefficient-level output\ntidy(lm.1)\n\n\n  \n\n\n\nInterpreting this output:\n\nDifferences in mammals’ brain weight explain 92.1% of the variation in body weight.\n\nThe fitted equation is:\n\\[\n\\hat{\\ln(\\mathrm{Body~Weight})}_i = -2.51 + 1.22\\bigg[\\ln(\\mathrm{Brain~Weight}_i)\\bigg]\n\\]\n\nMammals with a log(brain weight) of 0 have a predicted log(body weight) of \\(-2.51\\), on average.\nEach one-unit change in log(brain weight) is associated with a change in log(body weight) of 1.22-units, on average.\n\nWe can also back-transform these log entities to get a better interpretation of the coefficients. For the intercept, when log(brain weight) is 0, actual brain weight = 1. Thus, mammals with a 1-gram brain weight have a predicted log(body weight) of \\(-2.51\\), on average. Exponentiating this (\\(e^{-2.51}=0.081\\)), so we can interpret the intercept as:\n\nMammals with a brain weight of 1 gram have a predicted body weight of 0.081 kg, on average.\n\nTo consider the interpretation of the slope, we utilize the fact that log-transforming X (using the natural logarithm) results in an interpretation that can be interpreted as a 1% change in X. As such, we choose a series of brain weights that differ by 1% and plug them into our fitted equation to get predicted log(body weights):\n\n# Choose brain weights that differ by 1%\nbody = c(100, 101, 102.01)\n\n# Get predicted ln(body weight) values\n-2.51 + 1.22 * log(body)\n\n[1] 3.108308 3.120447 3.132586\n\n\nThe interpretation is:\n\nEach 1% difference in brain weight is associated with a difference of 0.012 in log(body weight), on average.\n\nHere 0.012 is the slope coefficient divided by 100. Now let’s transform the log(body weight) values to raw body weights. To do this, we exponentiate these predicted values:\n\n# Exponentiate the predicted values\nexp(-2.51 + 1.22 * log(body))\n\n[1] 22.38313 22.65651 22.93322\n\n\nThis results in a constant multiplicative difference of 1.0122, Namely,\n\nEach 1% difference in brain weight is associated with a 1.012-fold difference in body weight, on average.\n\nOr, interpreting this as a percent change:\n\nEach 1% difference in brain weight is associated with a 1.22% difference in body weight, on average.\n\nThis 1.22% change is essentially the slope coefficient from the fitted equation. Thus when we log-transform both X and Y using the natural logarithm, we can interpret both the change in X and change in Y as a percent change. In general:\n\nEach 1% difference in X is associated with a \\(\\hat\\beta_1\\)% difference in Y, on average.\n\nWe can also plot the fitted curve to facilitate a graphical interpretation.\n\n# Plot fitted curve\nggplot(data = mammal, aes(x = brain_weight, y = body_weight)) +\n  geom_point(alpha = 0.2) +\n  geom_function(fun = function(x){exp(-2.509 + 1.225*log(x))}) +\n  theme_light() +\n  xlab(\"Brain weight (in g)\") +\n  ylab(\"Body weight (in kg)\")\n\n\n\n\n\n\n\nFigure 8: Fitted regression curve showing the relationship between body weight and brain weight for 62 mammals.\n\n\n\n\n\n\nNote that the relationship between brain weight and body weight is referred to as a proportional growth model. (If the relationship was negative we would call it a proportional decay model.) To “linearize” a proportional growth model, we fit a log–log model."
  },
  {
    "objectID": "notes/10-optional-linear-probability-model.html",
    "href": "notes/10-optional-linear-probability-model.html",
    "title": "Linear Probability Model",
    "section": "",
    "text": "In this set of notes, you will learn how about linear probability models, and why they are not typically used to model dichotomous categorical outcome variables (e.g., dummy coded outcome). We will use data from the file graduation.csv.\n\n[CSV]\n[Data Codebook]\n\nIn particular, we will explore predictors of college graduation.\n\n# Load libraries\nlibrary(broom)\nlibrary(corrr)\nlibrary(educate)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Read in data\ngrad = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/graduation.csv\")\n\n# View data\ngrad\n\n\n  \n\n\n\nNote that in these analyses the outcome variable (degree) is a categorical variable indicating whether or not a student graduated."
  },
  {
    "objectID": "notes/10-optional-linear-probability-model.html#relationship-between-act-score-and-proportion-of-students-who-obtain-a-degree",
    "href": "notes/10-optional-linear-probability-model.html#relationship-between-act-score-and-proportion-of-students-who-obtain-a-degree",
    "title": "Linear Probability Model",
    "section": "Relationship between ACT Score and Proportion of Students who Obtain a Degree",
    "text": "Relationship between ACT Score and Proportion of Students who Obtain a Degree\nRecall that the regression model predicts the average Y at each X. Since our outcome is a dichotomous dummy-coded variable, the average represents the proportion of students with a 1. In other words, the regression will predict the proportion of students who obtained a degree for a particular ACT score. To get a sense for this, we can compute the sample proportion of students who obtained a degree for each ACT score from the data.\n\n# Compute \nprop_grad = grad |&gt;\n  group_by(act, degree) |&gt;\n  summarize(\n    N = n()  # Compute sample sizes by degree for each ACT score\n    ) |&gt;\n  mutate(\n    Prop = N / sum (N) #Compute proportion by degree for each ACT score\n    ) |&gt;\n  filter(degree == \"Yes\") |&gt; # Only use the \"Yes\" responses\n  ungroup() #Makes the resulting tibble regular\n\n# View data\nprop_grad |&gt;\n  print(n = Inf) #Print all the rows\n\n# A tibble: 24 × 4\n     act degree     N  Prop\n   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n 1    11 Yes        1 0.25 \n 2    13 Yes        4 0.5  \n 3    14 Yes        6 0.5  \n 4    15 Yes       13 0.448\n 5    16 Yes        6 0.24 \n 6    17 Yes       18 0.429\n 7    18 Yes       26 0.531\n 8    19 Yes       50 0.685\n 9    20 Yes       74 0.679\n10    21 Yes       97 0.678\n11    22 Yes      103 0.632\n12    23 Yes      149 0.764\n13    24 Yes      168 0.789\n14    25 Yes      163 0.700\n15    26 Yes      190 0.802\n16    27 Yes      147 0.778\n17    28 Yes      133 0.787\n18    29 Yes      132 0.830\n19    30 Yes       92 0.793\n20    31 Yes       61 0.813\n21    32 Yes       41 0.82 \n22    33 Yes       24 0.828\n23    34 Yes       16 1    \n24    35 Yes        3 0.6  \n\n\nIn general, the proportion of students who obtain their degree is higher for higher SAT values. We can also see this same relationship by plotting the proportion of students who graduate versus ACT score.\n\n# Scatterplot\nggplot(data = prop_grad, aes(x = act, y = Prop)) +\n  geom_point(aes(size = N)) +\n  geom_smooth(data = grad, aes(y = got_degree), method = \"loess\", se = FALSE, \n              color = \"red\", linetype = \"dashed\") +\n  geom_smooth(data = grad, aes(y = got_degree), method = \"lm\", se = FALSE) +\n  theme_bw() +\n  xlab(\"ACT score\") +\n  ylab(\"Proportion of students who obtained a degree\")\n\n\n\n\n\n\n\nFigure 4: Proportion of students who graduate conditional on ACT score. Size of the dot is proportional to sample size. The regression smoother (solid, blue line) and loess smoother (dashed, red line) are based on the raw data.\n\n\n\n\n\nThere are a few things to note here:\n\nThe regression line is based on the raw data rather than the proportion data. This is important because the sample sizes differ (e.g., there are far more students who have an ACT score near 25 than near 10).\nBoth the regression line and loess smoother indicate there is a positive relationship between ACT score and the proportion of students who graduate. This suggests that higher ACT scores are associated with higher proportions of students who graduate.\nThe loess smoother suggests there may be a curvilinear relationship between ACT score and the proportion of students who obtain their degree.\n\n\nBecause the regression is about the proportion of students who obtain their degree, the scatterplot we show should also indicate the proportion of students who graduate. Thus, when the outcome is categorical, we tend not to look at a scatterplot of the raw data in practice.\n\n# Scatterplot\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  xlab(\"ACT score\") +\n  ylab(\"Graduated\")\n\n\n\n\n\n\n\nFigure 5: Scatterplot of whether a student graduated versus ACT score. The regression smoother (blue, solid line) and loess smoother (red, dashed line) shows the positive relationship between ACT score and the proportion of students who obtain their degree.\n\n\n\n\n\nSince the raw data can only have outcome values of 0 or 1, we don’t see many of the observations (this is called overplotting). More importantly, the regression line and loess smoother, which indicates the proportion of students who graduate is using a different scale (values between 0 and 1) than the raw data (only values of 0 or 1).Because of this scale incompatibility, this plot is generally not provided in practice.\n\nWe can also see the positive relationship between ACT score and the proportion of students who obtain their degree by examining the correlation matrix between the two variables. The correlation coefficient suggests a weak, positive relationship, but we interpret this with caution since the relationship may not be linear (as suggested by the loess smoother).\n\n# Correlation\ngrad |&gt; \n  select(got_degree, act) |&gt; \n  correlate()"
  },
  {
    "objectID": "notes/10-optional-linear-probability-model.html#understanding-the-residuals-from-the-linear-probability-model",
    "href": "notes/10-optional-linear-probability-model.html#understanding-the-residuals-from-the-linear-probability-model",
    "title": "Linear Probability Model",
    "section": "Understanding the Residuals from the Linear Probability Model",
    "text": "Understanding the Residuals from the Linear Probability Model\nLook closely at the scatterplot of the residuals versus the fitted values. At each fitted value, there are only two residual values. Why is this? Recall that residuals are computed as \\(\\epsilon_i=Y_i - \\hat{Y}_i\\). Now, remember that \\(Y_i\\) can only be one of two values, 0 or 1. Also remember that in the linear probability model \\(\\hat{Y}_i=\\hat{\\pi}_i\\). Thus, for \\(Y=0\\),\n\\[\n\\begin{split}\n\\epsilon_i &= 0 - \\hat{Y}_i \\\\\n&= - \\hat{\\pi}_i\n\\end{split}\n\\]\nAnd, if \\(Y=1\\),\n\\[\n\\begin{split}\n\\epsilon_i &= 1 - \\hat{Y}_i \\\\\n&= 1 - \\hat{\\pi}_i\n\\end{split}\n\\]\nThis means that the residual computed using a particular fitted value can only take on one of two values: \\(- \\hat{\\pi}_i\\) or \\(1 - \\hat{\\pi}_i\\). Likewise, the standardized residuals can only take on two values for each fitted value.That is why in the plot of the standardized residuals versus the fitted values we do not see any scatter; there are only two possibilities for the standardized residuals at each fitted value.\nFurthermore, the residual (and therefore the standardized residuals) are always the same distance apart for each fitted value, and get smaller for higher fitted values. This is why we see the two parallel strips (having negative slopes) in this plot.\nIf we plot a distribution of these residuals, we will get two humps; one centered based on the distribution of \\(1 - \\hat{\\pi}_i\\) residuals and one centered based on the \\(-\\hat{\\pi}_i\\) residuals. This is why we see the bimodal distribution when we look at the density plot of the standardized residuals.\n\nFitting a linear model when the outcome is dichotomous will result in gross violations of the distributional assumptions. In the next set of notes we will examine more appropriate models for modeling variation in dichotomous categorical outcomes."
  },
  {
    "objectID": "notes/10-optional-linear-probability-model.html#footnotes",
    "href": "notes/10-optional-linear-probability-model.html#footnotes",
    "title": "Linear Probability Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe could also have used the original grad data set in our data= argument of ggplot(). In that case we would not use stat=\"Identity\") in the geom_bar() layer since the counts would be computed for us from the raw data.↩︎\nNote that \\n creates a newline to put the proportions on a separate line from the counts.↩︎"
  },
  {
    "objectID": "notes/12-introduction-to-mixed-effects-models.html",
    "href": "notes/12-introduction-to-mixed-effects-models.html",
    "title": "Introduction to Mixed-Effects Models",
    "section": "",
    "text": "In this set of notes, you will learn the conceptual ideas behind linear mixed-effects models, also called multilevel models or hierarchical linear models. To do this, we will use data from the file minneapolis.csv.\nThese data include repeated measurements of vertically scaled reading achievement scores for \\(n=22\\) students. We will use these data to explore the question of whether students’ reading achievement is changing over time.\n# Load libraries\nlibrary(broom.mixed) #for tidy, glance, and augment with lmer models\nlibrary(educate)\nlibrary(lme4) #for fitting mixed-effects models\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Import data\nmpls = read_csv(file = \"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/minneapolis.csv\")\n\n# View data\nmpls"
  },
  {
    "objectID": "notes/12-introduction-to-mixed-effects-models.html#residual-analysis",
    "href": "notes/12-introduction-to-mixed-effects-models.html#residual-analysis",
    "title": "Introduction to Mixed-Effects Models",
    "section": "Residual Analysis",
    "text": "Residual Analysis\n\n# Augment the model\nout = augment(lm.1)\n\n# Density plot\np1 = ggplot(data = out, aes(x = .std.resid)) +\n  educate::stat_density_confidence(model = \"normal\") +\n  geom_density() +\n  theme_minimal() +\n  xlab(\"Standardized residuals\") +\n  ylab(\"Probability density\")\n\n# Scatterplot\np2 = ggplot(data = out, aes(x = .fitted, y = .std.resid)) +\n  geom_smooth(method = \"lm\", color = \"lightgrey\") + #Create conf envelope for where the average residual should be\n  geom_hline(yintercept = 0) +\n  geom_point() +\n  theme_minimal() +\n  xlab(\"Fitted values\") +\n  ylab(\"Standardized residuals\")\n\n# Output plots\np1 | p2\n\n\n\n\n\n\n\nFigure 1: Residual plots for the fixed-effects regression model.\n\n\n\n\n\n\nBecause the loess smoother completely changes the scale, the residual_plots() function does not work here, so I created the residual plots by hand.\n\nThe assumption that the mean residual is 0 seems reasonably satisfied, as does the tenability of the normality and homoscedasticity assumptions. However, the assumption of independence (which we don’t evaluate from the common residual plots) is probably not tenable. Recall that the data includes multiple reading achievment scores for the same student. These scores (and thus the residuals) are probably correlated—this is a violation of independence which assumes that the correlation between each set of residuals is 0.\nBecause we have a variable that identifies each student, we can actually examine this by plotting the residuals separately for each student. To do so we need to include the student_id variable as a new column in the augmented data (using mutate()) so we can show the residuals versus the fitted values by student.\n\n# Augment the model and mutate on student ID\nout = augment(lm.1) |&gt;\n  mutate(student_id = mpls$student_id)\n\n### Show residuals by student\nggplot(data = out, aes(x = .fitted, y = .std.resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    theme_bw() +\n  xlab(\"Fitted values\") +\n  ylab(\"Studentized residuals\") +\n    facet_wrap(~student_id)\n\n\n\n\n\n\n\nFigure 2: Scatterplots of the standardized residuals versus the fitted values for 22 students.\n\n\n\n\n\nThe residuals for several of the students show a systematic trends of being primarily positive or negative within students. For example, the residuals for several students (e.g., Sudents 1, 3, 4, 6, 8, 9) are primarily negative. Knowing that a residual is negative gives us information that other residuals for that student are also negative. There are similar trends for students that have positive residuals. This is a sign of non-independence of the residuals. If we hadn’t had the student ID variable we could have still made a logical argument about this non-independence via substantive knowledge. For example, students who perform above average in Grade 5 will likely tend to perform well in subsequent grades (i.e., have positive residuals) relative to the population. Similarly, students who perform below average in 5th grade will tend to perform below average, relative to the population, in other grades. That is, a student’s reading achievement scores across grade levels tend to be correlated.\nTo account for this within-student correlation we need to use a statistical model that accounts for the correlation among the residuals within student. This is what mixed-effects models bring to the table. By correctly modeling the non-independence, we get more accurate standard errors and p-values.\nAnother benefit of using mixed-effects models is that we also get estimates of the variation accounted for at both the between- and within-student levels. This disaggregation of the variation allows us to determine which level is explaining more variation and to study predictors appropriate to explaining that variation. For example, suppose that you disaggregated the variation in reading achievement scores and found that:\n\n82% of the variation in these scores was at the within-student level, and\n18% of the variation in these scores was at the between-student level.\n\nWe could conclude that most of the variation in reading achievement scores is within-student, which means that we would be better off thinking about within-student predictors. (These are predictors that may vary across grade-levels for a given student.) The only within-student predictor in our example data is grade-level. Both special education status, and attendance are between-student predictors—they vary for different students, but have the same value for any one student. By including between-student predictors in the model, you would only be “chipping away” at that 18% of the variation that is between-student variation. This type of decompostion of the unexplained variation helps focus your attention and resources on the levels of variation that matter!\n\nWithin- and between-student predictors go by many other names. Other names for these predictors are time-varying predictors (within-student) and non-time-varying (between-students) predictors. Also level-1 (within-student) and level-2 (between-student) predictors."
  },
  {
    "objectID": "notes/12-introduction-to-mixed-effects-models.html#footnotes",
    "href": "notes/12-introduction-to-mixed-effects-models.html#footnotes",
    "title": "Introduction to Mixed-Effects Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome authors refer to this as a completely pooled model or the mean model.↩︎"
  },
  {
    "objectID": "notes/14-lmer-other-random-effects-and-covariates.html",
    "href": "notes/14-lmer-other-random-effects-and-covariates.html",
    "title": "LMER: Other Random-Effects and Covariates",
    "section": "",
    "text": "In this set of notes, you will continue to learn how to use the linear mixed-effects model to examine the mean change over time in a set of longitudinal/repeated measurements. You will also learn how to expand the model to allow cases to have different growth rates. Lastly, you will learn how to include covariates to evaluate whether the average growth rates vary for different groups. To do so, we will use data from the file minneapolis.csv.\nWe will use these data to explore the change in students’ reading scores over time (i.e., is there longitudinal variation in students’ reading scores). As in most longitudinal analyses, our primary goal is to examine the mean change (i.e., pattern of growth) of the outcome over time.\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom.mixed) #for tidy, glance, and augment functions for lme4 models\nlibrary(corrr)\nlibrary(educate)\nlibrary(lme4) #for fitting mixed-effects models\nlibrary(patchwork)\nlibrary(texreg)\nlibrary(tidyverse)\n\n# Import data\nmpls = read_csv(\"https://raw.githubusercontent.com/zief0002/benevolent-anteater/main/data/minneapolis.csv\")\n\n# View data\nmpls\nThe data are already in the long/tidy format, so we do not need to re-structure them. We will start the analysis by examining the average and individual growth profiles.\nggplot(data = mpls, aes(x = grade, y = reading_score)) +\n  geom_line(aes(group = student_id), alpha = 0.3) +  #Add individual profiles\n  stat_summary(fun = mean, geom = \"line\", size = 2, group = 1, color = \"#FF2D21\") + #Add mean profile line\n  theme_light() +\n  xlab(\"Grade\") +\n  ylab(\"Reading Score\")\n\n\n\n\n\n\n\nFigure 1: Plot showing the change in reading score over time for 22 students. The average growth profile is displayed as a thicker line.\nFocusing on the average growth profile, it appears that the students’ average reading score gets higher over time, and that this change is fairly linear. We will next fit the unconditional random intercepts model (to get a baseline measure of the unaccounted for variation) and the unconditional linear growth model. In the latter model we center grade on the initial measurement occasion. We display the results from these fitted models in a table.\n# Fit unconditional random-intercepts models\nlmer.a = lmer(reading_score ~ 1 + (1 | student_id), data = mpls, REML = FALSE)\n\n#Fit unconditional growth model with centered grade\nlmer.b = lmer(reading_score ~ 1 + I(grade-5) + (1 | student_id), data = mpls, REML = FALSE) \n\n# Coefficient-level output (not displayed)\n# tidy(lmer.a, effects = \"fixed\") \n# tidy(lmer.b, effects = \"fixed\") \n\n# Variance estimates (not displayed)\n# tidy(lmer.a, effects = \"ran_pars\") \n# tidy(lmer.b, effects = \"ran_pars\")\nCode\nhtmlreg(\n  l = list(lmer.a, lmer.b),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20, #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model A\", \"Model B\"), \n  custom.coef.names = c(\"Intercept\", \"Grade\"),\n  reorder.coef = c(2, 1), #Put intercept at bottom of table\n  include.loglik = FALSE, #Omit log-likelihood\n  include.aic = FALSE,    #Omit AIC\n  include.bic = FALSE,    #Omit BIC\n  include.nobs = FALSE,   #Omit sample size\n  include.groups = FALSE, #Omit group size\n  include.deviance = TRUE,\n  custom.gof.names = c(\"Deviance\", \"Level-2 Variance (Intercept)\", \"Level-1 Variance\"), # Rename variance component rows\n  custom.gof.rows = list(\n    AICc = c(AICc(lmer.a), AICc(lmer.b))  # Add AICc values\n  ), \n  reorder.gof = c(3, 4, 2, 1),\n  caption = NULL,\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"*Note.* The grade predictor was centered at the initial measurement occasion of 5th-grade.\"\n)\n\n\n\n\n\n \n\n\nModel A\n\n\nModel B\n\n\n\n\n\n\nGrade\n\n\n \n\n\n4.36\n\n\n\n\n \n\n\n \n\n\n(0.52)\n\n\n\n\nIntercept\n\n\n212.64\n\n\n206.09\n\n\n\n\n \n\n\n(3.96)\n\n\n(4.04)\n\n\n\n\nLevel-2 Variance (Intercept)\n\n\n329.66\n\n\n337.57\n\n\n\n\nLevel-1 Variance\n\n\n61.62\n\n\n29.89\n\n\n\n\nDeviance\n\n\n680.78\n\n\n633.02\n\n\n\n\nAICc\n\n\n687.07\n\n\n641.50\n\n\n\n\n\n\nNote. The grade predictor was centered at the initial measurement occasion of 5th-grade.\nFrom Model A we find that most of the unaccounted for variation in reading scores is between-student variation (84%). There is also unaccounted for variation at the within-student level. Model B includes the time-varying predictor of grade to explain within-student variation. This model explains 51.4% of the within-student variation, and \\(-2.2\\)% of the between-student variation (mathematical artifact). Below we write the global fitted equation and Student 1 and 2’s fitted equation based on the results of Model B.\n\\[\n\\begin{split}\n\\mathbf{Global:~} \\hat{\\mathrm{Reading~Score}}_{ij} &= 206 + 4.36(\\mathrm{Grade}_{ij} - 5) \\\\[1ex]\n\\mathbf{Student~1:~} \\hat{\\mathrm{Reading~Score}}_{i1} &= 177 + 4.36(\\mathrm{Grade}_{i1} - 5) \\\\[1ex]\n\\mathbf{Student~2:~} \\hat{\\mathrm{Reading~Score}}_{i2} &= 201 + 4.36(\\mathrm{Grade}_{i2}-5 ) \\\\[1ex]\n\\end{split}\n\\]\nThe coefficient interpretations from the global fitted equation are:\nThe unconditional growth model we fitted included a random-effect of intercept. This term accounted for the non-independence of reading scores in the data. It also allowed the student-specific equations to differ from the global equation in the intercept. That is, it allowed students to have different reading scores at the initial measurement occasion. Substantively, this implies that the reason an individual student has a higher (or lower) reading score than average is because their reading score at the initial measurement occasion was higher (or lower) than the average student.\nAnother reason that students might have a higher (or lower) reading score than average is because their growth rate is different than the average growth rate. The model we fitted did not allow students to have different growth rates than the average student. To evaluate whether we should allow this kind of flexibility in the model, we will focus on a plot of the individual student profiles to see whether they appear to have different rates-of-growth than the average profile."
  },
  {
    "objectID": "notes/14-lmer-other-random-effects-and-covariates.html#plots-of-the-individual-student-profiles",
    "href": "notes/14-lmer-other-random-effects-and-covariates.html#plots-of-the-individual-student-profiles",
    "title": "LMER: Other Random-Effects and Covariates",
    "section": "Plots of the Individual Student Profiles",
    "text": "Plots of the Individual Student Profiles\nAlthough we included the student profiles in the spaghetti plot we created earlier, in cases with a larger number of students it would be difficult to see the individual profiles in the spaghetti plot. To remedy this, you could facet on student so that each individual profile appears in a separate panel of the plot. (Note: If you have a lot of cases, rather than looking at all profiles, you could select a random sample of the cases and view those profiles.)\n\n# Get the mean reading score at each grade\n# Need to use this to add the average profile in each facet\nglobal = mpls |&gt;\n  group_by(grade) |&gt;\n  summarize(\n    reading_score = mean(reading_score)\n    ) |&gt;\n  ungroup()\n\n# Plot individual profiles in different panels\n# Add average profile to each panel\nggplot(data = mpls, aes(x = grade, y = reading_score)) +\n  geom_line(aes(group = student_id), alpha = 0.3) + #Individual profiles\n  geom_line(data = global, size = 2, group = 1, color = \"#FF2D21\") + #Mean profile\n  theme_light() +\n  xlab(\"Grade\") +\n  ylab(\"Reading Score\")  +\n  facet_wrap(~student_id)\n\n\n\n\n\n\n\nFigure 2: Plot showing the change in reading score over time for 22 students. The average growth profile is displayed as a thicker, red line.\n\n\n\n\n\nThe different slopes seen in the individual profiles indicate that students have different growth rates in their reading score over time, which also differ from the average growth rate. For example, Students 14 and 22 have higher growth rates than the average student. Student 16, on the other hand, has a smaller growth rate than the average student. This suggests that we also need to include a random-effect of grade (time) in our model."
  },
  {
    "objectID": "notes/14-lmer-other-random-effects-and-covariates.html#variance-estimates",
    "href": "notes/14-lmer-other-random-effects-and-covariates.html#variance-estimates",
    "title": "LMER: Other Random-Effects and Covariates",
    "section": "Variance Estimates",
    "text": "Variance Estimates\nWe can also examine the variance components for the fitted model:\n\n# Obtain variance estimates\ntidy(lmer.c, effects = \"ran_pars\")\n\n\n  \n\n\n\nWe now have four total variance components:\n\nBetween-Student Variance (Intercept): \\(\\mathrm{Var}(b_0) = 19.5^2 = 380.25\\)\nBetween-Student Variance (Rate-of-Change): \\(\\mathrm{Var}(b_1) = 2.72^2 = 7.40\\)\nBetween-Student Correlation (between Intercepts and Rate-of-Change): \\(\\mathrm{Corr}(b_0,b_1) = -0.36\\)\nWithin-Student Variance: \\(\\mathrm{Var}(e) = 4.19^2 = 17.56\\)\n\n\nBetween-student variance components are Level-2 variance components and the within-student variance component is the Level-1 variance component.\n\nAdding a random-effect resulted in two additional variance components. We now have split the between-student variance into two parts: (1) variation in students’ intercepts, and (2) variation in students growth rates. That is we are saying that variation in reading scores is due to student differences in reading scores at the initial measurement occasion, differences in students’ growth rates, and within-student variation (random error).\nThe correlation indicates the relationship between the student-specific random effects of intercept and rate-of-change. The fact that this is negative indicates that students who have a lower random-effect of intercept tend to have a higher random-effect of grade In other words, students who have a lower reading score at the initial measurement occasion tend to have a higher growth rate.\nBelow, we add Model C to our table of regression results.\n\n\nCode\nhtmlreg(\n  l = list(lmer.a, lmer.b, lmer.c),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20, #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model A\", \"Model B\", \"Model C\"),\n  custom.coef.names = c(\"Intercept\", \"Grade\"),\n  reorder.coef = c(2, 1), #Put intercept at bottom of table\n  include.loglik = FALSE, #Omit log-likelihood\n  include.aic = FALSE,    #Omit AIC\n  include.bic = FALSE,    #Omit BIC\n  include.nobs = FALSE,   #Omit sample size\n  include.groups = FALSE, #Omit group size\n  include.deviance = TRUE,\n  custom.gof.names = c(\"Deviance\", \"Level-2 Variance (Intercept)\", \"Level-1 Variance\", \"Level-2 Variance (Slope)\", \"Level-2 Covariance\"), # Rename variance component rows\n  custom.gof.rows = list(\n    AICc = c(AICc(lmer.a), AICc(lmer.b), AICc(lmer.c))  # Add AICc values\n  ),\n  reorder.gof = c(3, 5, 6, 4, 2, 1),\n  caption = NULL,\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"*Note.* The grade predictor was centered at the initial measurement occasion of 5th-grade.\"\n)\n\n\n\n\n\n \n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\n\n\n\n\nGrade\n\n\n \n\n\n4.36\n\n\n4.36\n\n\n\n\n \n\n\n \n\n\n(0.52)\n\n\n(0.70)\n\n\n\n\nIntercept\n\n\n212.64\n\n\n206.09\n\n\n206.09\n\n\n\n\n \n\n\n(3.96)\n\n\n(4.04)\n\n\n(4.23)\n\n\n\n\nLevel-2 Variance (Intercept)\n\n\n329.66\n\n\n337.57\n\n\n381.44\n\n\n\n\nLevel-2 Variance (Slope)\n\n\n \n\n\n \n\n\n7.38\n\n\n\n\nLevel-2 Covariance\n\n\n \n\n\n \n\n\n-19.12\n\n\n\n\nLevel-1 Variance\n\n\n61.62\n\n\n29.89\n\n\n17.59\n\n\n\n\nDeviance\n\n\n680.78\n\n\n633.02\n\n\n622.52\n\n\n\n\nAICc\n\n\n687.07\n\n\n641.50\n\n\n635.56\n\n\n\n\n\n\nNote. The grade predictor was centered at the initial measurement occasion of 5th-grade.\n\n\n\n\n\nNotice that the htmlreg() function produces the covariance between the intercept and slope random effects rather than the correlation that is produced from tidy(). There is a direct relationship between the correlation and covariance:\n\\[\n\\mathrm{Cov}(b_0,b_1) = \\mathrm{Corr}(b_0,b_1) \\times \\sqrt{\\mathrm{Var}(b_0)} \\times \\sqrt{\\mathrm{Var}(b_1)}\n\\]\nIn our example,\n\\[\n\\begin{split}\n\\mathrm{Cov}(b_0,b_1) &= -0.360 \\times \\sqrt{380.25} \\times \\sqrt{7.3984} \\\\[1ex]\n&= -19.09\n\\end{split}\n\\] Sometimes the variances and covariances of the random-effects are reported in a matrix referred to as the variance-covariance matrix of random-effects and denoted as G.\n\\[\n\\mathbf{G} = \\begin{bmatrix}\\mathrm{Var}(b_0) & \\mathrm{Cov}(b_0,b_1)\\\\ \\mathrm{Cov}(b_0,b_1) & \\mathrm{Var}(b_1)\\end{bmatrix}\n\\]\nIn our example,\n\\[\n\\mathbf{G} = \\begin{bmatrix}380.25 & -19.09\\\\ -19.09 & 7.40\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "notes/14-lmer-other-random-effects-and-covariates.html#explained-variation-and-pseudo-r2-values",
    "href": "notes/14-lmer-other-random-effects-and-covariates.html#explained-variation-and-pseudo-r2-values",
    "title": "LMER: Other Random-Effects and Covariates",
    "section": "Explained Variation and Pseudo-\\(R^2\\) Values",
    "text": "Explained Variation and Pseudo-\\(R^2\\) Values\nIt is difficult to compare the variance components for Model C to those from the unconditional random intercepts model to get pseudo-\\(R^2\\) values, like we did for Model B. This is because Model C includes an additional source of unexplained variation, namely \\(Var(b_1)\\) that the unconditional random intercepts model did not include.\nAs we add other fixed-effects predictors to the model, the variance components from those models could be compared to the variance components from Model C (so long as these models continue to include both the random intercepts and slopes). In other words, Model C becomes the new baseline model for other models that also include random intercepts and slopes."
  },
  {
    "objectID": "notes/14-lmer-other-random-effects-and-covariates.html#student-specific-fitted-equations",
    "href": "notes/14-lmer-other-random-effects-and-covariates.html#student-specific-fitted-equations",
    "title": "LMER: Other Random-Effects and Covariates",
    "section": "Student-Specific Fitted Equations",
    "text": "Student-Specific Fitted Equations\nWe again need to obtain the estimated random-effects for students in order to write their fitted equations. Moreover, we will also need their special education status. For example, we obtain Student 1’s random-effects using:\n\n# Obtain student-specific coefficients for Student 2\ntidy(lmer.d, effects = \"ran_vals\") |&gt;\n  filter(level == 1)\n\n\n  \n\n\n\nWe also note that Student 1 is a non-special education student. Writing this student’s fitted equation:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Reading~Score}_{i1}} &= \\bigg[208 -33.7\\bigg] + \\bigg[4.36 + 1.26\\bigg](\\mathrm{Grade}_{i1}-5) - 15.77(0) \\\\[1ex]\n&= 174.3 + 5.62(\\mathrm{Grade}_{i1}-5)\n\\end{split}\n\\]\nStudent 8, on the other hand is a special education student. That student’s random-effects are \\(\\hat{b}_0=-2.14\\) and \\(\\hat{b}_1=-2.51\\). Their fitted equation is:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Reading~Score}_{i1}} &= \\bigg[208 - 2.14\\bigg] + \\bigg[4.36 - 2.51\\bigg](\\mathrm{Grade}_{i1}-5) - 15.77(1) \\\\[1ex]\n&= 190.09 + 1.85(\\mathrm{Grade}_{i1}-5)\n\\end{split}\n\\]\nWe could also plot the average profiles and also any student profiles that we wanted to. Here I will plot the two average profiles (one for each special education status), along with the profile for Student 8. Since Student 8 is a special education student, I will use the same color for that student’s profile and the average profile for special education students.\n\nggplot(data = mpls, aes(x = grade-5, y = reading_score)) +\n  geom_point(alpha = 0) +  #Add individual profiles\n  geom_abline(intercept = 208, slope = 4.36, color = \"#D55E00\") + #Non-Sped\n  geom_abline(intercept = 192.2, slope = 4.36, color = \"#0072B2\") + #Sped\n  geom_abline(intercept = 190.09, slope = 1.85, color = \"#0072B2\", linetype = \"dashed\") + #Student 8 (Sped)\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"5th\", \"6th\", \"7th\", \"8th\")\n    ) +\n  ylab(\"Reading Score\")\n\n\n\n\n\n\n\nFigure 3: Plot showing the average growth profile of reading scores over time for special education (blue) and non-special education (vermillion) students based on Model D. The individual growth profile for Student 8 (dotted line) is also displayed.\n\n\n\n\n\n\nSince the model was based on grade level being centered at the initial measurement occasion, we need to use the centered grade in the x= argument of the ggplot() aesthetic mapping. Then we can re-label the x-scale to our original grade levels for easier interpretation for readers."
  },
  {
    "objectID": "notes/14-lmer-other-random-effects-and-covariates.html#controlling-for-attendance",
    "href": "notes/14-lmer-other-random-effects-and-covariates.html#controlling-for-attendance",
    "title": "LMER: Other Random-Effects and Covariates",
    "section": "Controlling for Attendance",
    "text": "Controlling for Attendance\nWe will fit one additional model that adds to Model E by controlling for attendance. The statistical model can be expressed as:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\beta_2(\\mathrm{Special~Education}_j) + \\beta_3(\\mathrm{Attendance}_j) + \\\\\n&\\beta_4(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]\nwhere,\n\n\\(\\mathrm{Reading~Score}_{ij}\\) is the reading score for Student j at time point i;\n\\(\\beta_0\\) is the fixed-effect of intercept;\n\\(b_{0j}\\) is the random-effect of intercept for Student j;\n\\(\\beta_1\\) is the fixed-effect of grade-level;\n\\(\\mathrm{Grade}_{ij}\\) is the grade-level for Student j at time point i;\n\\(b_{1j}\\) is the random-effect of grade-level for Student j;\n\\(\\beta_2\\) is the fixed-effect of special education status;\n\\(\\mathrm{Special~Education}_{j}\\) is the dummy-coded special education status for Student j\n\\(\\beta_3\\) is the fixed-effect of attendance;\n\\(\\mathrm{Attendance}_{j}\\) is the attendance for Student j\n\\(\\beta_4\\) is the interaction-effect (fixed-effect) between special education status grade-level; and\n\\(\\epsilon_{ij}\\) is the error for Student j at time point i.\n\nWe fit the model using ML estimation, obtain both the fixed-effect estimates and estimated variance components, and add these results to our table of regression results. To facilitate better interpretation of the intercept, we will mean center the attendance variable.\n\n# Fit model\nlmer.f = lmer(reading_score ~ 1 + I(grade-5) + sped + I(attendance - 0.9564) + I(grade-5):sped +\n                (1 + I(grade-5) | student_id), data = mpls, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.f, effects = \"fixed\")\n\n\n  \n\n\n# Obtain variance estimates\ntidy(lmer.f, effects = \"ran_pars\")\n\n\n  \n\n\n\n\n\nCode\nhtmlreg(\n  l = list(lmer.a, lmer.b, lmer.c, lmer.d, lmer.e, lmer.f),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20, #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model A\", \"Model B\", \"Model C\", \"Model D\", \"Model E\", \"Model F\"),\n  custom.coef.names = c(\"Intercept\", \"Grade\", \"Special Education Status\", \"Attendance\",\n                        \"Grade x Special Education Status\"),\n  reorder.coef = c(2:5, 1), #Put intercept at bottom of table\n  include.loglik = FALSE, #Omit log-likelihood\n  include.aic = FALSE,    #Omit AIC\n  include.bic = FALSE,    #Omit BIC\n  include.nobs = FALSE,   #Omit sample size\n  include.groups = FALSE, #Omit group size\n  include.deviance = TRUE,\n  custom.gof.names = c(\"Deviance\", \"Level-2 Variance (Intercept)\", \"Level-1 Variance\", \"Level-2 Variance (Slope)\", \"Level-2 Covariance\"), # Rename variance component rows\n  custom.gof.rows = list(\n    AICc = c(AICc(lmer.a), AICc(lmer.b), AICc(lmer.c), AICc(lmer.d), AICc(lmer.e), AICc(lmer.f))  # Add AICc values\n  ),\n  reorder.gof = c(3, 5, 6, 4, 2, 1),\n  caption = NULL,\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"*Note.* The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group. The attendance variable was mean centered.\"\n)\n\n\n\n\n\n \n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\nModel D\n\n\nModel E\n\n\nModel F\n\n\n\n\n\n\nGrade\n\n\n \n\n\n4.36\n\n\n4.36\n\n\n4.36\n\n\n4.11\n\n\n4.11\n\n\n\n\n \n\n\n \n\n\n(0.52)\n\n\n(0.70)\n\n\n(0.70)\n\n\n(0.74)\n\n\n(0.74)\n\n\n\n\nSpecial Education Status\n\n\n \n\n\n \n\n\n \n\n\n-15.77\n\n\n-19.40\n\n\n-22.01\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(10.96)\n\n\n(11.61)\n\n\n(11.38)\n\n\n\n\nAttendance\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n1.89\n\n\n1.89\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(2.01)\n\n\n(2.01)\n\n\n\n\nGrade x Special Education Status\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n218.39\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(89.83)\n\n\n\n\nIntercept\n\n\n212.64\n\n\n206.09\n\n\n206.09\n\n\n208.24\n\n\n208.74\n\n\n209.10\n\n\n\n\n \n\n\n(3.96)\n\n\n(4.04)\n\n\n(4.23)\n\n\n(4.26)\n\n\n(4.29)\n\n\n(4.18)\n\n\n\n\nLevel-2 Variance (Intercept)\n\n\n329.66\n\n\n337.57\n\n\n381.44\n\n\n338.62\n\n\n337.07\n\n\n320.00\n\n\n\n\nLevel-2 Variance (Slope)\n\n\n \n\n\n \n\n\n7.38\n\n\n7.38\n\n\n6.96\n\n\n6.96\n\n\n\n\nLevel-2 Covariance\n\n\n \n\n\n \n\n\n-19.12\n\n\n-15.60\n\n\n-14.79\n\n\n-23.85\n\n\n\n\nLevel-1 Variance\n\n\n61.62\n\n\n29.89\n\n\n17.59\n\n\n17.59\n\n\n17.59\n\n\n17.59\n\n\n\n\nDeviance\n\n\n680.78\n\n\n633.02\n\n\n622.52\n\n\n620.61\n\n\n619.74\n\n\n615.05\n\n\n\n\nAICc\n\n\n687.07\n\n\n641.50\n\n\n635.56\n\n\n636.01\n\n\n637.56\n\n\n635.36\n\n\n\n\n\n\nNote. The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group. The attendance variable was mean centered.\n\n\n\n\n\nThe fitted equation for the average model is:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &209 + 4.11(\\mathrm{Grade}_{ij}-5) - 22(\\mathrm{Special~Education}_j) + 218(\\mathrm{Attendance}_j) +\\\\ &1.89(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j)\n\\end{split}\n\\]\nInterpreting the interaction effect:\n\nThe effect of grade-level on reading scores varies by special education status after controlling for differences in attendance. OR\nThe effect of special education status on reading scores varies by grade-level after controlling for differences in attendance.\n\nTo better understand this interaction, we can write out and then plot the average fitted equation for special education students and that for non-special education students. We will control for attendance by setting it to its mean value. Note that since we mean centered attendance, this will drop the attendance term from the fitted equation.\n\\[\n\\begin{split}\n\\mathbf{Non\\mbox{-}SpEd:~}\\mathrm{Reading~Score}_{ij} &= 209 + 4.11(\\mathrm{Grade}_{ij}-5) - 22(0) + 218(0.9564 - 0.9564) + 1.89(\\mathrm{Grade}_{ij}-5)(0) \\\\[1ex]\n&= 209 + 4.11(\\mathrm{Grade}_{ij}-5) \\\\[4ex]\n\\mathbf{SpEd:~}\\mathrm{Reading~Score}_{ij} &= 209 + 4.11(\\mathrm{Grade}_{ij}-5) - 22(1) + 218(0.9564 - 0.9564) + 1.89(\\mathrm{Grade}_{ij}-5)(1) \\\\[1ex]\n&= 187 + 6.00(\\mathrm{Grade}_{ij}-5)\n\\end{split}\n\\]\nThis results in essentially the same plot as Model E (the special education students profile will have a slightly lower reading score in the 5th grade). The difference is now the profiles represent the growth in reading scores for special education and non special education students who have an average attendance. Student specific fitted equations would have to be computed using the student’s special education status and the student’s actual attendance, as well as their random-effects."
  },
  {
    "objectID": "notes/14-lmer-other-random-effects-and-covariates.html#footnotes",
    "href": "notes/14-lmer-other-random-effects-and-covariates.html#footnotes",
    "title": "LMER: Other Random-Effects and Covariates",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the random effects we included in Model D allow individual students to have different growth rates from the average growth rate.↩︎"
  },
  {
    "objectID": "readings/01-regression-review.html",
    "href": "readings/01-regression-review.html",
    "title": "📖 Regression Review",
    "section": "",
    "text": "Here are some things to do before class:\n\nUpdate R to the most recent version (v. 4.3.2; Eye Holes)\nUpdate RStudio Desktop to the most recent version (v. 2023.12.0+369)\nUpdate all of your R packages (In the Packages tab in RStudio click Update)\nUpdate the {educate} package (to at least v.0.3.0.1). This is a github package, so see the instructions in Installing Packages from GitHub\n\nRemind yourself of the EPsy 8251 content\n\nStatistical Modeling and Computation for Educational Scientists"
  },
  {
    "objectID": "readings/03-introduction-to-quarto-day-02.html",
    "href": "readings/03-introduction-to-quarto-day-02.html",
    "title": "Introduction to Quarto (Day 2)",
    "section": "",
    "text": "For the next class you will need a citation manager. If you don’t have a citation manager, I strongly recommend Zotero.\n\nInstall Zotero.\n\nIf you use a different citation manager, you will need to figure out how to export a .BIB file from your citation manager."
  },
  {
    "objectID": "readings/03-introduction-to-quarto-day-02.html#install-a-citation-manager",
    "href": "readings/03-introduction-to-quarto-day-02.html#install-a-citation-manager",
    "title": "Introduction to Quarto (Day 2)",
    "section": "",
    "text": "For the next class you will need a citation manager. If you don’t have a citation manager, I strongly recommend Zotero.\n\nInstall Zotero.\n\nIf you use a different citation manager, you will need to figure out how to export a .BIB file from your citation manager."
  },
  {
    "objectID": "readings/03-introduction-to-quarto-day-02.html#download-and-import-two-papers",
    "href": "readings/03-introduction-to-quarto-day-02.html#download-and-import-two-papers",
    "title": "Introduction to Quarto (Day 2)",
    "section": "Download and Import Two Papers",
    "text": "Download and Import Two Papers\nDownload the following two papers from the UMN library:\n\nCarmichael, L. (1954). Laziness and the scholarly life. The Scientific Monthly, 78(4), 208–213.\nRoss, C. T., Winterhalder, B., & McElreath, R. (2020). Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates. Social Psychological and Personality Science, 194855062091607. https://doi.org/10.1177/1948550620916071\n\nImport both of these into Zotero by dragging the PDFs into Zotero. After they are imported, check that all the metadata is correct. (Page numbers often need to be updated in Zotero.)"
  },
  {
    "objectID": "readings/04-probability-distributions.html",
    "href": "readings/04-probability-distributions.html",
    "title": "Introduction to Probability Distributions",
    "section": "",
    "text": "If you need to refresh your knowledge about probability distributions, I recommend reading Section 3.1.1: (Probability Basics) in Fox (2009). You could also go through the Kahn Academy: Random Variables and Probability Distributions tutorial.\nHere is some ideas you will need to be familiar with from those readings/tutorials:\nBelow, I introduce some ideas about probability distributions (especially continuous probability distributions) that will be useful for understanding likelihood."
  },
  {
    "objectID": "readings/04-probability-distributions.html#probability-density",
    "href": "readings/04-probability-distributions.html#probability-density",
    "title": "Introduction to Probability Distributions",
    "section": "Probability Density",
    "text": "Probability Density\nIn a continuous distribution we also need to account be able to talk about the fact that some outcomes are more likely than other outcomes. For example, in our standard normal distribution outcomes near zero are more probable than outcomes near 1, which are more probable than outcomes near 2, etc. Since we can’t use probability to do this (remember the probability of each outcome is the same, namely 0), we use something called probability density. This is akin to a relative probability, so outcomes with a higher probability density are more likely than outcomes with a lower probability density.\nThe mapping of all the outcomes to their probability densities is called a probability density function (PDF). Thus the equation or “bell-shaped” curve describing the standard normal distribution in Figure 1 is technically a PDF Here are some laws governing PDFs:\n\nProbability densities are always positive.\nThe probability of an outcome x between a and b equals the integral (area under the curve) between a and b of the probability density function. That is:\n\n\\[\np(a \\leq x \\leq b) = \\int_a^b p(x) dx\n\\]\n\nThe area under the curve from negative infinity to positive infinity is 1. That is:\n\n\\[\np(-\\infty \\leq x \\leq +\\infty) = \\int_{-\\infty}^{+\\infty} p(x) = 1\n\\]\nNext we will look at the PDF for a normal distribution."
  },
  {
    "objectID": "readings/04-probability-distributions.html#other-useful-r-functions-for-working-with-normal-probability-distributions",
    "href": "readings/04-probability-distributions.html#other-useful-r-functions-for-working-with-normal-probability-distributions",
    "title": "Introduction to Probability Distributions",
    "section": "Other Useful R Functions for Working with Normal Probability Distributions",
    "text": "Other Useful R Functions for Working with Normal Probability Distributions\nWe use dnorm() when we want to compute the probability density associated with a particular x-value in a given normal distribution. There are three other functions that are quite useful for working with the normal probability distribution:\n\npnorm() : To compute the probability (area under the PDF)\nqnorm() : To compute the \\(x\\) value given a particular probability\nrnorm() : To draw a random observation from the distribution\n\nEach of these function also requires the arguments mean= and sd=. Below we will examine how to use each of these additional functions."
  },
  {
    "objectID": "readings/04-probability-distributions.html#pnorm-computing-cumulative-probability-density",
    "href": "readings/04-probability-distributions.html#pnorm-computing-cumulative-probability-density",
    "title": "Introduction to Probability Distributions",
    "section": "pnorm(): Computing Cumulative Probability Density",
    "text": "pnorm(): Computing Cumulative Probability Density\nThe function pnorm() computes the area under the PDF curve from \\(-\\infty\\) to some x-value. (Sometimes this is referred to as the cumulative probability density of x.) It is important to note that the PDF is defined such that the entire area under the curve is equal to 1. Because of this, we can also think about using area under the curve as an analog to probability in a continuous distribution.\nFor example, we might ask about the probability of observing an x-value that is less than or equal to 65 given it is from a \\(\\mathcal{N}(50,10)\\) distribution. Symbolically, we want to find:\n\\[\nP\\bigg(x \\leq 65 \\mid \\mathcal{N}(50,10)\\bigg)\n\\]\nThis is akin to finding the proportion of the area under the \\(\\mathcal{N}(50,10)\\) PDF that is to the left of 65. The figure below shows a graphical depiction of the cumulative probability density for \\(x=65\\).\n\n\nCode\n# Create dataset\nfig_03 = data.frame(\n  X = seq(from = 10, to = 90, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 50, sd = 10)\n    )\n\n# Filter out X&lt;=65\nshaded = fig_03 %&gt;%\n  filter(X &lt;= 65)\n\n# Create plot\nggplot(data = fig_03, aes(x = X, y = Y)) +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"X\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\n\nFigure 3: Plot of the probability density function (PDF) for a \\(\\mathcal{N}(50,10)\\) distribution. The area that is shaded grey (relative to the total area under the PDF) represents the cumulative probability density for \\(x=65\\).\n\n\n\n\n\nWe can compute the cumulative probability density using the pnorm() function. The “p” stand for “probability”.\n\n# Find P(x&lt;=65 | N(50,10) )\npnorm(q = 65, mean = 50, sd = 10)\n\n[1] 0.9331928\n\n\nWe can interpret this as:\n\nThe probability of observing an x-value that is less than or equal to 65 (if it is drawn from a normal distribution with a mean of 50 and standard deviation of 10) is 0.933.\n\nIn mathematics, the area under a curve is called an integral. The grey-shaded area in the previous figure can also be expressed as an integral of the probability density function:\n\\[\n\\int_{-\\infty}^{65} p(x) dx\n\\]\nwhere \\(p(x)\\) is the PDF for the normal distribution.\nThe most common application for finding the cumulative density is to compute a p-value. The p-value is just the area under the distribution (curve) that is AT LEAST as extreme as some observed value. For example, assume we computed a test statistic of \\(z=2.5\\), and were evaluating whether this was different from 0 (two-tailed test). Graphically, we want to determine the proportion of the area under the PDF that is shaded grey in the figure below.\n\n\nCode\n# Create data\nfig_04 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 0, sd = 1)\n    )\n\n# Filter data for shading\nshade_01 = fig_04 %&gt;%\n  filter(X &gt;= 2.5)\n\nshade_02 = fig_04 %&gt;%\n  filter(X &lt;= -2.5)\n\n# Create plot\nggplot(data = fig_04, aes(x = X, y = Y)) +\n  geom_ribbon(data = shade_01, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_ribbon(data = shade_02, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"z\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\n\nFigure 4: Plot of the probability density function (PDF) for the standard normal distribution (\\(M=0\\), \\(SD=1\\)). The cumulative density representing the p-value for a two-tailed test evaluating whether \\(\\mu=0\\) using an observed mean of 2.5 is also displayed.\n\n\n\n\n\nIf the distribution of the test statistic is normally distributed, we can use pnorm() to compute the p-value. If we assume the test statistic, z, has been scaled to use standardized units, the standard deviation we use in pnorm() will be sd=1. The mean is based on the value being tested in the null hypothesis. In most null hypotheses, we are testing a difference from 0 (e.g., \\(H_0: \\mu=0\\), \\(H_0: \\beta=0\\)), so we would use mean=0 in the pnorm() function.\nRemember, pnorm() computes the proportion of the area under the curve TO THE LEFT of a particular value. Here we will compute the area to the left of \\(-2.5\\) and then double it to produce the actual p-value. (We can double it because the normal distribution is symmetric so the area to the left of \\(-2.5\\) is the same as the area to the right of \\(+2.5\\).)\n\n# Compute the p-value based on z=2.5\n2 * pnorm(q = -2.5, mean = 0, sd = 1)\n\n[1] 0.01241933\n\n\nWe interpret this p-value as:\n\nThe probability of observing a statistic at least as extreme as 2.5, assuming the null hypothesis is true, is 0.012. This is evidence against the null hypothesis since the data are inconsistent with the assumed hypothesis.\n\n\n\nqnorm(): Computing Quantiles\nThe qnorm() function is essentially the inverse of the pnorm() function. The pnorm() function computes the cumulative probability GIVEN a particular quantile (x-value). The qnorm() function computes the quantile GIVEN a cumulative probability. For example, in the \\(\\mathcal{N}(50, 10)\\) distribution, half of the area under the PDF is below the x-value (quantile) of 50.\nTo use the qnorm() function to give the x-value (quantile) that defines the lower 0.5 of the area under the \\(\\mathcal{N}(50, 10)\\) PDF, the syntax would be:\n\n# Find the quantile that has a cumulative density of 0.5 in the N(50, 10) distribution\nqnorm(p = 0.5, mean = 50, sd = 10)\n\n[1] 50\n\n\n\n\n\nrnorm(): Generating Random Observations\nThe rnorm() function can be used to generate random observations drawn from a specified normal distribution. Aside from the mean= and sd= arguments, we also need to specify the number of observations to generate by including the argument n=. For example, to generate 15 observations drawn from a \\(\\mathcal{N}(50,10)\\) distribution we would use the following syntax:\n\n# Generate 15 observations from N(50,10)\nset.seed(100)\nrnorm(n = 15, mean = 50, sd = 10)\n\n [1] 44.97808 51.31531 49.21083 58.86785 51.16971 53.18630 44.18209 57.14533\n [9] 41.74741 46.40138 50.89886 50.96274 47.98366 57.39840 51.23380\n\n\nThe set.seed() function sets the state of the random number generator used in R so that the results are reproducible. If you don’t use set.seed() you will get a different set of observations each time you run rnorm(). Here we set the starting seed to 100, but you can set this to any integer you want."
  },
  {
    "objectID": "readings/04-probability-distributions.html#computing-f-from-the-anova-partitioning",
    "href": "readings/04-probability-distributions.html#computing-f-from-the-anova-partitioning",
    "title": "Introduction to Probability Distributions",
    "section": "Computing F from the ANOVA Partitioning",
    "text": "Computing F from the ANOVA Partitioning\nWe can also compute the model-level F-statistic directly using the partitioning of variation from the ANOVA table.\n\n# Partition the variation\nanova(lm.1)\n\n\n  \n\n\n\nThe F-statistic is a ratio of the mean square for the model and the mean square for the error. To compute a mean square we use the general formula:\n\\[\n\\mathrm{MS} = \\frac{\\mathrm{SS}}{\\mathrm{df}}\n\\]\nThe model includes both the education and seniority predictor, so we combine the SS and df. The MS model is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Model}} &= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{4147.3 + 722.9}{1 + 1} \\\\[1ex]\n&= \\frac{4870.2}{2} \\\\[1ex]\n&= 2435.1\n\\end{split}\n\\]\nThe MS error is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Error}} &= \\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{1695.3 }{29} \\\\[1ex]\n&= 58.5\n\\end{split}\n\\]\nThen, we compute the F-statistic by computing the ratio of these two mean squares.\n\\[\n\\begin{split}\nF &= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{2435.1}{58.5} \\\\[1ex]\n&= 41.6\n\\end{split}\n\\]\nSince a mean square represents the average amount of variation (per degree of freedom), we can see that F is a ratio between the average amount of variation explained by the model and the average amount of variation unexplained by the model. In our example, this ratio is 41.6; on average the model explains 41.6 times the variation that is unexplained.\nNote that this is an identical computation (although reframed) as the initial computation for F. We can use mathematics to show this equivalence:\n\\[\n\\begin{split}\nF &= \\frac{R^2}{1-R^2} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Total}}}}{\\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Total}}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\mathrm{MS}_{\\mathrm{Model}} \\times \\frac{1}{\\mathrm{MS}_{\\mathrm{Error}}}\\\\[1ex]\n&= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}}\n\\end{split}\n\\]"
  },
  {
    "objectID": "readings/04-probability-distributions.html#testing-the-model-level-null-hypothesis",
    "href": "readings/04-probability-distributions.html#testing-the-model-level-null-hypothesis",
    "title": "Introduction to Probability Distributions",
    "section": "Testing the Model-Level Null Hypothesis",
    "text": "Testing the Model-Level Null Hypothesis\nWe evaluate our test statistic (F in this case) in the appropriate test distribution, in this case an F-distribution with 2 and 29 degrees of freedom. The figure below, shows the \\(F(2,29)\\)-distribution as a solid, black line. The p-value is the area under the curve that is at least as extreme as the observed F-value of 41.7.\n\n\nCode\n# Create data\nfig_11 = data.frame(\n  X = seq(from = 0, to = 50, by = 0.01)\n  ) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 2, df2 = 29)\n    )\n\n# Filter shaded area\nshade = fig_11 %&gt;%\n  filter(X &gt;= 41.7)\n\n# Create plot\nggplot(data = fig_11, aes(x = X, y = Y)) +\n  geom_line() +\n  theme_bw() +\n  geom_ribbon(data = shade, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\n\n\n\nFigure 11: Plot of the probability density function (PDF) for the \\(F(2,~29)\\)-distribution. The cumulative density representing the p-value for a test evaluating whether \\(\\rho^2=0\\) using an observed F-statistic of 41.7 is also displayed.\n\n\n\n\n\nThe computation using the cumulative density function, pf(), to obtain the p-value is:\n\n# p-value for F(2,29)=41.7\n1 - pf(41.7, df1 = 2, df2 = 29)\n\n[1] 0.000000002942114\n\n\nBecause we want the upper-tail, rather than taking the difference from 1, we can also use the lower.tail=FALSE argument in pf().\n\n# p-value for F(2,29)=41.7\npf(41.7, df1 = 2, df2 = 29, lower.tail = FALSE)\n\n[1] 0.000000002942114"
  },
  {
    "objectID": "readings/04-probability-distributions.html#mean-squares-are-variance-estimates",
    "href": "readings/04-probability-distributions.html#mean-squares-are-variance-estimates",
    "title": "Introduction to Probability Distributions",
    "section": "Mean Squares are Variance Estimates",
    "text": "Mean Squares are Variance Estimates\nMean squares are also estimates of the variance. Consider the computational formula for the sample variance,\n\\[\n\\hat{\\sigma}^2 = \\frac{\\sum(Y - \\bar{Y})^2}{n-1}\n\\]\nThis is the total sum of squares divided by the total df. The variance of the outcome variable is interpreted as the average amount of variation in the outcome variable (in the squared metric). Thus, it is also referred to as the mean square total.\nWhen we compute an F-statistic, we are finding the ratio of two different variance estimates—one based on the model (explained variance) and one based on the error (unexplained variance). Under the null hypothesis that \\(\\rho^2 = 0\\), we are assuming that all the variance is unexplained. In that case, our F-statistic would be close to zero. When the model explains a significant amount of variation, the numerator gets larger relative to the denominator and the F-value is larger.\nThe mean squared error (from the anova() output) plays a special role in regression analysis. It is the variance estimate for the conditional distributions of the residuals in our visual depiction of the distributional assumptions of the residuals underlying linear regression.\n\n\n\n\n\n\n\n\nFigure 12: Visual Depiction of the Distributional Assumptions of the Residuals Underlying Linear Regression\n\n\n\n\n\nRecall that we made implicit assumptions about the conditional distributions of the residuals, namely that they were identically and normally distributed with a mean of zero and some variance. Based on the estimate of the mean squared error, the variance of each of these distributions is 58.5.\nWhile the variance is a mathematical convenience, the standard deviation is often a better descriptor of the variation in a distribution since it is measured in the original metric. The standard deviation fro the residuals (error) is 7.6. Because the residuals are statistics (summaries computed from sample data), their standard deviation is referred to as a “standard error”.\n\nThe residual standard error (RSE) is sometimes referred to as the Root Mean Squared Error (RMSE).\n\n\n# Compute RMSE\nsqrt(58.5)\n\n[1] 7.648529\n\n\nWhy is this value important? It gives the expected variation in the conditional residual distributions, which is a measure of the average amount of error. For example, since all of the conditional distributions of the residuals are assumed to be normally distributed, we would expect that 95% of the residuals would fall between \\(\\pm2\\) standard errors from 0; or, in this case, between \\(-15.3\\) and \\(+15.3\\). Observations with residuals that are more extreme may be regression outliers.\nMore importantly, it is a value that we need to estimate in order to specify the model."
  },
  {
    "objectID": "readings/04-probability-distributions.html#confidencecompatibility-intervals-for-the-coefficients",
    "href": "readings/04-probability-distributions.html#confidencecompatibility-intervals-for-the-coefficients",
    "title": "Introduction to Probability Distributions",
    "section": "Confidence/Compatibility Intervals for the Coefficients",
    "text": "Confidence/Compatibility Intervals for the Coefficients\nThe confidence interval for the kth regression coefficient is computed as:\n\\[\n\\mathrm{CI} = \\hat\\beta_k \\pm t^{*}(\\mathrm{SE}_{\\hat\\beta_k})\n\\]\nwhere \\(t^*\\) is the quantile of the t-distribution that defines the confidence level for the interval. (This t-distribution, again, has degrees-of-freedom equal to the error df in the model.) The confidence level is related to the alpha level (type I error rate) used in inference. Namely,\n\\[\n\\mathrm{Confidence~Level} = 1 - \\alpha\n\\]\nSo, if you use \\(\\alpha=.05\\), then the confidence level would be \\(.95\\), and we would call this a 95% confidence interval. The alpha value also helps determine the quantile we use in the CI formula,\n\\[\nt^* = (1-\\frac{\\alpha}{2}) ~ \\mathrm{quantile}\n\\] For the example using \\(\\alpha=.05\\), a 95% confidence interval, the \\(t^*\\) value would be associated with the quantile of 0.975. We would denote this as:\n\\[\nt^{*}_{.975}\n\\]\nSay we wanted to find the 95% confidence interval for the education coefficient. We know that the estimated coefficient for education is 2.25, and the standard error for this estimate is 0.335. We also know that based on the model fitted, the residual df is 29. We need to find the 0.975th quantile in the t-distribution with 29 df.\n\n# Find 0.975th quantile\nqt(p = 0.975, df = 29)\n\n[1] 2.04523\n\n\nNow we can use all of this information to compute the confidence interval:\n\\[\n\\begin{split}\n95\\%~CI  &= 2.25 \\pm 2.04523(0.335) \\\\[1ex]\n&= \\big[1.56,~2.94\\big]\n\\end{split}\n\\]"
  },
  {
    "objectID": "readings/04-probability-distributions.html#footnotes",
    "href": "readings/04-probability-distributions.html#footnotes",
    "title": "Introduction to Probability Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember, the mean and standard deviations in the population are called “parameters”.↩︎\nWhether this is actually t-distributed depends on whether the model assumptions are met.↩︎"
  },
  {
    "objectID": "readings/06-information-criteria-and-model-selection.html",
    "href": "readings/06-information-criteria-and-model-selection.html",
    "title": "📖 Polynomial Effects",
    "section": "",
    "text": "Required\nRead the following:\n\nElliott, L. P., & Brook, B. W. (2007). Revisiting Chamberlin: Multiple working hypotheses for the 21st century. BioScience, 57(7), 608–614. doi: 10.1641/B570708\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about information criteria and model selection. Here are some resources that may be helpful in that endeavor:\n\nAnderson, D. R. (2008). Model based inference in the life sciences: A primer on evidence. New York: Springer. [Optional Textbook]\nBurnham, K. P., & Anderson, D. R. (2002). Model selection and multimodel inference: A practical information-theoretic approach. New York: Springer.\nBurnham, K. P., Anderson, D. R., & Huyvaert, K. P. (2010). AIC model selection and multimodel inference in behavioral ecology: Some background, observations, and comparisons. Behavioral Ecology and Sociobiology, 65(1), 23–35."
  },
  {
    "objectID": "readings/08-logarithmic-transformations-predictor.html",
    "href": "readings/08-logarithmic-transformations-predictor.html",
    "title": "📖 Log-Transforming the Predictor",
    "section": "",
    "text": "Required\nRead the following:\n\nOsborne, Jason (2002). Notes on the use of data transformations. Practical Assessment, Research & Evaluation, 8(6).\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about log-transformations. Here are some resources that may be helpful in that endeavor:\n\nUCLA IDRE: Interpreting Log-Transformed Variables in Regression\nInterpret Regression Coefficient Estimates"
  },
  {
    "objectID": "readings/13-lmer-average-change-over-time.html",
    "href": "readings/13-lmer-average-change-over-time.html",
    "title": "📖 LMER: Average Change Over Time",
    "section": "",
    "text": "Required\nRead the following::\n\nGrolemund, G., & Wickham, H. (2017). Relational data. In R for data science: Visualize, model, transform, tidy, and import data. O’Reilly."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Calendar",
    "section": "",
    "text": "The calendar below lists the tentative course topics. The dates listed are subject to change at the instructor’s discretion.\nAs part of the course, there are several articles, papers and technical reports that you will need to read during the semester. Most of the articles themselves are accessible through the University of Minnesota library website. In order to access the full text of some of the articles, you will need to log in using your University x500 username and password. More detailed information, including references or links to specific readings, are linked under the day’s “Reading”.\n\n\n\n\n\n\n\n\nDate\n\n\nPrep\n\n\nTopic\n\n\nNotes\n\n\nQMD\n\n\nScript\n\n\n\n\n\n\n\n\n\n\n \n\n\nJan. 16\n\n\n\n\n\nWelcome to Epsy 8252\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nJan. 18\n\n\n\n\n\nRegression Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nJan. 23\n\n\n\n\n\n\n\nUnit 01: Introduction to Quarto\n\n\n\n\n \n\n\nJan. 25\n\n\n\n\n\nProject Organization\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nJan. 30\n\n\n\n\n\nIntroduction to Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nFeb. 01\n\n\n\n\n\n\n\nMore Quarto (Citations, Etc.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Tables with {gt}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 02: Model Selection and Evidence\n\n\n\n\n \n\n\nFeb. 06\n\n\n\n\n\nLikelihood: A Framework for Evidence\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nFeb. 08\n\n\n\n\n \n\n\nOptional\n\n\n\n\n\nLikelihood: A Framework for Estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 03: Dealing with Nonlinearity\n\n\n\n\n \n\n\nFeb. 13\n\n\n\n\n\nPolynomial Effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nFeb. 15\n\n\n\n\n \n\n\nFeb. 20\n\n\n\n\nUnit 02 (Revisited): Model Selection and Evidence\n\n\n\n\n \n\n\nFeb. 22\n\n\n\n\n\nInformation Criteria and Model Selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 03 (Revisited): Dealing with Nonlinearity\n\n\n\n\n \n\n\nFeb. 27\n\n\n\n\n\nLog-Transforming the Outcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nFeb. 29\n\n\n\n\n \n\n\nMar. 06–10\n\n\n\n\n🎉🍹 SPRING BREAK — NO CLASS   🛌\n\n\n\n\n \n\n\nMar. 12\n\n\n\n\n\nLog-Transforming the Outcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nMar. 14\n\n\n\n\n\nLog-Transforming the Predictor\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nMar. 19\n\n\n\n\n \n\n\nMar. 21\n\n\n\n\n\nRule of the Bulge—An Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 04: Modeling Dichotomous Outcomes\n\n\n\n\n \n\n\nOptional\n\n\n\n\n\nLinear Probability Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nMar. 26\n\n\n\n\n\nLogistic Regression Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nMar. 28\n\n\n\n\n\nMore Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nApr. 02\n\n\n\n\n\n\n\nUnit 05: Linear Mixed-Effects Regression Models\n\n\n\n\n \n\n\nApr. 04\n\n\n\n\n\nIntroduction to Mixed-Effects Regression Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nApr. 09\n\n\n\n\n \n\n\nApr. 11\n\n\n\n\n\nLMER: Average Change Over Time\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nApr. 16\n\n\n\n\n \n\n\nApr. 18\n\n\n\n\n\nLMER: Other Random-Effects and Covariates\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nApr. 23\n\n\n\n\n \n\n\nOptional\n\n\n\n\n\nLMER: Alternative Representations & Assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nApr. 25\n\n\n\n\n\nQ&A and Future Coursework"
  }
]